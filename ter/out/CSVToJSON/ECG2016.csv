Id;series;booktitle;year;title;abstract;authors;pdf1page;pdfarticle;MS;place;Latitude;Longitude
1104;Revue des Nouvelles Technologies de l'Information;EGC;2016;A Relevant Passage Retrieval and Re-ranking Approach for Open-Domain Question Answering;Les systèmes de questions-réponses (SQR)s visent à retourner directement des réponsesprécises à des questions posées en langage naturel. L'extraction et le reclassement des passagessont considérés comme les tâches les plus difficiles dans un SQR typique et exigent encore uneffort non trivial. Dans cet article, nous proposons une nouvelle approche pour L'extraction etle reclassement des passages en utilisant les n-grammes et SVM. Notre système d'extractionde passages basé sur la technique des n-grammes repose sur une nouvelle mesure de similaritéentre un passage et une question. Les passages extraits sont ensuite réordonnés en utilisant unmodèle basé sur RankSVM combinant différentes mesures de similarité afin de retourner lepassage le plus pertinent pour une question donnée. Nos expériences et nos résultats étaientprometteurs et ont démontré que notre approche est concurrentielle.;Nouha Othman, Rim Faiz;http://editions-rnti.fr/render_pdf.php?p1&p=1002161;http://editions-rnti.fr/render_pdf.php?p=1002161;2;Reims;49.258329;4.031696
1105;Revue des Nouvelles Technologies de l'Information;EGC;2016;Adaptation des Mappings entre Systèmes d'Organisation de la Connaissance du domaine Biomédical;"Cette thèse de doctorat propose une approche originale pour adapter les mappingsbasés sur les changements détectés dans l'évolution de SOCs du domaine biomédical.Notre proposition consiste à comprendre précisément les mappings entre SOCs, à exploiterles types de changements intervenant lorsque les SOCs évoluent, puis à proposerdes actions de modification des mappings appropriées. Nos contributions sont multiples: (i) nous avons réalisé un travail expérimental approfondi pour comprendre l'évolutiondes mappings entre SOCs; nous proposons des méthodes automatiques (ii) pour analyserles mappings affectés par l'évolution de SOCs, et (iii) pour reconnaître l'évolutiondes concepts impliqués dans les mappings via des patrons de changement; enfin (iv)nous proposons des techniques d'adaptation des mappings à base d'heuristiques. Nousproposons un cadre complet pour l'adaptation des mappings, appelé DyKOSMap, etun prototype logiciel. Nous avons évalué les méthodes proposées et le cadre formel avecdes jeux de données réelles contenant plusieurs versions de mappings entre SOCs du domaine biomédical. Les résultats des expérimentations ont démontré l'efficacité desprincipes sous-jacents à l'approche proposée. La maintenance des mappings, en grandepartie automatique, est de bonne qualité..";Julio Cesar Dos Reis;http://editions-rnti.fr/render_pdf.php?p1&p=1002152;http://editions-rnti.fr/render_pdf.php?p=1002152;13;Reims;49.258330;4.031697
1106;Revue des Nouvelles Technologies de l'Information;EGC;2016;Analyse d'activité et exposition de la vie privée sur les médias sociaux;Anonymous use of Social network do not prevent users from privacy risks resulting frominfering and cross-checking information published by themselves or their relationhips. Withthis in mind we have conducted a survey in order to measure sensitiveness of personal datapublished on social media and to analyze the users behaviors. We have shown that 76 %of internet users that have answered the survey are vulnerable to identity or sensitive datadisclosure. Our study is completed by the description of an automatic procedure that showshow easily these vulnerabilities can be exploited and motivates the need for more advancedprotection mechanisms.;Younes Abid, Abdessamad Imine, Amedeo Napoli, Chedy Raïssi, Marc Rigolot, Michaël Rusinowitch;http://editions-rnti.fr/render_pdf.php?p1&p=1002223;http://editions-rnti.fr/render_pdf.php?p=1002223;11;Reims;49.258331;4.031698
1107;Revue des Nouvelles Technologies de l'Information;EGC;2016;Analyse exploratoire par k-Coclustering avec Khiops CoViz;"En analyse exploratoire, l'identification et la visualisation des interactionsentre variables dans les grandes bases de données est un défi (Dhillon et al.,2003; Kolda et Sun, 2008). Nous présentons Khiops CoViz, un outil qui permetd'explorer par visualisation les relations importantes entre deux (ou plusieurs)variables, qu'elles soient catégorielles et/ou numériques. La visualisation d'unrésultat de coclustering de variables prend la forme d'une grille (ou matrice) dontles dimensions sont partitionnées: les variables catégorielles sont partitionnéesen clusters et les variables numériques en intervalles. L'outil permet plusieurs variantesde visualisations à différentes échelles de la grille au moyen de plusieurscritères d'intérêt révélant diverses facettes des relations entre les variables.";Bruno Guerraz, Marc Boullé, Dominique Gay, Vincent Lemaire, Fabrice Clérot;http://editions-rnti.fr/render_pdf.php?p1&p=1002206;http://editions-rnti.fr/render_pdf.php?p=1002206;11;Reims;49.258332;4.031699
1108;Revue des Nouvelles Technologies de l'Information;EGC;2016;Analyse géographique de séries de publications : application aux conférences EGC;"Dans cet article, nous présentons une méthodologie originale permettantde faire des analyses scientométriques basées sur trois dimensions (spatiale,temporelle et thématique) à partir d'un corpus de publications. Cette méthodologiecomporte 3 étapes : (1) la préparation et la validation des données pourcompléter les critères usuels tels que les noms d'auteurs, affiliation, ... par descritères spatiaux, temporels et thématiques ; (2) l'indexation des contenus despublications et métadonnées associées ; (3) l'analyse et/ou la recherche d'informationmultidimentionnelle. Les expérimentations sont menées sur la série depublications des conférences EGC de 2004 à 2015.";Eric Kergosien, Marie-Noëlle Bessagnet, Christian Sallaberry, Annig Le Parc - Lacayrelle, Albert Royer;http://editions-rnti.fr/render_pdf.php?p1&p=1002190;http://editions-rnti.fr/render_pdf.php?p=1002190;9;Reims;49.258333;4.031700
1109;Revue des Nouvelles Technologies de l'Information;EGC;2016;Analyses synchroniques et diachroniques des thématiques EGC- Défi ECG 2016;Les articles scientifiques publiés dans les actes des conférences EGC,qui se déroulent chaque année depuis 2001, constituent la richesse de ces évènementsmettant en avant le fer de lance de la recherche francophone portantsur la gestion et l'extraction de connaissances. Nous nous sommes penchés surl'analyse de ces publications scientifiques afin d'en extraire l'essence en termesde thématiques de recherches abordées. Premièrement, nous avons analysé lespoints communs et les spécificités des publications dans les différentes éditionsde la conférence ainsi que les principales différences entre les éditions consécutives.Puis nous nous sommes intéressés à la façon dont les publications s'articulentautour des thématiques extraites et sur lesquelles nous avons essayé devisualiser une approximation sémantique. Enfin nous nous sommes intéresséà l'évolution des thématiques depuis les débuts de cette conférence et jusqu'àl'édition 2015.;Sofiane Bouzid, Adrian Tanasescu;http://editions-rnti.fr/render_pdf.php?p1&p=1002195;http://editions-rnti.fr/render_pdf.php?p=1002195;9;Reims;49.258334;4.031701
1110;Revue des Nouvelles Technologies de l'Information;EGC;2016;Apprentissage du signal prix de l'électricité. Arbres de régression, séries temporelles et prédictions à long terme;"Predicting the price of the electricity commodity in the long term is a challenge that currenttechniques do not meet satisfactorily (Karakatsani et Bunn, 2010; Weron, 2014). In this paper,we introduce a new regression tree based model that yields good predictions on a long-termperiod with low computational resources requirements. Our approach is validated by temporalseries collected from an electricity provider.";Louis-Victor Pasquier, Antoine Cornuéjols, Suzanne Pinson;http://editions-rnti.fr/render_pdf.php?p1&p=1002222;http://editions-rnti.fr/render_pdf.php?p=1002222;11;Reims;49.258335;4.031702
1111;Revue des Nouvelles Technologies de l'Information;EGC;2016;Approche de Clustering de Flux basée sur les Graphes de Voisinage;We propose a neighborhood-based approach for data streams clustering. Instead of processingeach new element one by one, we propose to process each group of new elements simultaneously.A neighborhood-based clustering is applied on each new group. We also definean incremental construction method of the neighborhood graph based on the stream evolution.To validate the approach, we apply it to multiple data sets and we compare it with variousstream clustering approaches.;Ibrahim Louhi, Lydia Boudjeloud-Assala, Thomas Tamisier;http://editions-rnti.fr/render_pdf.php?p1&p=1002217;http://editions-rnti.fr/render_pdf.php?p=1002217;11;Reims;49.258336;4.031703
1112;Revue des Nouvelles Technologies de l'Information;EGC;2016;Arbres de modèles et flux de données incomplets;Model tree is a useful and convenient method for predictive analytics in data streams.Often, this issue is solved by pre-processing techniques applied prior to the training phase ofthe model. In this article, we propose a new method that estimates and adjusts missing valuesbefore the model tree training. A prototype was developed and tested on several data streams.;Olivier Parisot, Yoanne Didry, Thomas Tamisier, Benoît Otjacques;http://editions-rnti.fr/render_pdf.php?p1&p=1002210;http://editions-rnti.fr/render_pdf.php?p=1002210;11;Reims;49.258337;4.031704
1113;Revue des Nouvelles Technologies de l'Information;EGC;2016;Associer argumentation et simulation en aide à la décision : Illustration en agroalimentaire;Prendre une décision impliquant plusieurs acteurs aux objectifs divergentsnécessite de considérer des informations tant qualitatives – les préférencesdes acteurs sur les décisions possibles – que quantitatives – les paramètres servantd'indicateurs pour les acteurs. Dans cet article nous nous intéressons à l'associationde ces deux types d'approches. Le modèle qualitatif considéré est l'argumentation.Le modèle quantitatif simulant les scénarios découlant de chaquedécision est la dynamique des systèmes. Cet article s'intéresse aux éléments permettantde connecter les deux formalismes. Un exemple en agroalimentaire vienten appui à cette réflexion.;Rallou Thomopoulos, Sébastien Gaucel, Bernard Moulin;http://editions-rnti.fr/render_pdf.php?p1&p=1002187;http://editions-rnti.fr/render_pdf.php?p=1002187;4;Reims;49.258338;4.031705
1114;Revue des Nouvelles Technologies de l'Information;EGC;2016;Caractérisation d'instances d'apprentissage pour un méta-mining évolutionnaire;Machine learning has proven to be a powerful tool in diverse fields, and is getting moreand more widely used by non-experts. One of the foremost difficulties they encounter liesin the choice and calibration of the machine learning algorithm to use. Our objective is thusto provide assistance in the matter, using a meta-learning approach based on an evolutionaryheuristic. We introduce here this approach as a potential solution to the limitation of currentdata characterization.;William Raynaut, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau, Cédric Dray, Philippe Valet;http://editions-rnti.fr/render_pdf.php?p1&p=1002221;http://editions-rnti.fr/render_pdf.php?p=1002221;11;Reims;49.258339;4.031706
1115;Revue des Nouvelles Technologies de l'Information;EGC;2016;Catégorisation et Désambiguïsation des Intérêts des Individus dans le Web Social;Cet article présente une approche pour la catégorisation et la désambiguïsationdes intérêts que les individus renseignent sur les réseaux sociaux enutilisant Wikipédia.;Coriane Nana Jipmo, Gianluca Quercini, Nacéra Bennacer;http://editions-rnti.fr/render_pdf.php?p1&p=1002212;http://editions-rnti.fr/render_pdf.php?p=1002212;11;Reims;49.258340;4.031707
1116;Revue des Nouvelles Technologies de l'Information;EGC;2016;Clustering par apprentissage de distance guidé par des préférences sur les attributs;Ces dernières années de nombreuses méthodes semi-supervisées declustering ont intégré des contraintes entre paires d'objets ou d'étiquettes declasse, afin que le partitionnement final soit en accord avec les besoins de l'utilisateur.Pourtant dans certains cas où les dimensions d'études sont clairementdéfinies, il semble opportun de pouvoir directement exprimer des contraintessur les attributs pour explorer des données. De plus, une telle formulation permettraitd'éviter les écueils classiques de la malédiction de la dimensionnalitéet de l'interprétation des clusters. Cet article propose de prendre en compte lespréférences de l'utilisateur sur les attributs afin de guider l'apprentissage de ladistance pendant le clustering. Plus précisément, nous montrons comment paramétrerla distance euclidienne par une matrice diagonale dont les coefficientsdoivent être au plus proche des poids fixés par l'utilisateur. Cette approche permetd'ajuster le clustering pour obtenir un compromis entre les approches guidéespar les données et par l'utilisateur. Nous observons que l'ajout des préférencesest parfois essentiel pour atteindre un clustering de meilleure qualité.;Adnan El Moussawi, Ahmed Cheriat, Arnaud Giacometti, Nicolas Labroche, Arnaud Soulet;http://editions-rnti.fr/render_pdf.php?p1&p=1002185;http://editions-rnti.fr/render_pdf.php?p=1002185;4;Reims;49.258341;4.031708
1117;Revue des Nouvelles Technologies de l'Information;EGC;2016;Clustering visuel semi-interactif;Nous proposons dans cet article une approche de clustering visuelsemi-interactif. L'approche proposée utilise la perception visuelle pour guiderl'utilisateur dans le processus interactif. Les clusters sont extraits de manièresuccessive et itérative, puis évalués selon leur ordre d'extraction. Pour l'utilisateur,l'approche semi-interactive permet non seulement d'évaluer les classes enfonction d'un critère déterminé mais aussi d'évaluer l'influence de l'extractiond'un cluster sur ceux précédemment extraits. Un protocole de test est présentéafin de comparer cette approche avec les approches purement automatiques etpurement interactives. Cet article est un résumé d'un papier accepté 1 pour unjournal international.;Lydia Boudjeloud, Philippe Pinheiro, Alexandre Blansché, Thomas Tamisier, Benoît Otjacques;http://editions-rnti.fr/render_pdf.php?p1&p=1002183;http://editions-rnti.fr/render_pdf.php?p=1002183;6;Reims;49.258342;4.031709
1118;Revue des Nouvelles Technologies de l'Information;EGC;2016;Combinaison de méthodes numériques et symboliques pour l'analyse de données métabolomiques;Our work consists in developing a workflow using Knowledge Discovery methodologiesto propose advanced predictive biomarkers discovery solutions from metabolomic data. Wepropose to use machine learning algorithms for feature selection and FCA for visualization.;Dhouha-Grissa, Blandine Comte, Estelle Pujos-Guillot, Amedeo Napoli;http://editions-rnti.fr/render_pdf.php?p1&p=1002228;http://editions-rnti.fr/render_pdf.php?p=1002228;11;Reims;49.258343;4.031710
1119;Revue des Nouvelles Technologies de l'Information;EGC;2016;Concept drift vs suicide: comment l'un peut prévenir l'autre?;Le suicide devient d'année en année une problématique plus préoccupante.Les organismes de santé tels que l'OMS se sont engagés à réduire lenombre de suicides de 10% dans l'ensemble des pays membres d'ici 2020. Sile suicide est généralement un geste impulsif, il existe souvent des actes et desparoles qui peuvent révéler un mal être et représenter des signes précurseurs deprédispositions au suicide. L'objectif de cette étude est de mettre en place unsystème pour détecter semi-automatiquement ces comportements et ces parolesau travers des réseaux sociaux. Des travaux précédents ont proposé la classificationde messages issus de Twitter suivant des thèmes liés au suicide : tristesse,blessures psychologiques, état mental, etc. Dans cette étude, nous ajoutons la dimensiontemporelle pour prendre en compte l'évolution de l'état des personnesmonitorées. Nous avons implémenté pour cela différentes méthodes d'apprentissagedont une méthode originale de concept drift. Nous avons expérimenté avecsuccès cette méthode sur des données réelles issues du réseau social Facebook.;Cédric Maigrot, Sandra Bringay, Jérôme Azé;http://editions-rnti.fr/render_pdf.php?p1&p=1002172;http://editions-rnti.fr/render_pdf.php?p=1002172;7;Reims;49.258344;4.031711
1120;Revue des Nouvelles Technologies de l'Information;EGC;2016;Construction incrémentale d'une structure hiérarchique pour l'exploration visuelle et interactive de larges collections d'images;Dans cet article, nous étudions de manière conjointe la construction etl'exploration visuelle d'une structure de classification pour de très grande based'images. Pour garantir que la structure construite vérifiera les contraintes detaille nécessaires à sa visualisation dans une interface Web tout en reflétant lespropriétés topologiques des données (clusters), nous combinons la classificationhiérarchique de BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)avec la construction de graphes de voisinage : un graphe de voisinageest créé et mis à jour de manière incrémentale pour représenter les fils de chaquenoeud de l'arbre. De plus, un ensemble d'images représentatives est remonté àchaque noeud interne pour guider l'utilisateur lors de l'exploration visuelle del'arbre. L'ensemble des algorithmes utilisés sont incrémentaux pour gérer l'insertionde nouvelles images dans la collection. Nous présentons les premiersrésultats sur des dizaines de milliers d'images qui peuvent être ainsi structuréesen une minute de temps de calcul. L'exploration dans l'interface est fluide grâceaux propriétés de la structure construite.;Frédéric Rayar, Sabine Barrat, Fatma Bouali, Gilles Venturini;http://editions-rnti.fr/render_pdf.php?p1&p=1002184;http://editions-rnti.fr/render_pdf.php?p=1002184;6;Reims;49.258345;4.031712
1121;Revue des Nouvelles Technologies de l'Information;EGC;2016;Contributions à la coloration des hypergraphes basées sur les traverses minimales;In this paper, we propose two contributions about the determination of chromatic numberand the verification of the 2-colorability property. We introduce an unreleased relation betweenthe problem of hypergraph coloring and the computation of minimal transversals hypergraphand, especially, a subset of them. Thereby, we propose two algorithms in order to optimize theverification of the 2-colorability property of hypergraphs and the evaluation of the chromaticnumber. Experiments carried out on several types of hypergraphs, showed that our algorithmobtains very interesting results.;M. Nidhal Jelassi, Sadok Ben Yahia, Christine Largeron;http://editions-rnti.fr/render_pdf.php?p1&p=1002224;http://editions-rnti.fr/render_pdf.php?p=1002224;11;Reims;49.258346;4.031713
1122;Revue des Nouvelles Technologies de l'Information;EGC;2016;Découverte de labels dupliqués par l'exploration du treillis des classifieurs binaires;L'analyse des données comportementales représente aujourd'hui ungrand enjeu. Tout individu génère des traces d'activité et de mobilité. Lorsqu'ellessont associées aux individus, ou labels, qui les ont créées, il est possiblede construire un modèle qui prédit avec précision l'appartenance d'une nouvelletrace. Sur internet, il est cependant fréquent qu'un utilisateur possède différentesidentités virtuelles, ou labels doublons. Les ignorer provoque une grande réductionde la précision de l'identification. Il est ainsi question dans cet article du problèmede déduplication de labels, et l'on présente une méthode originale baséesur l'exploration du treillis des classifieurs binaires. Chaque sous-ensemble delabels est classifié face à son complémentaire et des contraintes rendent possiblel'identification des labels doublons en élaguant l'espace de recherche. Des expérimentationssont menées sur des données issues du jeu vidéo STARCRAFT 2.Les résultats sont de bonne qualité et encourageants.;Quentin Labernia, Victor Codocedo, Mehdi Kaytoue, Celine Robardet;http://editions-rnti.fr/render_pdf.php?p1&p=1002177;http://editions-rnti.fr/render_pdf.php?p=1002177;1;Reims;49.258347;4.031714
1123;Revue des Nouvelles Technologies de l'Information;EGC;2016;Découverte de motifs intelligibles et caractéristiques d'anomalies dans les traces unitaires;"De nombreuses industries manufacturières s'intéressent aujourd'hui àl'exploitation des grandes collections de traces unitaires. Les applications sontmultiples et vont du simple ""reporting"" à la détection de fraudes en passant parla gestion de retours ou encore la mise en évidence d'incohérences dans lescircuits de distribution. Une étape importante consiste à détecter des anomaliesdans des collections de traces. Si les travaux concernant la détection d'anomaliessont assez nombreux, peu permettent de caractériser les anomalies détectées parune description intelligible. Étant donné un ensemble de traces unitaires, nousdéveloppons une méthode d'extraction de motifs pour détecter et contextualiserdes comportements non conformes à un modèle expert (fourni ou construit àpartir des données). Le degré d'anomalie est alors quantifié grâce à la proportiondu nombre de mouvements des objets qui ne sont pas prévus dans le modèleexpert. Cette recherche est financée partiellement par un programme industrielqui ne permet ni de dévoiler le contexte concret ni de parler des données réelles.Ainsi, nous validons empiriquement la valeur ajoutée de la méthode proposéepar l'étude de traces de mobilité dans un jeu vidéo : nous pouvons alors discuterd'un motif qui explicite les raisons de l'inexpérience de certains joueurs.";Olivier Cavadenti, Victor Codocedo, Mehdi Kaytoue, Jean-François Boulicaut;http://editions-rnti.fr/render_pdf.php?p1&p=1002153;http://editions-rnti.fr/render_pdf.php?p=1002153;3;Reims;49.258348;4.031715
1124;Revue des Nouvelles Technologies de l'Information;EGC;2016;Défi EGC 2016 : Analyse par Motifs Fréquents et Topic Modeling;Dans le domaine de l'analyse de textes, l'extraction de motifs est unetechnique très populaire pour mettre en évidence des relations fréquentes entreles mots. De même, les techniques de topic modeling ont largement fait leurspreuves lorsqu'il s'agit de classer automatiquement des ensembles de textes partageantdes thématiques similaires. Ainsi, ce papier a pour ambition de montrerl'intérêt de l'utilisation conjointe de ces deux techniques afin de mettre en évidence,sous la forme d'un graphe biparti, des mots partageant des thématiquessimilaires mais aussi leurs relations fréquentes, intra et inter thématiques. Lesdonnées du Défi EGC 2016 permettent de valider l'intérêt de l'approche, touten montrant l'évolution des thématiques et des mots clés parmi les papiers de laconférence EGC sur ces onze dernières années.;Julien Aligon, Fabrice Guillet, Julien Blanchard, Fabien Picarougne;http://editions-rnti.fr/render_pdf.php?p1&p=1002192;http://editions-rnti.fr/render_pdf.php?p=1002192;9;Reims;49.258349;4.031716
1125;Revue des Nouvelles Technologies de l'Information;EGC;2016;Défi EGC 2016 Vues Conceptuelles des Collaborations aux Conférences EGC depuis 2004: Une modélisation descriptive;Dans ce travail, nous analysons les données concernant les articles publiés à laconférence EGC. Notre objectif est d'identifier et de comprendre les tendances en matièrede collaborations. Pour ce faire, nous adoptons une modélisation descriptive, à travers uneapproche réseau qui consiste à générer tout d'abord le réseau de collaborations des auteursà partir des données. Nous enrichissons ensuite les noeuds de ce réseau d'une dizained'attributs individuels extraits à partir des données. Enfin, nous recherchons des vuesconceptuelles, une approche récente de clustering de liens, qui permet de synthétiser desréseaux en mettant en évidence les ensembles d'attributs retrouvés fréquemment liés dansle réseau. Les résultats obtenus montrent les tendances existantes dans les comportementsde collaborations. Dans ce papier, nous présentons ces tendances et montrons commentelles évoluent selon différents seuils d'extraction.;Erick Stattner;http://editions-rnti.fr/render_pdf.php?p1&p=1002196;http://editions-rnti.fr/render_pdf.php?p=1002196;9;Reims;49.258350;4.031717
1126;Revue des Nouvelles Technologies de l'Information;EGC;2016;Détection de données aberrantes à partir de motifs fréquents sans énumération exhaustive;La détection de données aberrantes (outliers) consiste à détecter desobservations anormales au sein des données. Durant la dernière décennie, desméthodes de détection d'outliers utilisant les motifs fréquents ont été proposées.Elles extraient dans une première phase tous les motifs fréquents, puis assignentà chaque transaction un score mesurant son degré d'aberration (en fonction dunombre de motifs fréquents qui la couvre). Dans cet article, nous proposons deuxnouvelles méthodes pour calculer le score d'aberration fondé sur les motifs fréquents(FPOF). La première méthode retourne le FPOF exact de chaque transactionsans extraire le moindre motif. Cette méthode s'avère en temps polynomialpar rapport à la taille du jeu de données. La seconde méthode est une méthodeapprochée où l'utilisateur final peut contrôler l'erreur maximale sur l'estimationdu FPOF. Une étude expérimentale montre l'intérêt des deux méthodes pour lesjeux de données volumineux où une approche exhaustive échoue à calculer unesolution exacte. Pour un même nombre de motifs, la précision de notre méthodeapprochée est meilleure que celle de la méthode classique.;Arnaud Giacometti, Arnaud Soulet;http://editions-rnti.fr/render_pdf.php?p1&p=1002155;http://editions-rnti.fr/render_pdf.php?p=1002155;3;Reims;49.258351;4.031718
1127;Revue des Nouvelles Technologies de l'Information;EGC;2016;Détection de messages falsifiés de localisation de navires;The Automatic Identification System was initially designed for safety purposes. However,the system is not secured and the messages contain errors and undergo attacks and falsifications.This article proposes a methodological approach for the detection of falsified AISmessages.;Clément Iphar, Aldo Napoli, Cyril Ray;http://editions-rnti.fr/render_pdf.php?p1&p=1002229;http://editions-rnti.fr/render_pdf.php?p=1002229;11;Reims;49.258352;4.031719
1128;Revue des Nouvelles Technologies de l'Information;EGC;2016;Enrichissement de schéma multidimensionnel en constellation grâce à la Classification Ascendante Hiérarchique;Les hiérarchies sont des structures cruciales dans un entrepôt de donnéespuisqu'elles permettent l'agrégation de mesures dans le but de proposerune vue analytique plus ou moins globale sur les données entreposées, selon leniveau hiérarchique auquel on se place. Cependant, peu de travaux s'intéressentà la construction de hiérarchies, via un algorithme de fouille de données, prenanten compte le contexte multidimensionnel de la dimension concernée. Danscet article, nous proposons donc un algorithme, implémenté sur une architectureROLAP, permettant d'enrichir une dimension avec des données factuelles.;Lucile Sautot, Sandro Bimonte, Ludovic Journaux, Arnaud Larrère, Kévin Saint-Paul, Bruno Faivre;http://editions-rnti.fr/render_pdf.php?p1&p=1002209;http://editions-rnti.fr/render_pdf.php?p=1002209;11;Reims;49.258353;4.031720
1129;Revue des Nouvelles Technologies de l'Information;EGC;2016;Évaluation et Prédiction de la Centralité de Groupes de Recherche dans un Réseau de Collaborations Scientifiques;De nos jours, il y a un fort intérêt pour de nouvelles méthodes d'évaluationdes groupes de recherche afin de quantifier l'impact de leur travail surtoute la communauté scientifique et de tenter de prédire leurs performances dansle futur. Dans ce contexte, nous proposons une nouvelle approche hybride quimesure la centralité d'un groupe de chercheurs publiants. Cette mesure profitede l'expressivité et de la capacité d'inférence apportées par une modélisationontologique des groupes et des thématiques inférées, et d'une modélisation engraphe qui permet d'explorer les interactions entre ces différents groupes aufil du temps. Ce modèle permet également de détecter les groupes capables decollaborer avec d'autres tout en maintenant un haut niveau de production, etd'identifier ceux qui sont plus déterminants sur les thématiques déduites, afin dedévelopper des collaborations de recherche plus fructueuses.;Aurélien Bossard, Mario Cataldi, Myriam Lamolle, Chan Le Duc;http://editions-rnti.fr/render_pdf.php?p1&p=1002193;http://editions-rnti.fr/render_pdf.php?p=1002193;9;Reims;49.258354;4.031721
1130;Revue des Nouvelles Technologies de l'Information;EGC;2016;Exploration des Données du Défi EGC 2016 à l'aide d'un Système d'Information Logique;"Nous présentons dans cet article les méthodes employées et les résultatsobtenus en réponse au Défi EGC 2016. Notre approche repose d'une partsur des chaînes automatiques de traitements linguistiques en français et en anglaisutilisant le plus possible des ressources et outils publics et d'autre part surun environnement d'exploration des données basé sur les systèmes d'informationlogiques ; ces systèmes exploitent une généralisation des treillis de conceptsformels appliquée aux données attribut-valeur ou au web sémantique.";Peggy Cellier, Sébastien Ferré, Annie Foret, Olivier Ridoux;http://editions-rnti.fr/render_pdf.php?p1&p=1002198;http://editions-rnti.fr/render_pdf.php?p=1002198;9;Reims;49.258355;4.031722
1131;Revue des Nouvelles Technologies de l'Information;EGC;2016;Extension de C-SPARQL pour l'échantillonnage de flux de graphes RDF;Les technologies du web sémantique sont de plus en plus utiliséespour la gestion de flux de données. Plusieurs systèmes de traitement de fluxRDF ont été proposés : C-SPARQL, CQELS, SPARQLstream, EP-SPARQL,SPARKWAVE, etc. Ces derniers étendent tous à la base, le langage d'interrogationsémantique SPARQL. Les données à l'entrée du système sont volumineuseset générées en continu à un rythme rapide et variable. De ce fait, le stockage etle traitement de la totalité du flux deviennent coûteux et le raisonnement presqueimpossible. Par conséquent, le recours à des techniques permettant de réduire lacharge tout en conservant la sémantique des données, permet d'optimiser les traitementsvoire le raisonnement. Cependant, aucune des extensions de SPARQLn'inclut cette fonctionnalité. Ainsi, dans cet article, nous proposons d'étendre lesystème C-SPARQL pour générer des échantillons à la volée sur flux de graphesRDF. Nous ajoutons trois opérateurs d'échantillonnage (UNIFORM, RESERVOIRet CHAIN) à la syntaxe de C-SPARQL. Les expérimentations montrent laperformance de notre extension en terme de temps d'exécution, et de la préservationde la sémantique des données.;Amadou Fall Dia, Zakia Kazi Aoul, Aliou Boly;http://editions-rnti.fr/render_pdf.php?p1&p=1002167;http://editions-rnti.fr/render_pdf.php?p=1002167;1;Reims;49.258356;4.031723
1132;Revue des Nouvelles Technologies de l'Information;EGC;2016;Extraction automatique d'affixes pour la reconnaissance d'entités nommées chimiques;Nous détaillerons ici une approche permettant de détecter des affixes àpartir de dictionnaires en se basant sur l'algorithme de la plus longue sous-chaînecommune, dans le cadre de la reconnaissance d'entités nommées chimiques surCHEMDNER. Nous verrons ensuite des méthodes de sélection et de tri afin deles intégrer au mieux dans un système d'apprentissage automatique.;Yoann Dupont, Isabelle Tellier, Christian Lautier, Marco Dinarelli;http://editions-rnti.fr/render_pdf.php?p1&p=1002216;http://editions-rnti.fr/render_pdf.php?p=1002216;11;Reims;49.258357;4.031724
1133;Revue des Nouvelles Technologies de l'Information;EGC;2016;Extraction de clés de liage de données (résumé étendu);De grandes quantités de données sont publiées sur le web des données.Les lier consiste à identifier les mêmes ressources dans deux jeux de donnéespermettant l'exploitation conjointe des données publiées. Mais l'extractionde liens n'est pas une tâche facile. Nous avons développé une approche qui extraitdes clés de liage (link keys). Les clés de liage étendent la notion de cléde l'algèbre relationnelle à plusieurs sources de données. Elles sont fondées surdes ensembles de couples de propriétés identifiant les objets lorsqu'ils ont lesmêmes valeurs, ou des valeurs communes, pour ces propriétés. On présenteraune manière d'extraire automatiquement les clés de liage candidates à partir dedonnées. Cette opération peut être exprimée dans l'analyse formelle de concepts.La qualité des clés candidates peut-être évaluée en fonction de la disponibilité(cas supervisé) ou non (cas non supervisé) d'un échantillon de liens. La pertinenceet de la robustesse de telles clés seront illustrées sur un exemple réel.;Jérôme Euzenat;http://editions-rnti.fr/render_pdf.php?p1&p=1002150;http://editions-rnti.fr/render_pdf.php?p=1002150;0;Reims;49.258358;4.031725
1134;Revue des Nouvelles Technologies de l'Information;EGC;2016;Extraction de commentaires utilisateurs sur le Web;Dans cet article, nous présentons CommentsMiner, une solution d'extractionnon supervisée pour l'extraction de commentaires utilisateurs. Notreapproche se base sur une combinaison de techniques de fouille de sous-arbresfréquents, d'extraction de données et d'apprentissage de classement. Nos expérimentationsmontrent que CommentsMiner permet de résoudre le problèmed'extraction de commentaires sur 84% d'un jeu de données représentatif et publiquementaccessible, loin devant les techniques existantes d'extraction.;Julien Subercaze, Christophe Gravier, Frédérique Laforest;http://editions-rnti.fr/render_pdf.php?p1&p=1002174;http://editions-rnti.fr/render_pdf.php?p=1002174;7;Reims;49.258359;4.031726
1135;Revue des Nouvelles Technologies de l'Information;EGC;2016;Extraction de connaissances dans les Systèmes d'Information Pervasifs par l'Analyse Formelle de Concepts;Nous présentons une méthode d'extraction de connaissances dans dessystèmes d'information pervasifs. Nous étudions l'impact du contexte (environnement)d'un utilisateur sur les applications qu'il utilise sur son smartphone.Notre proposition pour gérer la complexité des données contextuelles repose surl'Analyse Formelle de Concepts et les treillis de Galois. Nous nous focalisonssur l'automatisation du processus d'interprétation de ces treillis, pour généraliserl'extraction de connaissances et passer à l'échelle. Nous présentons desmétriques originales illustrées sur des données réelles.;Ali Jaffal, Bénédicte Le Grand, Manuele Kirsh-Pinheiro;http://editions-rnti.fr/render_pdf.php?p1&p=1002180;http://editions-rnti.fr/render_pdf.php?p=1002180;1;Reims;49.258360;4.031727
1136;Revue des Nouvelles Technologies de l'Information;EGC;2016;Fabrique logicielle de réseaux sociaux spécialisés (aspects fonctionnels);This paper introduces a software factory for developing social networks. This factory takesan abstract social network and creates a concrete one, using mechanisms such as sub-typingand behavior overloading.;Benjamin Billet, David Fernandez, Didier Parigot;http://editions-rnti.fr/render_pdf.php?p1&p=1002215;http://editions-rnti.fr/render_pdf.php?p=1002215;11;Reims;49.258361;4.031728
1137;Revue des Nouvelles Technologies de l'Information;EGC;2016;Fairness-Aware Data Mining;In data mining we often have to learn from biased data, because, for instance, data comesfrom different batches or there was a gender or racial bias in the collection of social data. Insome applications it may be necessary to explicitly control this bias in the models we learn fromthe data. Recently this topic received considerable interest both in the research community aswell as more general, as witnessed by several recent articles in popular news media such asthe New York Times. In this talk I will introduce and motivate research in fairness-aware datamining. Different techniques in unsupervised and supervised data mining will be discussed,dividing these techniques into three categories: algorithms of the first category adapt the inputdata in such a way to remove harmful biases while the second adapts the learning algorithmsand the third category modifies the output models in such a way that its predictions becomeunbiased. Furthermore different ways to quantify unfairness, and indirect and conditionaldiscrimination will be discussed, each with their own pros and cons. With this talk I hope toconvincingly argument the validity and necessity of this often contested research area.;Toon Calders;http://editions-rnti.fr/render_pdf.php?p1&p=1002147;http://editions-rnti.fr/render_pdf.php?p=1002147;0;Reims;49.258362;4.031729
1138;Revue des Nouvelles Technologies de l'Information;EGC;2016;FODOMUST: une plateforme pour la fouille de données multistratégie multitemporelle;La plateforme FODOMUST 1 est une implantation concrète des méthodes,librairies et interfaces proposées au sein d'ICube. Elle intègre une versionmultisource de la méthode de classification collaborative multistratégie SAMARAH.Elle propose aussi un ensemble d'algorithmes de segmentation soitpropres à ICUBE soit faisant appel à l'OTB. Enfin, trois interfaces dédiées chacuneà un type de données différent permettent une interaction avec l'utilisateur.Sa principale originalité est qu'elle permet la classification, basée sur DTW (DynamicTimeWarping) de données temporelles symboliques ou numériques et deséries temporelles d'images;Pierre Gançarski, Abdoul-Djawadou Salaou;http://editions-rnti.fr/render_pdf.php?p1&p=1002204;http://editions-rnti.fr/render_pdf.php?p=1002204;11;Reims;49.258363;4.031730
1139;Revue des Nouvelles Technologies de l'Information;EGC;2016;Fouille de motifs séquentiels avec ASP;Cet article présente l'utilisation de la programmation par ensemblesréponses (ASP) pour répondre à une tâche de fouille de motifs séquentiels. Lasyntaxe de l'ASP, proche du Prolog, en fait un langage très pertinent pour représenterdes connaissances de manière aisée et ses mécanismes de résolution,basés sur des solveurs efficaces, en font une solution alternative aux approchesde programmation par contraintes pour la fouille déclarative de motifs. Nousproposons un premier encodage de la tâche classique d'extraction de motifs séquentielset de ses variantes (motifs clos et maximaux). Nous comparons lesperformances calculatoires de ses encodages avec une approche de programmationpar contraintes. Les performances obtenues sont inférieures aux approchesde programmation par contraintes, mais l'encodage purement déclaratif offreplus de perspectives d'intégration de connaissances expertes.;Thomas Guyet, Yves Moinard, Rene Quiniou, Torsten Schaub;http://editions-rnti.fr/render_pdf.php?p1&p=1002154;http://editions-rnti.fr/render_pdf.php?p=1002154;3;Reims;49.258364;4.031731
1140;Revue des Nouvelles Technologies de l'Information;EGC;2016;Fusion de données redondantes : une approche explicative;"Nous nous intéressons, dans le cadre du projet ANR Qualinca au traitementdes données redondantes. Nous supposons dans cet article que cette redondancea déjà été établie par une étape préalable de liage de données. Laquestion abordée est la suivante : comment proposer une représentation uniqueen fusionnant les ""duplicats"" identifiés ? Plus spécifiquement, comment décider,pour chaque propriété de la donnée considérée, quelle valeur choisir parmi cellesfigurant dans les ""duplicats"" à fusionner ? Quelle méthode adopter dans le butde pouvoir, par la suite, retracer et expliquer le résultat obtenu de façon transparenteet compréhensible par l'utilisateur ? Nous nous appuyons pour cela surune approche de décision multicritère et d'argumentation.";Fatiha Saïs, Rallou Thomopoulos;http://editions-rnti.fr/render_pdf.php?p1&p=1002189;http://editions-rnti.fr/render_pdf.php?p=1002189;4;Reims;49.258365;4.031732
1141;Revue des Nouvelles Technologies de l'Information;EGC;2016;Génération de contraintes pour le clustering à partir d'une ontologie - Application à la classification d'images satellites;"L'utilisation des connaissances a priori peut fortement améliorer laclassification non-supervisée. L'injection de ces connaissances sous forme decontraintes sur les données figure parmi les techniques les plus efficaces de lalittérature. Cependant, la génération des contraintes est très coûteuse et demandel'intervention de l'expert ; la sémantique apportée par l'étiquetage de l'expertest aussi perdue dans ce type de techniques, seuls les contraintes sont retenuespar le clustering. Dans cet article, nous proposons une nouvelle approche hybrideexploitant le raisonnement à base d'ontologie pour générer automatiquementdes contraintes permettant de guider et améliorer le clustering. L'utilisationd'une ontologie comme connaissance a priori a plusieurs avantages. Elle permetl'interprétation automatisée des connaissances, ajoute de la modularité dans lachaîne de traitement et améliore la qualité du clustering en prenant en comptela vision de l'utilisateur. Pour évaluer notre approche, nous l'avons appliquée àla classification d'images satellites et les résultats obtenus démontrent des améliorationsnotables à la fois au niveau de la qualité du clustering et au niveau del'étiquetage sémantique des clusters sans intervention de l'expert.";Hatim Chahdi, Nistor Grozavu, Isabelle Mougenot, Laure Berti-Equille, Younès Bennani;http://editions-rnti.fr/render_pdf.php?p1&p=1002158;http://editions-rnti.fr/render_pdf.php?p=1002158;2;Reims;49.258366;4.031733
1142;Revue des Nouvelles Technologies de l'Information;EGC;2016;Identification de Classes Sémantiques Basée sur des Mesures de Proximité Sémantique;Semantic relations are the core of a growing number of knowledge-intensive systems. Theneed to validate automatically such relations remains an up-to-date challenge. In this paper, wepresent a web-based method enabling the automatic identification of the class of a semantic relation.Using measures based on syntactic patterns as entry features for a learning algorithm,we are able to successfully identify 72% of semantic relations divided in 4 classes in a semanticallyrich environment.;Jean Petit, Jean-Charles Risch;http://editions-rnti.fr/render_pdf.php?p1&p=1002213;http://editions-rnti.fr/render_pdf.php?p=1002213;11;Reims;49.258367;4.031734
1143;Revue des Nouvelles Technologies de l'Information;EGC;2016;Intégration de connaissances lexicales et sémantiques pour l'analyse de sentiments dans les SMS;With the explosive growth of the social media (forums, blogs, and social networks) on theWeb, the exploitation of these new information sources became essential. In this paper, wepresent a new automatic method to integrate knowledge for sentiment detection from a SMScorpus by combining lexical and semantic information.;Wedjene Khiari, Mathieu Roche, Asma Bouhafs Hafsia;http://editions-rnti.fr/render_pdf.php?p1&p=1002227;http://editions-rnti.fr/render_pdf.php?p=1002227;11;Reims;49.258368;4.031735
1144;Revue des Nouvelles Technologies de l'Information;EGC;2016;Intégration des Influences Géographique et Temporelle pour la Recommandation de Points d'Intérêt;La recommandation de points d'intérêts (ou POI), est devenue un problèmemajeur avec l'émergence des réseaux sociaux (ou LBSN). À la différencedes approches de recommandation traditionnelles, les données des LBSN présententdes caractéristiques géographique et temporelle importantes qui limitentles performances des algorithmes traditionnels existant. L'intégration de ces caractéristiquesdans un unique modèle de factorisation pour augmenter la qualitéde la recommandation n'a pas été un problème très étudié jusqu'à présent. Dansce papier nous présentons GeoMF-TD, une extension d'un modèle de factorisationgéographique avec des dépendances temporelles. Nos expérimentationssur un jeu de données réel montre jusqu'à 20% de gain sur la précision de larecommandation.;Jean-Benoît Griesner, Talel Abdesssalem, Hubert Naacke;http://editions-rnti.fr/render_pdf.php?p1&p=1002166;http://editions-rnti.fr/render_pdf.php?p=1002166;2;Reims;49.258369;4.031736
1145;Revue des Nouvelles Technologies de l'Information;EGC;2016;Khiops: outil d'apprentissage supervisé automatique pour la fouille de grandes bases de données multi-tables;Khiops est un outil d'apprentissage supervisé automatique pour lafouille de grandes bases de données multi-tables. L'importance prédictive desvariables est évaluée au moyen de modèles de discrétisation dans le cas numériqueet de groupement de valeurs dans le cas catégoriel. Dans le cas d'unebase multi-tables, par exemple des clients avec leurs achats, une table d'analyseindividus × variables est produite par construction automatique de variables.Le modèle de classification utilisé est un classifieur Bayésien naïf avec sélectionde variables et moyennage de modèles. L'outil est adapté à l'analyse desgrandes bases de données, avec des millions d'individus, des dizaines de milliersde variables et des centaines de millions d'enregistrements dans les tablessecondaires.;Marc Boullé;http://editions-rnti.fr/render_pdf.php?p1&p=1002208;http://editions-rnti.fr/render_pdf.php?p=1002208;11;Reims;49.258370;4.031737
1146;Revue des Nouvelles Technologies de l'Information;EGC;2016;L'analyse relationnelle de concepts pour la fouille de données temporelles – Application à l'étude de données hydroécologiques;"Cet article présente une méthode d'exploration de données temporelles,fondée sur l'analyse relationnelle de concepts (ARC) et appliquée à desdonnées séquentielles construites à partir d'échantillons physico-chimiques etbiologiques prélevés dans des cours d'eau. Notre but est de mettre au jour dessous-séquences pertinentes et hiérarchisées, associant les deux types de paramètres.Pour faciliter la lecture, ces sous-séquences sont représentées sous laforme de motifs partiellement ordonnés (po-motifs). Le processus de fouille dedonnées se décompose en plusieurs étapes : construction d'un modèle temporelad hoc et mise en oeuvre de l'ARC ; extraction des sous-séquences synthétiséessous la forme de po-motifs ; sélection des po-motifs intéressants grâce à unemesure exploitant la distribution des extensions de concepts. Le processus a ététesté sur un jeu de données réelles et évalué quantitativement et qualitativement.";Cristina Nica, Agnès Braud, Xavier Dolques, Marianne Huchard, Florence Le Ber;http://editions-rnti.fr/render_pdf.php?p1&p=1002178;http://editions-rnti.fr/render_pdf.php?p=1002178;13;Reims;49.258371;4.031738
1147;Revue des Nouvelles Technologies de l'Information;EGC;2016;La génération des résumés visuels de flux de données de capteurs météorologiques avec des chorèmes;This paper describes a new approach for the automatic generation of visual summariesdealing with cartographic visualization methods and modeling of data coming from sensors inreal time for meteorology. Indeed the concept of chorems seems to be an interesting candidateto visualize real time geographic database summaries.;Zina Bouattou, Robert Laurini, Hafida Belbachir;http://editions-rnti.fr/render_pdf.php?p1&p=1002214;http://editions-rnti.fr/render_pdf.php?p=1002214;11;Reims;49.258372;4.031739
1148;Revue des Nouvelles Technologies de l'Information;EGC;2016;La r-confiance pour l'identification de trajectoires de patients;Sequential patterns mining consist in identifying frequent sequences of ordered events. Tosolve the problem of the large number of patterns obtained, we extend the interest measurecalled confidence, conventionally used to select association rules to sequential patterns. Wefocused on a case study: myocardial infarction (MI), in order to predict the trajectory of patientswith MI between 2009 and 2013. The results were submitted to an expert for discussionand validation.;Yves Mercadier, Jessica Pinaire, Jérôme Azé, Sandra Bringay, Maguelonne Teisseire;http://editions-rnti.fr/render_pdf.php?p1&p=1002218;http://editions-rnti.fr/render_pdf.php?p=1002218;11;Reims;49.258373;4.031740
1149;Revue des Nouvelles Technologies de l'Information;EGC;2016;La révolution de l'assurance par la donnée : défis scientifiques de l'extraction à la gestion de connaissances;"La quantité de données dans notre monde a explosé et l'analyse de grands ensemblesde données – aussi connu dans l'industrie sous le nom « Big Data » – deviendra un atoutmajeur de compétitivité, principalement dû à une croissance de productivité et surtoutà grâce à plus d'innovation. La croissance exponentielle de données est alimentée parla facilité de la captation et par la multiplication de canaux numériques d'acquisition.On pense non seulement à tous les processus qui sont informatisés aujourd'hui, maisaussi aux médias sociaux et aux objets connectés.L'assurance vie une révolution tout particulière. L'assureur, traditionnellement gestionnairedu risque en s'appuyant sur une longue expérience, qu'on traduirait aujourd'huipar une captation systématique de données, est après la révolution numériquepartiellement exclus de canaux digitaux.Ceci est en même temps une menace et une opportunité. Il s'agit d'un défi puisquel'industrie doit réaliser une forte mutation pour se positionner la où la donnée setrouve aujourd'hui, i.e. dans le digital. Il s'agit d'une opportunité puisque ces nouvellesdonnées permettront de mieux appréhender les risques, et plus particulièrement,permettront d'estimer au plus près les risques à la source, plutôt que passer par devariables intermédiaires, comme peut l'être l'âge pour le risque d'accident en conduite.L'opportunité est d'autant plus grande qu'en accédant aux données au plus près desutilisateurs il est possible de faire de la prévention évitant ainsi des accidents coûteuxpour l'assureur, mais surtout désastreux pour les victimes.Une fois la révolution engagée, ceci implique, un certain nombre de transformationsdans les processus d'extraction et gestion de connaissances. Les défis scientifiques sontnombreux, allant de la captation non-intrusive de la donnée, à la visualisation et gestionde connaissances extraites, en passant par de l'apprentissage artificiel pour pouvoirservir à de millions d'utilisateurs simultanément. Dans cette présentation nous allonscouvrir rapidement chacune de ces thématiques avec une attention particulière auxdéfis scientifiques sous-jacents.Nous allons illustrer notre propos par un exemple phare de cette révolution : lafamille d'offres d'assurance dite « pay as you drive » où généralement on obtient unedécote ou réduction en fonction de sa façon de conduire. Nous allons ce que ceci impliqueen termes d'extraction et de gestion de connaissances.Pour conclure, il est important de mentionner que cette révolution implique d'autreschallenges cruciaux qui dépassent ce qui est abordé ici. En particulier, pour ne mentionnerque deux grands axes : la protection de la vie privée, aussi bien du point de vuetechnique que juridique ; et la transformation de métiers accompagné d'une pénurie detalents déjà entamé.";Marcin Detyniecki;http://editions-rnti.fr/render_pdf.php?p1&p=1002148;http://editions-rnti.fr/render_pdf.php?p=1002148;0;Reims;49.258374;4.031741
1150;Revue des Nouvelles Technologies de l'Information;EGC;2016;Learning from Massive, Incompletely annotated & Structured Data;The MAESTRA project (http://maestra-project.eu/) addresses the ambitious taskof predicting different types of structured outputs in several challenging settings, suchas semi-supervised learning, mining data streams and mining network data. It developsmachine learning methods that work in each of these settings, as well as combinationsthereof. The techniques developed are applied to problems from the area of biology andbioinformatics, sensor data analysis, multimedia annotation and retrieval, and socialnetwork analysis. The talk will give an introduction to the project and the topicsit addresses, an overview of the results of the project, and a detailed description ofselected techniques and applications: Semi-supervised learning for structured-outputprediction (SOP) and SOP on data streams will be discussed for the task of multitargetregression (MTR), as well as applications of MTR for the annotation/retrievalof images.;Saso Dzeroski;http://editions-rnti.fr/render_pdf.php?p1&p=1002149;http://editions-rnti.fr/render_pdf.php?p=1002149;0;Reims;49.258375;4.031742
1151;Revue des Nouvelles Technologies de l'Information;EGC;2016;LibRe: Protocole de gestion de la cohérence dans les systèmes de stockage distribués;Nous présentons dans ce papier un protocole de gestion de la cohérenceappelé LibRe adapté aux systèmes de stockage orientés Cloud (telles queles bases de données NoSQL). Ce protocole garantit l'accès à la donnée la plusrécente tout en ne consultant qu'une seule réplique. Cet algorithme est évaluépar simulation et est également implémenté au sein du système de stockage Cassandra.Les résultats de ces expérimentations ont démontré l'efficacité de notreapproche.;Raja Chiky, Sathya Prabhu Kumar, Sylvain Lefebvre, Eric Gressier-Soudan;http://editions-rnti.fr/render_pdf.php?p1&p=1002171;http://editions-rnti.fr/render_pdf.php?p=1002171;1;Reims;49.258376;4.031743
1152;Revue des Nouvelles Technologies de l'Information;EGC;2016;Manipulation interactive d'ensemble de motifs : application aux parcours hospitaliers;Dans cette démonstration, nous proposons une application de visualisationdes résultats de la fouille de données séquentielles. Pour illustrer le fonctionnementde cette application, nous avons utilisé des données PMSI hospitalières,plus précisément dans le cas de l'infarctus du myocarde (IM). Les résultatsobtenus ont été soumis à un spécialiste pour discussion et validation.;Yves Mercadier, Jessica Pinaire, Jérôme Azé, Sandra Bringay, Maguelonne Teisseire;http://editions-rnti.fr/render_pdf.php?p1&p=1002200;http://editions-rnti.fr/render_pdf.php?p=1002200;11;Reims;49.258377;4.031744
1153;Revue des Nouvelles Technologies de l'Information;EGC;2016;Nettoyage de données guidé par la sémantique inter-colonnes;Today, the volume of unstructured and heterogeneous data is exploding, coming from multiplesources with different levels of quality. Therefore, it is very likely to manipulate datawithout knowledge about their structures and their semantics. In fact, the meta-data may beinsufficient or totally absent. Data anomalies may be due to the poverty of their semantic descriptions,or even the absence of their descriptions. We propose an approach to understandbetter the semantics and the structure of the data. It helps to correct the intra-column anomalies(homogenization) and then the inter-columns ones caused by the violation of semanticdependencies.;Houda Zaidi, Faouzi Boufarès, Yann Pollet;http://editions-rnti.fr/render_pdf.php?p1&p=1002225;http://editions-rnti.fr/render_pdf.php?p=1002225;11;Reims;49.258378;4.031745
1154;Revue des Nouvelles Technologies de l'Information;EGC;2016;Nouveaux algorithmes de fouilles de données relationnelles de clowdflows;Clowdflows est un logiciel open source qui permet à un utilisateur deréaliser des processus entiers de fouille de données à partir d'un navigateur etd'une connexion internet. Les calculs sont réalisés dans le “nuage”, c'est-à-direde façon transparente sur plusieurs serveurs exécutant les calculs ou hébergeantles données. Dans cet article, nous rappelons les points forts de clowdflows etnous présentons trois familles d'algorithmes de fouille de données relationnellesque nous venons d'y intégrer. En effet clowdflows est la seule plateforme webpermettant d'exécuter, voire comparer, plusieurs techniques de fouille de donnéesrelationnelles, souvent appelée programmation logique inductive.;Nicolas Lachiche, Alain Shakour;http://editions-rnti.fr/render_pdf.php?p1&p=1002207;http://editions-rnti.fr/render_pdf.php?p=1002207;11;Reims;49.258379;4.031746
1155;Revue des Nouvelles Technologies de l'Information;EGC;2016;Nouvelle méthode de calcul de la réputation dans les forums de santé;De plus en plus de forums, tels que Slashdot ou Stack Exchange, proposentdes systèmes de réputations qui se basent sur le vote collaboratif. Lesutilisateurs peuvent ainsi donner un score à chaque message posté selon sa pertinenceou son utilité. Cependant, ces fonctionnalités de vote sont rarement utiliséesdans de nombreuses communautés en ligne tels que les forums de santé.Dans ces forums, les utilisateurs préfèrent poster un nouveau message exprimantde l'accord ou du remerciement vis à vis des messages pertinents plutôtque de cliquer sur un bouton de vote. Dans ce travail, nous proposons d'utiliserces formes implicites d'expression de la confiance pour estimer la réputation desutilisateurs dans les forums de santé.;Amine Abdaoui;http://editions-rnti.fr/render_pdf.php?p1&p=1002173;http://editions-rnti.fr/render_pdf.php?p=1002173;7;Reims;49.258380;4.031747
1156;Revue des Nouvelles Technologies de l'Information;EGC;2016;Observations sur les distributions latentes aux matrices laplaciennes de graphes;L'algorithme de clustering spectral permet en principe d'extraire desclusters de formes arbitraires à partir de données numériques. Cette propriété acontribué à sa popularité, et même si ses bases théoriques sont établies depuisplus d'une décennie, des variantes en ont été proposées jusqu'à récemment. Sonfonctionnement repose sur une transformation vers un espace latent dans lequeldes formes de clusters arbitraires sont converties en structures faciles à traiterpar un algorithme tel que k-means. Toutefois, les distributions dans cet espacelatent n'ont été que peu discutées, beaucoup d'auteurs supposant que les propriétésprédites par la théorie sont vérifiées. Cet article propose alternativementune approche qualitative pour vérifier si cette structure idéale est effectivementobtenue en pratique. Le travail consiste également à identifier les paramètresde variabilité commandant à la transformation vers l'espace latent, via un étatde l'art synthétique de la théorie sous-jacente au clustering spectral. Les observationstirées de nos expériences permettent d'identifier les combinaisons deparamètres efficaces, et les conditions de cette efficacité.;Pierrick Bruneau, Benoît Otjacques;http://editions-rnti.fr/render_pdf.php?p1&p=1002159;http://editions-rnti.fr/render_pdf.php?p=1002159;2;Reims;49.258381;4.031748
1157;Revue des Nouvelles Technologies de l'Information;EGC;2016;PersoRec : un système personnalisé de recommandations pour les folksonomies basé sur les concepts quadratiques;Nous proposons un nouveau système appelé PersoRec afin de personnaliserles recommandations (d'amis, de tags ou de ressources) faites aux utilisateursdans les folksonomies. La personnalisation des recommandations estréalisée en prenant en compte le profil des utilisateurs. Cette nouvelle donnéepermet de proposer aux utilisateurs des tags ou/et ressources plus adaptées àleurs besoins. En plus du profil des utilisateurs, nous avons recours à leur historiquede partage de tags et de ressources dans le but de regrouper les utilisateursayant partagé des tags et des ressources en commun tout en ayant des profilséquivalents (i.e., des structures appelées concepts quadratiques). Ces deux donnéesprises en compte au moment du processus de recommandation a permisd'améliorer la qualité des recommandations faites aux utilisateurs. PersoRec estdonc capable de générer une recommandation personnalisée pour chaque utilisateurselon le mode de recommandation qu'il désire (recommandation d'amis,de tags ou de ressources) et selon le profil qu'il possède.;Mohamed Nader Jelassi, Sadok Ben Yahia, Engelbert Mephu Nguifo;http://editions-rnti.fr/render_pdf.php?p1&p=1002205;http://editions-rnti.fr/render_pdf.php?p=1002205;11;Reims;49.258382;4.031749
1158;Revue des Nouvelles Technologies de l'Information;EGC;2016;Plongement de métrique pour le calcul de similarité sémantique à l'échelle;Nous explorons le plongement de la métrique de plus court chemindans l'hypercube de Hamming, dans l'objectif d'améliorer les performances desimilarité sémantique dans Wordnet (Subercaze et al. (2015)). Nous montronsque bien qu'un plongement isométrique est impossible en pratique, nous obtenonsde très bons plongements non isométriques. Nous obtenons une améliorationdes performances de trois ordres de grandeur pour le calcul de la similaritéde Leacock et Chodorow (LCH).;Julien Subercaze, Christophe Gravier, Frédérique Laforest;http://editions-rnti.fr/render_pdf.php?p1&p=1002163;http://editions-rnti.fr/render_pdf.php?p=1002163;2;Reims;49.258383;4.031750
1159;Revue des Nouvelles Technologies de l'Information;EGC;2016;Prédiction de la qualité dans les plateformes collaboratives : une approche générique par les graphes hétérogènes;La qualité des contenus sur les plateformes collaboratives est très hétérogène.Dans la littérature scientifique, les algorithmes d'analyse structurelleappliqués à la tâche de détection de contenu de qualité reposent généralement surdes graphes définis à partir d'un seul type de noeuds et de relations. Pourtant lesgraphes sur lesquels reposent ces récentes plateformes présentent de nombreusessémantiques de noeuds et relations différentes, e.g., producteurs/consommateurs,questions/réponses, etc. Ces solutions souffrent d'un manque de généricité et nepeuvent s'adapter facilement à l'évolution des plateformes. Nous proposons unemodélisation générique de ces platformes par les graphes hétérogènes pouvantintégrer automatiquement de nouvelles sémantiques de noeuds et de relations. Unalgorithme de prédiction de qualité des contenus reposant sur ce modèle est proposé.Nous montrons qu'il généralise plusieurs travaux de la littérature. Enfin,en intégrant certaines relations inter-utilisateurs, nous montrons que notre solution,évaluée surWikipedia et Stack Exchange, améliore la tâche de détection decontenu de qualité.;Baptiste de La Robertie, Yoann Pitarch, Olivier Teste;http://editions-rnti.fr/render_pdf.php?p1&p=1002175;http://editions-rnti.fr/render_pdf.php?p=1002175;7;Reims;49.258384;4.031751
1160;Revue des Nouvelles Technologies de l'Information;EGC;2016;Recherche de groupes parallèles en classification non-supervisée;"Dans cet article, nous nous intéressons à une situation de classificationnon supervisée dans laquelle nous souhaitons imposer une ""forme"" commune àtous les clusters. Dans cette approche, la ""forme"" commune sera caractérisée parun hyperplan qui sera le même pour tous les groupes, à une translation près.Les points sont donc supposés être distribués autour d'hyperplans parallèles. Lafonction objectif utilisée peut naturellement s'exprimer comme la minimisationde la somme des distances de chaque point à son hyperplan. Comme pour le casde k-means, la résolution est effectuée par l'alternance de phases d'affectationde chaque point à l'hyperplan le plus proche et de phases de calcul de l'hyperplanqui ajuste au mieux l'ensemble des points qui lui sont affectés. L'objectifétant d'obtenir des hyperplans parallèles, cette phase de calcul est menée simultanémentpour tous les hyperplans, par une méthode de régression.";Lionel Martin, Matthieu Exbrayat, Teddy Debroutelle, Aladine Chetouani, Sylvie Treuillet, Sébastien Jesset;http://editions-rnti.fr/render_pdf.php?p1&p=1002157;http://editions-rnti.fr/render_pdf.php?p=1002157;2;Reims;49.258385;4.031752
1161;Revue des Nouvelles Technologies de l'Information;EGC;2016;Régression logistique pour la classification d'images à grande échelle;Nous présentons un nouvel algorithme parallèle de régression logistique(PAR-MC-LR) pour la classification d'images à grande échelle. Nous proposonsplusieurs extensions de l'algorithme original de régression logistique àdeux classes pour en développer une version efficace pour les grands ensemblesde données d'images avec plusieurs centaines de classes. Nous présentons unnouvel algorithme LR-BBatch-SGD de descente de gradient stochastique de régressionlogistique en batch équilibré avec un apprentissage parallèle (approcheun contre le reste) multi-classes sur de multiples coeurs. Les résultats expérimentauxsur des ensembles de données d'ImageNet montrent que notre algorithmeest efficace comparés aux algorithmes de classification linéaires de l'état de l'art.;Thanh-Nghi Do, François Poulet;http://editions-rnti.fr/render_pdf.php?p1&p=1002182;http://editions-rnti.fr/render_pdf.php?p=1002182;6;Reims;49.258386;4.031753
1162;Revue des Nouvelles Technologies de l'Information;EGC;2016;Relaxation des Requêtes Skyline : Une Approche Centrée Utilisateur;Les requêtes skyline constituent un outil puissant pour l'analyse dedonnées multidimensionnelles et la décision multicritère. En pratique, le calculdu skyline peut conduire à deux scénarios : soit (i) un nombre important d'objetssont retournés, soit (ii) un nombre réduit d'objets sont retournés, ce qui peut êtreinsuffisant pour la prise de décisions. Dans cet article, nous abordons le secondproblème et proposons une approche permettant de le traiter. L'idée consiste àrendre le skyline plus permissive en lui ajoutant les objets, non skyline, les pluspréférés. L'approche s'appuie sur une nouvelle relation de dominance floue appelée«Much Preferred». Un algorithme efficace pour calculer le skyline relaxéest proposé. Une série d'expériences sont menées pour démontrer la pertinencede l'approche et la performance de l'algorithme proposé.;Djamal Belkasmi, Allel HadjAli, Hamid Azzoune;http://editions-rnti.fr/render_pdf.php?p1&p=1002188;http://editions-rnti.fr/render_pdf.php?p=1002188;4;Reims;49.258387;4.031754
1163;Revue des Nouvelles Technologies de l'Information;EGC;2016;Requêtes discriminantes pour l'exploration des données;À l'ère du Big Data, les profils d'utilisateurs deviennent de plus enplus diversifiés et les données de plus en plus complexes, rendant souvent trèsdifficile l'exploration des données. Dans cet article, nous proposons une techniquede réécriture de requêtes pour aider les analystes à formuler leurs interrogations,pour explorer rapidement et intuitivement les données. Nous introduisonsles requêtes discriminantes, une restriction syntaxique de SQL, avecune condition de sélection qui dissocie des exemples positifs et négatifs. Nousconstruisons un ensemble de données d'apprentissage dont les exemples positifscorrespondent aux résultats souhaités par l'analyste, et les exemples négatifs àceux qu'il ne veut pas. En utilisant des techniques d'apprentissage automatique,la requête initiale est reformulée en une nouvelle requête, qui amorce un processusitératif d'exploration des données. Nous avons implémenté cette idée dansun prototype (iSQL) et nous avons mené des expérimentations dans le domainede l'astrophysique.;Julien Cumin, Jean-Marc Petit, Fabien Rouge, Vasile-Marian Scuturici, Christian Surace, Sabine Surdu;http://editions-rnti.fr/render_pdf.php?p1&p=1002170;http://editions-rnti.fr/render_pdf.php?p=1002170;1;Reims;49.258388;4.031755
1164;Revue des Nouvelles Technologies de l'Information;EGC;2016;SAFFIET : un système d'extraction de règles d'associations spatiales et fonctionnelles dans les séries de données géographiques;Nous partons de l'hypothèse que les dynamiques spatiales et l'évolutiondes usages des objets géographiques peuvent en partie être explicitées(voire anticipées) par leurs différentes évolutions précédentes et les configurationsspatiales dans lesquelles ils se situent. Aussi afin d'analyser et comprendreles changements de fonction des objets géographiques au cours du temps, et endéduire un modèle prospectif et puis prédictif, nous proposons l'outil SAFFIETqui exploite la recherche des motifs fréquents et des règles d'associations, pourextraire des règles d'évolution régissant les dynamiques spatiales.;Asma Gharbi, Cyril de Runz, Sami Faiz, Herman Akdag;http://editions-rnti.fr/render_pdf.php?p1&p=1002202;http://editions-rnti.fr/render_pdf.php?p=1002202;11;Reims;49.258389;4.031756
1165;Revue des Nouvelles Technologies de l'Information;EGC;2016;SArEM: Un méta-modèle pour la spécification des processus d'extraction d'architectures logicielles;We propose a meta-model, called SArEM, that specifies the basic elements of the softwarearchitecture extraction. SArEM serves as a tool to compare the different software architectureextraction approaches that aim to extract a system architecture from the source code.;Mira Abboud, Hala Naja, Mourad Oussalah, Mohamad Dbouk;http://editions-rnti.fr/render_pdf.php?p1&p=1002211;http://editions-rnti.fr/render_pdf.php?p=1002211;11;Reims;49.258390;4.031757
1166;Revue des Nouvelles Technologies de l'Information;EGC;2016;Segmentation comportementale à l'aide des réseaux communautaires;La mise en place d'actions marketing efficaces passe par la segmentationde la clientèle. C'est-à-dire que les clients sont regroupés en ensembles homogènesen fonction de leurs habitudes de consommation, ce qui rend possibleles actions ciblées. Ces dernières, en personnalisant l'offre permettent d'obtenirdes taux de transformation plus importants et de meilleures ventes.Dans cet article, une méthode originale de segmentation comportementale de laclientèle est présentée. Elle permet de visualiser les segments de clients à traversdes réseaux de communautés et de déceler aisément des mutations soudainesou graduelles dans les comportements de quelques individus ou d'un ensembleplus important. L'analyste bénéficie alors d'une meilleure visibilité et peut adapterl'offre à tout moment.;Gaël Bardury, Teddy Boula;http://editions-rnti.fr/render_pdf.php?p1&p=1002176;http://editions-rnti.fr/render_pdf.php?p=1002176;7;Reims;49.258391;4.031758
1167;Revue des Nouvelles Technologies de l'Information;EGC;2016;Sélection topologique de variables dans un contexte de discrimination;"En apprentissage automatique, la présence d'un grand nombre de variablesexplicatives conduit à une plus grande complexité des algorithmes et àune forte dégradation des performances des modèles de prédiction. Pour cela,une sélection d'un sous-ensemble optimal discriminant de ces variables s'avèrenécessaire. Dans cet article, une approche topologique est proposée pour la sélectionde ce sous-ensemble optimal. Elle utilise la notion de graphe de voisinagepour classer les variables par ordre de pertinence, ensuite, une méthode pas à pasde type ascendante ""forward"" est appliquée pour construire une suite de modèlesdont le meilleur sous-ensemble est choisi selon son degré d'équivalence topologiquede discrimination. Pour chaque sous-ensemble, le degré d'équivalence estmesuré en comparant la matrice d'adjacence induite par la mesure de proximitéchoisie à celle induite par la ""meilleure"" mesure de proximité discriminante ditede référence. Les performances de cette approche sont évaluées à l'aide de donnéessimulées et réelles. Des comparaisons de sélection de variables en discriminationavec une approche métrique montrent une bien meilleure sélection àpartir de l'approche topologique proposée.";Fatima-Zahra Aazi, Rafik Abdesselam;http://editions-rnti.fr/render_pdf.php?p1&p=1002162;http://editions-rnti.fr/render_pdf.php?p=1002162;2;Reims;49.258392;4.031759
1168;Revue des Nouvelles Technologies de l'Information;EGC;2016;Slider : un Raisonneur Incrémental Évolutif;The main drawbacks of current reasoning methods over ontologies are they struggle toprovide scalability for large datasets. The batch processing reasoners who provide the bestscalability so far are unable to infer knowledge from evolving data. We contribute to solvingthese problems by introducing Slider, an efficient incremental reasoner. Slider exhibits a performanceimprovement by more than a 70% compared to the OWLIM-SE reasoner. Slider isconceived to handle expanding data from streams with a growing background knowledge base.It natively supports df and RDFS, and its architecture allows to extend it to more complexfragments with a minimal effort.;Jules Chevalier, Julien Subercaze, Christophe Gravier, Frédérique Laforest;http://editions-rnti.fr/render_pdf.php?p1&p=1002219;http://editions-rnti.fr/render_pdf.php?p=1002219;11;Reims;49.258393;4.031760
1169;Revue des Nouvelles Technologies de l'Information;EGC;2016;Structures de haies dans un paysage agricole : une étude par chemin de Hilbert adaptatif et chaînes de Markov;Dans cet article nous présentons une approche couplant une courberemplissant l'espace et une chaîne de Markov pour analyser des données spatialesconcernant la localisation de haies. Du fait de l'hétérogénéité spatiale desdonnées, nous utilisons une courbe adaptative de Hilbert qui permet de linéariserl'espace en s'ajustant localement à la densité des données. Pour ensuite exploiterla séquence produite, il est nécessaire de caractériser la distance entre un pointet son prédecesseur sur la courbe ainsi que la densité locale. Nous proposonsde calculer un temps d'accès à un point à partir du point précédent en utilisantla notion de profondeur de découpe. Cette variable, couplée avec les variablescaractérisant les haies est ensuite analysée avec un modèle de Markov. Nousprésentons et interprétons les résultats obtenus sur un jeu de données d'environ10000 segments de haies d'une zone de la Basse vallée de la Durance.;Sébastien Da Silva, Florence Le Ber, Claire Lavigne;http://editions-rnti.fr/render_pdf.php?p1&p=1002179;http://editions-rnti.fr/render_pdf.php?p=1002179;1;Reims;49.258394;4.031761
1170;Revue des Nouvelles Technologies de l'Information;EGC;2016;Supervision de comportements remarquables d'objets mobiles à partir du suivi et de l'analyse de leurs trajectoires spatio-temporelles;We propose a new generic knowledge model dedicated to the consideration of temporaland spatial dimensions of moving objects. We extend usual approaches to meet the specificityof the representation of moving objects and their trajectories. An application on shipping andboat trip scenarii is done.;Mojdeh Soltanmohammadi, Isabelle Mougenot, Thérèse Libourel, Christophe Fagot;http://editions-rnti.fr/render_pdf.php?p1&p=1002230;http://editions-rnti.fr/render_pdf.php?p=1002230;11;Reims;49.258395;4.031762
1171;Revue des Nouvelles Technologies de l'Information;EGC;2016;TOM: A library for topic modeling and browsing;In this paper, we present TOM (TOpic Modeling), a Python libraryfor topic modeling and browsing. Its objective is to allow for an efficient analysisof a text corpus from start to finish, via the discovery of latent topics. To thisend, TOM features advanced functions for preparing and vectorizing a text corpus.It also offers a unified interface for two topic models (namely LDA usingeither variational inference or Gibbs sampling, and NMF using alternating leastsquarewith a projected gradient method), and implements three state-of-the-artmethods for estimating the optimal number of topics to model a corpus. What ismore, TOM constructs an interactive Web-based browser that makes exploringa topic model and the related corpus easy.;Adrien Guille, Edmundo-Pavel Soriano-Morales;http://editions-rnti.fr/render_pdf.php?p1&p=1002199;http://editions-rnti.fr/render_pdf.php?p=1002199;11;Reims;49.258396;4.031763
1172;Revue des Nouvelles Technologies de l'Information;EGC;2016;Topic modeling and hypergraph mining to analyze the EGC conference history;Dans le cadre du défi proposé à l'édition 2016 de la conférence EGC, nous exploitons lesarticles qui y ont été publiés de 2004 à 2015, avec pour but d'expliquer sa structure et sonévolution. A partir des thématiques latentes découvertes et d'autres propriétés des articles (e.g.auteurs, affiliations), nous mettons en lumière des caractéristiques intéressantes des structuresthématique et collaborative d'EGC. A l'aide d'une méthode d'extraction d'itemsets dans leshyper-graphes nous mettons aussi en avant des liens latents entre auteurs ou entre thématiques.De plus, nous proposons des recommandations d'auteurs ou de thématiques. Enfin, nous décrivonsune interface Web pour explorer les connaissances découvertes.;Adrien Guille, Edmundo-Pavel Soriano-Morales, Ciprian-Octavian Truica;http://editions-rnti.fr/render_pdf.php?p1&p=1002191;http://editions-rnti.fr/render_pdf.php?p=1002191;9;Reims;49.258397;4.031764
1173;Revue des Nouvelles Technologies de l'Information;EGC;2016;Towards generic and efficient constraint-based mining, a constraint programming approach;In today's data-rich world, pattern mining techniques allow us to extract knowledge fromdata. However, such knowledge can take many forms and often depends on the application athand. This calls for generic techniques that can be used in a wide range of settings. In recentyears, constraint programming has been shown to offer a generic methodology that fits manypattern mining settings, including novel ones. Existing constraint programming solvers do notscale very well though. In this talk, I will review different ways in which this limitation hasbeen overcome. Often, this is through principled integration of techniques and data structuresfrom pattern mining into the constraint solvers.;Tias Guns;http://editions-rnti.fr/render_pdf.php?p1&p=1002151;http://editions-rnti.fr/render_pdf.php?p=1002151;0;Reims;49.258398;4.031765
1174;Revue des Nouvelles Technologies de l'Information;EGC;2016;Transmute : un outil interactif pour assister l'extraction de connaissances à partir de traces;Alors que l'extraction de connaissances à partir de données(ecd) est un processus qualifié d'interactif et d'itératif, l'interactivité desoutils est souvent limitée et son étude est relativement récente. Elle estpourtant déterminante lors de l'interprétation pour choisir les motifs quideviendront des connaissances. Nous proposons Transmute, un outild'assistance à l'interprétation dans le processus d'ecd, dans le cadre dela recherche d'épisodes séquentiels à partir de traces. La phase d'interprétationest itérative et à chaque itération les résultats de la fouille sontmis à jour dynamiquement en fonction des interactions avec l'analyste.Des outils de visualisation et des mesures de qualité indépendantes dudomaine permettent de caractériser l'intérêt des motifs à interpréter pourfaciliter leur choix et accompagner le travail de l'analyste afin de l'aider àse focaliser plus rapidement sur les motifs potentiellement intéressants.;Pierre-Loup Barazzutti, Amélie Cordier, Béatrice Fuchs;http://editions-rnti.fr/render_pdf.php?p1&p=1002201;http://editions-rnti.fr/render_pdf.php?p=1002201;11;Reims;49.258399;4.031766
1175;Revue des Nouvelles Technologies de l'Information;EGC;2016;Un cadre collaboratif pour la segmentation et la classification d'images de télédétection;Dans cet article nous présentons CoSC, un cadre collaboratif pour lasegmentation et la classification d'images de télédétection permettant d'extraireles objets d'une classe thématique donnée. Le processus de collaboration estguidé par la qualité des données évaluée par des critères d'homogénéité ainsique des critères implicitement liés à la sémantique des objets afin d'extraire uneclasse thématique donnée. Nos expériences montrent que CoSC atteint des bonsrésultats en termes de classification, et améliore notablement la segmentation del'image de manière globale.;Andrés Troya-Galvis, Pierre Gançarski, Laure Berti-Equille;http://editions-rnti.fr/render_pdf.php?p1&p=1002181;http://editions-rnti.fr/render_pdf.php?p=1002181;6;Reims;49.258400;4.031767
1176;Revue des Nouvelles Technologies de l'Information;EGC;2016;Un outil d'exploration pour le Défi EGC 2016;Dans le cadre du défi EGC 2016, nous avons développé une applicationweb pour explorer les données décrivant les articles publiés depuis 2004 lorsdes conférences EGC. L'outil permet de découvrir les thèmes importants qui ontété abordés dans ces papiers. De plus, il permet de déterminer automatiquementles articles sémantiquement similaires à des thèmes donnés.;Olivier Parisot, Yoanne Didry, Thomas Tamisier;http://editions-rnti.fr/render_pdf.php?p1&p=1002197;http://editions-rnti.fr/render_pdf.php?p=1002197;9;Reims;49.258401;4.031768
1177;Revue des Nouvelles Technologies de l'Information;EGC;2016;Un protocole d'expérimentation sur les propriétés graphémiques avec l'algorithme SOM;Nous présentons une recherche sur la distribution et la classificationnon-supervisée des graphèmes. Nous visons à réduire l'écart entre les résultatsde recherches récentes qui montrent la capacité des algorithmes d'apprentissageet de classification non-supervisée pour détecter les propriétés de phonèmes, etles possibilités actuelles de la représentation textuelle d'Unicode. Nos procéduresdoivent assurer la reproductibilité des expériences et garantir que l'informationrecherchée n'est pas implicitement présente dans le pré-traitement desdonnées. Notre approche est capable de catégoriser correctement de potentielsgraphèmes, ce qui montre que les propriétés phonologiques sont présentes dansles données textuelles, et peuvent être automatiquement extraites à partir desdonnées textuelles brutes en Unicode, sans avoir besoin de les traduire en représentationsphonologiques.;Otman Manad, Nourredine Aliane, Gilles Bernard;http://editions-rnti.fr/render_pdf.php?p1&p=1002160;http://editions-rnti.fr/render_pdf.php?p=1002160;2;Reims;49.258402;4.031769
1178;Revue des Nouvelles Technologies de l'Information;EGC;2016;Un regard lexico-scientométrique sur le défi EGC 2016;Depuis 2001, les conférences EGC ont rassemblé 1 782 chercheursautour de l'extraction et la gestion de connaissances. En 2016, l'associationEGC réfléchit à son histoire et se projette en lançant un défi à sa communauté.Que peut-on révéler sur la communauté EGC via des approches développées enEGC ? Notre étude lexico-scientométrique apporte un éclairage sur les thématiquesdu congrès, les lieux de publication investis par ses auteurs, ou encore lesauteurs sollicitables comme évaluateurs. Les résultats sont intégrés à un site websous-tendu par un système d'information décisionnel.;Guillaume Cabanac, Gilles Hubert, Hong Diep Tran, Cécile Favre, Cyril Labbé;http://editions-rnti.fr/render_pdf.php?p1&p=1002194;http://editions-rnti.fr/render_pdf.php?p=1002194;9;Reims;49.258403;4.031770
1179;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une approche basée sur des données mixtes – mesures et estimations – pour la détection de défaillances d'un système robotisé;Mettre en place un dispositif de détection de pannes représente denos jours l'un des défis majeurs pour les constructeurs des systèmes robotisés.Le processus de détection nécessite l'utilisation d'un certain nombre de capteursafin de surveiller le fonctionnement de ces systèmes. Or, le coût ainsi queles contraintes liées à la mise en place de ces capteurs conduisent souvent lesconcepteurs à optimiser leurs nombres, ce qui mène à un manque de mesuresnécessaires pour la détection de défaillances. L'une des méthodes pour comblerce manque est d'estimer les paramètres non mesurables à partir d'un modèlemathématique décrivant la dynamique du système réel. Cet article présente uneapproche basée sur des données mixtes (données mesurées et données estimées)pour la détection de défaillances dans les systèmes robotisés. Cette détection esteffectuée en utilisant un classifieur de type arbre de décision. Les données utiliséespour son apprentissage proviennent des mesures prises sur le système réel.Ces données sont ensuite enrichies par des données estimées en provenance d'unobservateur basé sur un modèle analytique. Cet enrichissement sous forme d'attributssupplémentaires a pour but d'augmenter la connaissance du classifieursur le fonctionnement du système et par conséquent améliorer le taux de bonnedétection de défaillances. Une expérience sur un système d'actionnement d'unsiège robotisé, montrant l'intérêt de notre approche, sera présentée à la fin del'article.;Rabah Mazouzi, Rabih Taleb, Lynda Seddiki, Cyril de Runz, Kevin Guelton, Herman Akdag;http://editions-rnti.fr/render_pdf.php?p1&p=1002169;http://editions-rnti.fr/render_pdf.php?p=1002169;1;Reims;49.258404;4.031771
1180;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une approche combinée pour l'enrichissement d'ontologie à partir de textes et de données du LOD;Cet article porte sur l'étiquetage automatique de documents décrivantdes produits, avec des concepts très spécifiques traduisant des besoins précisd'utilisateurs. La particularité du contexte est qu'il se confronte à une triple difficulté: 1) les concepts utilisés pour l'étiquetage n'ont pas de réalisations terminologiquesdirectes dans les documents, 2) leurs définitions formelles ne sontpas connues au départ, 3) toutes les informations nécessaires ne sont pas forcémentprésentes dans les documents mêmes. Pour résoudre ce problème, nousproposons un processus d'annotation en deux étapes, guidé par une ontologie.La première consiste à peupler l'ontologie avec les données extraites des documents,complétées par d'autres issues de ressources externes. La deuxièmeest une étape de raisonnement sur les données extraites qui recouvre soit unephase d'apprentissage de définitions de concepts, soit une phase d'applicationdes définitions apprises. L'approche SAUPODOC est ainsi une approche originaled'enrichissement d'ontologie qui exploite les fondements du Web sémantique,en combinant les apports du LOD et d'outils d'analyse de texte, d'apprentissageautomatique et de raisonnement. L'évaluation, sur deux domaines d'application,donne des résultats de qualité et démontre l'intérêt de l'approche.;Céline Alec, Chantal Reynaud, Brigitte Safar;http://editions-rnti.fr/render_pdf.php?p1&p=1002168;http://editions-rnti.fr/render_pdf.php?p=1002168;1;Reims;49.258405;4.031772
1181;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une approche d'évolution du web de données;Sharing knowledge and data coming from different sources is one of the biggest advantageof linked data. Keeping this knowledge graph up to date may take in account both ontologyvocabularies and data since they should be consistent. Our general problem is to deal with webof data evolution in particular: We aim at modifing both levels : A-Box and T-Box.;Fatma Chamekh, Danielle Boulanger, Guilaine Talens;http://editions-rnti.fr/render_pdf.php?p1&p=1002226;http://editions-rnti.fr/render_pdf.php?p=1002226;11;Reims;49.258406;4.031773
1182;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une approche de réduction de dimensionnalité pour l'agrégation de préférences qualitatives;Nous présentons une méthode de réduction de dimensionnalité pourdes données de préférences multicritères lorsque l'espace des évaluations estun treillis distributif borné. Cette méthode vise à réduire la complexité desprocédures d'apprentissage d'un modèle d'agrégation sur des données qualitatives.Ainsi nous considérons comme modèle d'agrégation l'intégrale de Sugeno.L'apprentissage d'un tel modèle à partir de données empiriques est unproblème d'optimisation à 2n paramètres (où n est le nombre de critères considérés).La méthode de réduction que nous proposons s'appuie sur l'observationde certaines relations entre les éléments de ces données, et nous donnons despremiers résultats d'applications.;Quentin Brabant, Miguel Couceiro, Fabien Labernia, Amedeo Napoli;http://editions-rnti.fr/render_pdf.php?p1&p=1002186;http://editions-rnti.fr/render_pdf.php?p=1002186;4;Reims;49.258407;4.031774
1183;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une mesure de similarité entre phrases basée sur des noyaux sémantiques;Nous proposons une nouvelle approche pour le calcul de similarité sémantiqueentre phrases en utilisant les noyaux sémantiques qui les composent.Ces noyaux, sous la forme de triplets (sujet, verbe et objet) sont supposés porteursde l'information des phrases dont ils sont extraits. Sur la base de la comparaisonsémantique de noyaux, on extrait un ensemble d'indicateurs descriptifs.Nous utilisons ensuite un apprentissage automatique, sur un benchmark contenantdes phrases dont la similarité sémantique a été évaluée par des experts humains,afin de déterminer l'importance de chaque indicateur et de construireainsi un modèle capable de fournir une mesure de similarité sémantique entrephrases. Les expérimentations et les études comparatives, effectuées avec d'autresapproches permettant l'estimation des similarités sémantiques entre phrases,montrent les bonnes performances de notre approche. En se basant sur cette dernière,un outil de navigation sémantique est en cours de développement.;Samir Amir, Adrian Tanasescu, Djamel Abdelkader Zighed;http://editions-rnti.fr/render_pdf.php?p1&p=1002164;http://editions-rnti.fr/render_pdf.php?p=1002164;2;Reims;49.258408;4.031775
1184;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une méthode de découverte de motifs contextualisés dans les traces de mobilité d'une personne;Les traces de mobilité générées par les divers capteurs qui nous entourentpeuvent être analysées à des fins prédictives et explicatives pour répondreà divers problèmes du quotidien. Si de nombreuses méthodes ont été proposéespour décrire le comportement d'un individu de manière globale à partir destransitions entre ses différents points d'intérêts (par exemple via un modèle deMarkov), peu de travaux cherchent à l'expliquer de manière locale. Nous proposonsdans cet article une méthode qui permet d'extraire pour un individu donton a une trace de mobilité conséquente des motifs de mobilité dits contextualisés.Chaque motif est composé d'une description sur l'ensemble des visites auxdifférents points d'intérêt de l'individu qui maximise une ou plusieurs mesuresavec une sémantique particulière (le motif décrit une phase sédentaire ou exceptionnelde la mobilité de l'individu). Une expérimentation a été menée à partirde traces de mobilité de véhicules et donne des résultats encourageants.;Aimene Belfodil, Mehdi Kaytoue, Celine Robardet, Marc Plantevit, Julien Zarka;http://editions-rnti.fr/render_pdf.php?p1&p=1002156;http://editions-rnti.fr/render_pdf.php?p=1002156;3;Reims;49.258409;4.031776
1185;Revue des Nouvelles Technologies de l'Information;EGC;2016;Une méthode supervisée pour initialiser les centres des K-moyennes;Au cours des dernières années, la classification à base de clusterings'est imposée comme un sujet de recherche important. Cette approche vise àdécrire et à prédire un concept cible d'une manière simultanée. Partant du faitque le choix des centres pour l'algorithme des K-moyennes standard a un impactdirect sur la qualité des résultats obtenus, cet article vise alors à tester à quelpoint une méthode d'initialisation supervisée pourrait aider l'algorithme des Kmoyennesstandard à remplir la tâche de la classification à base des K-moyennes.;Oumaima Alaoui Ismaili, Vincent Lemaire, Antoine Cornuéjols;http://editions-rnti.fr/render_pdf.php?p1&p=1002165;http://editions-rnti.fr/render_pdf.php?p=1002165;2;Reims;49.258410;4.031777
1186;Revue des Nouvelles Technologies de l'Information;EGC;2016;Vers une approche Visual Analytics pour explorer les variantes de sujets d'un corpus;Our purpose is to implement a Visual Analytics tool for exploring topic variants in textcorpora. The overlapping bi-clustering methods extract multiple topics from the documents,but the interpretation of the results remains difficult. We make the assumption that bi-clusteroverlaps are articulation points between high-level topics, and their multiple variants and viewpoints.We propose to extract and visualize a hierarchical structure of bi-cluster overlaps, allowingto explore the corpus and to discover unsuspected viewpoints.;Nicolas Médoc, Mohammad Ghoniem, Mohamed Nadif;http://editions-rnti.fr/render_pdf.php?p1&p=1002220;http://editions-rnti.fr/render_pdf.php?p=1002220;11;Reims;49.258411;4.031778
1187;Revue des Nouvelles Technologies de l'Information;EGC;2016;Visualisation interactive de métadonnées pour aider les utilisateurs d'un logiciel de cartographie statistique à concevoir de meilleures cartes.;CD7Online est l'application SaaS de la 7ème version de Cartes &Données (C & D), le logiciel de cartographie statistique décisionnelle et de géomarketingédité par Articque. C & D permet aux utilisateurs occasionnels de réalisersimplement des cartes à partir de données statistiques et géographiques. 25ans de retours utilisateurs nous ont permis de voir que la qualité des cartes reposeen partie sur la bonne connaissance des données dont disposent les utilisateurset sur leur capacité à choisir des outils d'analyse et de représentation pertinents.Pour aider les utilisateurs à mieux comprendre leurs données et à réaliser descartes de meilleure qualité, nous avons développé une brique sémantique avecun outil de visualisation interactif permettant de visualiser les connaissances extraitesdes espaces de travail des utilisateurs. Nous décrivons ici l'applicationCD7Online ainsi que l'outil de visualisation que nous présenterons lors de ladémonstration logicielle.;Perrine Pittet;http://editions-rnti.fr/render_pdf.php?p1&p=1002203;http://editions-rnti.fr/render_pdf.php?p=1002203;11;Reims;49.258412;4.031779
