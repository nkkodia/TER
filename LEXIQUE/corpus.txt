
**** *annee_2016  *idArt_1104
Les systèmes de questions-réponses (SQR)s visent à retourner directement des réponses précises à des questions posées en langage naturel. L'extraction et le reclassement des passages sont considérés comme les tâches les plus difficiles dans un SQR typique et exigent encore un effort non trivial. Dans cet article, nous proposons une nouvelle approche pour L'extraction et le reclassement des passages en utilisant les n-grammes et SVM. Notre système d'extraction de passages basé sur la technique des n-grammes repose sur une nouvelle mesure de similarité entre un passage et une question. Les passages extraits sont ensuite réordonnés en utilisant un modèle basé sur RankSVM combinant différentes mesures de similarité afin de retourner le passage le plus pertinent pour une question donnée. Nos expériences et nos résultats étaient prometteurs et ont démontré que notre approche est concurrentielle.
**** *annee_2016  *idArt_1108
Dans cet article, nous présentons une méthodologie originale permettant de faire des analyses scientométriques basées sur trois dimensions (spatiale,temporelle et thématique) à partir d'un corpus de publications. Cette méthodologie comporte 3 étapes : (1) la préparation et la validation des données pour compléter les critères usuels tels que les noms d'auteurs, affiliation, ... par des critères spatiaux, temporels et thématiques ; (2) l'indexation des contenus des publications et métadonnées associées ; (3) l'analyse et/ou la recherche d'information multidimentionnelle. Les expérimentations sont menées sur la série de publications des conférences EGC de 2004 à 2015.
**** *annee_2016  *idArt_1109
Les articles scientifiques publiés dans les actes des conférences EGC,qui se déroulent chaque année depuis 2001, constituent la richesse de ces événements mettant en avant le fer de lance de la recherche francophone portant sur la gestion et l'extraction de connaissances. Nous nous sommes penchés sur l’analyse de ces publications scientifiques afin d'en extraire l'essence en termes de thématiques de recherches abordées. Premièrement, nous avons analysé les points communs et les spécificités des publications dans les différentes éditions de la conférence ainsi que les principales différences entre les éditions consécutives.Puis nous nous sommes intéressés à la façon dont les publications s’articulent autour des thématiques extraites et sur lesquelles nous avons essayé de visualiser une approximation sémantique. Enfin nous nous sommes intéressés à l'évolution des thématiques depuis les débuts de cette conférence et jusqu'à l 'édition 2015.
**** *annee_2016  *idArt_1113
Prendre une décision impliquant plusieurs acteurs aux objectifs divergents nécessite de considérer des informations tant qualitatives  les préférences des acteurs sur les décisions possibles  que quantitatives  les paramètres servant d'indicateurs pour les acteurs. Dans cet article nous nous intéressons à l'association de ces deux types d'approches. Le modèle qualitatif considéré est l'argumentation.Le modèle quantitatif simulant les scénarios découlant de chaque décision est la dynamique des systèmes. Cet article s'intéresse aux éléments permettant de connecter les deux formalismes. Un exemple en agroalimentaire vient en appui à cette réflexion.
**** *annee_2016  *idArt_1116
Ces dernières années de nombreuses méthodes semi-supervisées de clustering ont intégré des contraintes entre paires d'objets ou d'étiquettes de classe, afin que le partitionnement final soit en accord avec les besoins de l'utilisateur.Pourtant dans certains cas où les dimensions d'études sont clairement définies, il semble opportun de pouvoir directement exprimer des contraintes sur les attributs pour explorer des données. De plus, une telle formulation permettrait d'éviter les écueils classiques de la malédiction de la dimensionnalité et de l'interprétation des clusters. Cet article propose de prendre en compte les préférences de l'utilisateur sur les attributs afin de guider l'apprentissage de la distance pendant le clustering. Plus précisément, nous montrons comment paramétrerla distance euclidienne par une matrice diagonale dont les coefficients doivent être au plus proche des poids fixés par l'utilisateur. Cette approche permet d'ajuster le clustering pour obtenir un compromis entre les approches guidées par les données et par l'utilisateur. Nous observons que l'ajout des préférences est parfois essentiel pour atteindre un clustering de meilleure qualité.
**** *annee_2016  *idArt_1117
Nous proposons dans cet article une approche de clustering visuel semi-interactif. L'approche proposée utilise la perception visuelle pour guider l'utilisateur dans le processus interactif. Les clusters sont extraits de manière successive et itérative, puis évalués selon leur ordre d'extraction. Pour l'utilisateur,l'approche semi-interactive permet non seulement d'évaluer les classes en fonction d'un critère déterminé mais aussi d'évaluer l'influence de l’extraction d’un cluster sur ceux précédemment extraits. Un protocole de test est présenté afin de comparer cette approche avec les approches purement automatiques et purement interactives. Cet article est un résumé d'un papier accepté 1 pour un journal international.
**** *annee_2016  *idArt_1119
Le suicide devient d'année en année une problématique plus préoccupante.Les organismes de santé tels que l'OMS se sont engagés à réduire le nombre de suicides de 10% dans l'ensemble des pays membres d'ici 2020. Si le suicide est généralement un geste impulsif, il existe souvent des actes et des paroles qui peuvent révéler un mal être et représenter des signes précurseurs de prédispositions au suicide. L'objectif de cette étude est de mettre en place un système pour détecter semi-automatiquement ces comportements et ces paroles au travers des réseaux sociaux. Des travaux précédents ont proposé la classification de messages issus de Twitter suivant des thèmes liés au suicide : tristesse,blessures psychologiques, état mental, etc. Dans cette étude, nous ajoutons la dimension temporelle pour prendre en compte l'évolution de l'état des personnes monitorées. Nous avons implémenté pour cela différentes méthodes d'apprentissage dont une méthode originale de concept drift. Nous avons expérimenté avec succès cette méthode sur des données réelles issues du réseau social Facebook.
**** *annee_2016  *idArt_1120
Dans cet article, nous étudions de manière conjointe la construction et l'exploration visuelle d'une structure de classification pour de très grande base d'images. Pour garantir que la structure construite vérifiera les contraintes de taille nécessaires à sa visualisation dans une interface Web tout en reflétant les propriétés topologiques des données (clusters), nous combinons la classification hiérarchique de BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)avec la construction de graphes de voisinage : un graphe de voisinage est créé et mis à jour de manière incrémentale pour représenter les fils de chaque nœud de l'arbre. De plus, un ensemble d'images représentatives est remonté à chaque nœud interne pour guider l'utilisateur lors de l'exploration visuelle de l’arbre. L'ensemble des algorithmes utilisés sont incrémentaux pour gérer l’insertion de nouvelles images dans la collection. Nous présentons les premiers résultats sur des dizaines de milliers d'images qui peuvent être ainsi structurée sen une minute de temps de calcul. L'exploration dans l'interface est fluide grâce aux propriétés de la structure construite.
**** *annee_2016  *idArt_1122
L'analyse des données comportementales représente aujourd'hui un grand enjeu. Tout individu génère des traces d'activité et de mobilité. Lorsqu’elles sont associées aux individus, ou labels, qui les ont créées, il est possible de construire un modèle qui prédit avec précision l'appartenance d'une nouvelle trace. Sur internet, il est cependant fréquent qu'un utilisateur possède différentes identités virtuelles, ou labels doublons. Les ignorer provoque une grande réduction de la précision de l'identification. Il est ainsi question dans cet article du problème de déduplication de labels, et l'on présente une méthode originale basée sur l'exploration du treillis des classifieurs binaires. Chaque sous-ensemble de labels est classifié face à son complémentaire et des contraintes rendent possible l'identification des labels doublons en élaguant l'espace de recherche. Des expérimentations sont menées sur des données issues du jeu vidéo STARCRAFT 2.Les résultats sont de bonne qualité et encourageants.
**** *annee_2016  *idArt_1124
Dans le domaine de l'analyse de textes, l'extraction de motifs est une technique très populaire pour mettre en évidence des relations fréquentes entre les mots. De même, les techniques de topic modeling ont largement fait leurs preuves lorsqu'il s'agit de classer automatiquement des ensembles de textes partageant des thématiques similaires. Ainsi, ce papier a pour ambition de montrer l’intérêt de l'utilisation conjointe de ces deux techniques afin de mettre en évidence,sous la forme d'un graphe biparti, des mots partageant des thématiques similaires mais aussi leurs relations fréquentes, intra et inter thématiques. Les données du Défi EGC 2016 permettent de valider l'intérêt de l'approche, tout en montrant l'évolution des thématiques et des mots clés parmi les papiers de la conférence EGC sur ces onze dernières années.
**** *annee_2016  *idArt_1125
Dans ce travail, nous analysons les données concernant les articles publiés à la conférence EGC. Notre objectif est d'identifier et de comprendre les tendances en matière de collaborations. Pour ce faire, nous adoptons une modélisation descriptive, à travers une approche réseau qui consiste à générer tout d'abord le réseau de collaborations des auteurs à partir des données. Nous enrichissons ensuite les nœuds de ce réseau d'une dizaine d'attributs individuels extraits à partir des données. Enfin, nous recherchons des vues conceptuelles, une approche récente de clustering de liens, qui permet de synthétiser des réseaux en mettant en évidence les ensembles d'attributs retrouvés fréquemment liés dans le réseau. Les résultats obtenus montrent les tendances existantes dans les comportements de collaborations. Dans ce papier, nous présentons ces tendances et montrons comment elles évoluent selon différents seuils d'extraction.
**** *annee_2016  *idArt_1126
La détection de données aberrantes (outliers) consiste à détecter des observations anormales au sein des données. Durant la dernière décennie, des méthodes de détection d'outliers utilisant les motifs fréquents ont été proposées.Elles extraient dans une première phase tous les motifs fréquents, puis assignentà chaque transaction un score mesurant son degré d'aberration (en fonction du nombre de motifs fréquents qui la couvre). Dans cet article, nous proposons deux nouvelles méthodes pour calculer le score d'aberration fondé sur les motifs fréquents(FPOF). La première méthode retourne le FPOF exact de chaque transaction sans extraire le moindre motif. Cette méthode s'avère en temps polynomial par rapport à la taille du jeu de données. La seconde méthode est une méthode approchée où l'utilisateur final peut contrôler l'erreur maximale sur l’estimation du FPOF. Une étude expérimentale montre l'intérêt des deux méthodes pour les jeux de données volumineux où une approche exhaustive échoue à calculer une solution exacte. Pour un même nombre de motifs, la précision de notre méthode approchée est meilleure que celle de la méthode classique.
**** *annee_2016  *idArt_1129
De nos jours, il y a un fort intérêt pour de nouvelles méthodes d’évaluation des groupes de recherche afin de quantifier l'impact de leur travail sur toute la communauté scientifique et de tenter de prédire leurs performances dans le futur. Dans ce contexte, nous proposons une nouvelle approche hybride qui mesure la centralité d'un groupe de chercheurs publiants. Cette mesure profite de l'expressivité et de la capacité d'inférence apportées par une modélisation ontologique des groupes et des thématiques inférées, et d'une modélisation en graphe qui permet d'explorer les interactions entre ces différents groupes au fil du temps. Ce modèle permet également de détecter les groupes capables de collaborer avec d'autres tout en maintenant un haut niveau de production, et d’identifier ceux qui sont plus déterminants sur les thématiques déduites, afin de développer des collaborations de recherche plus fructueuses.
**** *annee_2016  *idArt_1130
Nous présentons dans cet article les méthodes employées et les résultats obtenus en réponse au Défi EGC 2016. Notre approche repose d'une part sur des chaînes automatiques de traitements linguistiques en français et en anglais utilisant le plus possible des ressources et outils publics et d'autre part sur un environnement d'exploration des données basé sur les systèmes d'information logiques ; ces systèmes exploitent une généralisation des treillis de concepts formels appliquée aux données attribut-valeur ou au web sémantique.
**** *annee_2016  *idArt_1131
Les technologies du web sémantique sont de plus en plus utilisées pour la gestion de flux de données. Plusieurs systèmes de traitement de flux RDF ont été proposés : C-SPARQL, CQELS, SPARQL stream, EP-SPARQL,SPARKWAVE, etc. Ces derniers étendent tous à la base, le langage d'interrogation sémantique SPARQL. Les données à l'entrée du système sont volumineuses et générées en continu à un rythme rapide et variable. De ce fait, le stockage etle traitement de la totalité du flux deviennent coûteux et le raisonnement presque impossible. Par conséquent, le recours à des techniques permettant de réduire la charge tout en conservant la sémantique des données, permet d'optimiser les traitements voire le raisonnement. Cependant, aucune des extensions de SPARQL n'inclut cette fonctionnalité. Ainsi, dans cet article, nous proposons d'étendre le système C-SPARQL pour générer des échantillons à la volée sur flux de graphes RDF. Nous ajoutons trois opérateurs d'échantillonnage (UNIFORM, RESERVOIR et CHAIN) à la syntaxe de C-SPARQL. Les expérimentations montrent la performance de notre extension en terme de temps d'exécution, et de la préservation de la sémantique des données.
**** *annee_2016  *idArt_1134
Dans cet article, nous présentons CommentsMiner, une solution d'extraction non supervisée pour l'extraction de commentaires utilisateurs. Notre approche se base sur une combinaison de techniques de fouille de sous-arbres fréquents, d'extraction de données et d'apprentissage de classement. Nos expérimentations montrent que CommentsMiner permet de résoudre le problème d'extraction de commentaires sur 84% d'un jeu de données représentatif et publiquement accessible, loin devant les techniques existantes d'extraction.
**** *annee_2016  *idArt_1135
Nous présentons une méthode d'extraction de connaissances dans des systèmes d'information pervasifs. Nous étudions l'impact du contexte (environnement)d'un utilisateur sur les applications qu'il utilise sur son smartphone.Notre proposition pour gérer la complexité des données contextuelles repose sur l'Analyse Formelle de Concepts et les treillis de Galois. Nous nous focalisons sur l'automatisation du processus d'interprétation de ces treillis, pour généraliser l'extraction de connaissances et passer à l'échelle. Nous présentons des métriques originales illustrées sur des données réelles.
**** *annee_2016  *idArt_1139
Cet article présente l'utilisation de la programmation par ensembles réponses (ASP) pour répondre à une tâche de fouille de motifs séquentiels. La syntaxe de l'ASP, proche du Prolog, en fait un langage très pertinent pour représenter des connaissances de manière aisée et ses mécanismes de résolution,basés sur des solveurs efficaces, en font une solution alternative aux approches de programmation par contraintes pour la fouille déclarative de motifs. Nous proposons un premier encodage de la tâche classique d'extraction de motifs séquentiels et de ses variantes (motifs clos et maximaux). Nous comparons les performances calculatoires de ses encodages avec une approche de programmation par contraintes. Les performances obtenues sont inférieures aux approches de programmation par contraintes, mais l'encodage purement déclaratif offre plus de perspectives d'intégration de connaissances expertes.
**** *annee_2016  *idArt_1141
L'utilisation des connaissances a priori peut fortement améliorer la classification non-supervisée. L'injection de ces connaissances sous forme de contraintes sur les données figure parmi les techniques les plus efficaces de la littérature. Cependant, la génération des contraintes est très coûteuse et demande l’intervention de l'expert ; la sémantique apportée par l'étiquetage de l'expert est aussi perdue dans ce type de techniques, seuls les contraintes sont retenues par le clustering. Dans cet article, nous proposons une nouvelle approche hybride exploitant le raisonnement à base d'ontologie pour générer automatiquement des contraintes permettant de guider et améliorer le clustering. L'utilisation d'une ontologie comme connaissance a priori a plusieurs avantages. Elle permet l'interprétation automatisée des connaissances, ajoute de la modularité dans la chaîne de traitement et améliore la qualité du clustering en prenant en compte la vision de l'utilisateur. Pour évaluer notre approche, nous l'avons appliquée à la classification d'images satellites et les résultats obtenus démontrent des améliorations notables à la fois au niveau de la qualité du clustering et au niveau de l'étiquetage sémantique des clusters sans intervention de l'expert.
**** *annee_2016  *idArt_1144
La recommandation de points d'intérêts (ou POI), est devenue un problème majeur avec l'émergence des réseaux sociaux (ou LBSN). À la différence des approches de recommandation traditionnelles, les données des LBSN présentent des caractéristiques géographique et temporelle importantes qui limitent les performances des algorithmes traditionnels existant. L'intégration de ces caractéristiques dans un unique modèle de factorisation pour augmenter la qualité de la recommandation n'a pas été un problème très étudié jusqu'à présent. Dans ce papier nous présentons GeoMF-TD, une extension d'un modèle de factorisation géographique avec des dépendances temporelles. Nos expérimentations sur un jeu de données réel montre jusqu'à 20% de gain sur la précision de la recommandation.
**** *annee_2016  *idArt_1151
Nous présentons dans ce papier un protocole de gestion de la cohérence appelé LibRe adapté aux systèmes de stockage orientés Cloud (telles que les bases de données NoSQL). Ce protocole garantit l'accès à la donnée la plus récente tout en ne consultant qu'une seule réplique. Cet algorithme est évalué par simulation et est également implémenté au sein du système de stockage Cassandra.Les résultats de ces expérimentations ont démontré l'efficacité de notre approche.
**** *annee_2016  *idArt_1155
De plus en plus de forums, tels que Slashdot ou Stack Exchange, proposent des systèmes de réputations qui se basent sur le vote collaboratif. Les utilisateurs peuvent ainsi donner un score à chaque message posté selon sa pertinence ou son utilité. Cependant, ces fonctionnalités de vote sont rarement utilisées dans de nombreuses communautés en ligne tels que les forums de santé.Dans ces forums, les utilisateurs préfèrent poster un nouveau message exprimant de l'accord ou du remerciement vis à vis des messages pertinents plutôt que de cliquer sur un bouton de vote. Dans ce travail, nous proposons d'utiliser ces formes implicites d'expression de la confiance pour estimer la réputation des utilisateurs dans les forums de santé.
**** *annee_2016  *idArt_1156
L'algorithme de clustering spectral permet en principe d'extraire des clusters de formes arbitraires à partir de données numériques. Cette propriété a contribué à sa popularité, et même si ses bases théoriques sont établies depuis plus d'une décennie, des variantes en ont été proposées jusqu'à récemment. Son fonctionnement repose sur une transformation vers un espace latent dans lequel des formes de clusters arbitraires sont converties en structures faciles à traiter par un algorithme tel que k-means. Toutefois, les distributions dans cet espace latent n'ont été que peu discutées, beaucoup d'auteurs supposant que les propriétés prédites par la théorie sont vérifiées. Cet article propose alternativement une approche qualitative pour vérifier si cette structure idéale est effectivement obtenue en pratique. Le travail consiste également à identifier les paramètres de variabilité commandant à la transformation vers l'espace latent, via un état de l'art synthétique de la théorie sous-jacente au clustering spectral. Les observations tirées de nos expériences permettent d'identifier les combinaisons de paramètres efficaces, et les conditions de cette efficacité.
**** *annee_2016  *idArt_1158
Nous explorons le plongement de la métrique de plus court chemin dans l'hypercube de Hamming, dans l'objectif d'améliorer les performances de similarité sémantique dans Wordnet (Subercaze et al. (2015)). Nous montrons que bien qu'un plongement isométrique est impossible en pratique, nous obtenons de très bons plongements non isométriques. Nous obtenons une amélioration des performances de trois ordres de grandeur pour le calcul de la similarité de Leacock et Chodorow (LCH).
**** *annee_2016  *idArt_1159
La qualité des contenus sur les plate-formes collaboratives est très hétérogène.Dans la littérature scientifique, les algorithmes d'analyse structurelle appliqués à la tâche de détection de contenu de qualité reposent généralement sur des graphes définis à partir d'un seul type de nœuds et de relations. Pourtant les graphes sur lesquels reposent ces récentes plate-formes présentent de nombreuses sémantiques de nœuds et relations différentes, e.g., producteurs/consommateurs,questions/réponses, etc. Ces solutions souffrent d'un manque de généricité et ne peuvent s'adapter facilement à l'évolution des plate-formes. Nous proposons une modélisation générique de ces plate-formes par les graphes hétérogènes pouvant intégrer automatiquement de nouvelles sémantiques de nœuds et de relations. Un algorithme de prédiction de qualité des contenus reposant sur ce modèle est proposé.Nous montrons qu'il généralise plusieurs travaux de la littérature. Enfin,en intégrant certaines relations inter-utilisateurs, nous montrons que notre solution,évaluée sur Wikipedia et Stack Exchange, améliore la tâche de détection de contenu de qualité.
**** *annee_2016  *idArt_1161
Nous présentons un nouvel algorithme parallèle de régression logistique(PAR-MC-LR) pour la classification d'images à grande échelle. Nous proposons plusieurs extensions de l'algorithme original de régression logistique à deux classes pour en développer une version efficace pour les grands ensembles de données d'images avec plusieurs centaines de classes. Nous présentons un nouvel algorithme LR-BBatch-SGD de descente de gradient stochastique de régression logistique en batch équilibré avec un apprentissage parallèle (approche un contre le reste) multi-classes sur de multiples cœurs. Les résultats expérimentaux sur des ensembles de données d'ImageNet montrent que notre algorithme est efficace comparés aux algorithmes de classification linéaires de l'état de l'art.
**** *annee_2016  *idArt_1162
Les requêtes skyline constituent un outil puissant pour l'analyse de données multidimensionnelles et la décision multicritère. En pratique, le calcul du skyline peut conduire à deux scénarios : soit (i) un nombre important d'objets sont retournés, soit (ii) un nombre réduit d'objets sont retournés, ce qui peut être insuffisant pour la prise de décisions. Dans cet article, nous abordons le second problème et proposons une approche permettant de le traiter. L'idée consiste à rendre le skyline plus permissive en lui ajoutant les objets, non skyline, les plus préférés. L'approche s'appuie sur une nouvelle relation de dominance floue appelée«Much Preferred». Un algorithme efficace pour calculer le skyline relaxé est proposé. Une série d'expériences sont menées pour démontrer la pertinence de l'approche et la performance de l'algorithme proposé.
**** *annee_2016  *idArt_1163
À l'ère du Big Data, les profils d'utilisateurs deviennent de plus en plus diversifiés et les données de plus en plus complexes, rendant souvent très difficile l'exploration des données. Dans cet article, nous proposons une technique de réécriture de requêtes pour aider les analystes à formuler leurs interrogations,pour explorer rapidement et intuitivement les données. Nous introduisons les requêtes discriminantes, une restriction syntaxique de SQL, avec une condition de sélection qui dissocie des exemples positifs et négatifs. Nous construisons un ensemble de données d'apprentissage dont les exemples positifs correspondent aux résultats souhaités par l'analyste, et les exemples négatifs à ceux qu'il ne veut pas. En utilisant des techniques d'apprentissage automatique,la requête initiale est reformulée en une nouvelle requête, qui amorce un processus itératif d'exploration des données. Nous avons implémenté cette idée dans un prototype (iSQL) et nous avons mené des expérimentations dans le domaine de l'astrophysique.
**** *annee_2016  *idArt_1166
La mise en place d'actions marketing efficaces passe par la segmentation de la clientèle. C'est-à-dire que les clients sont regroupés en ensembles homogènes en fonction de leurs habitudes de consommation, ce qui rend possible les actions ciblées. Ces dernières, en personnalisant l'offre permettent d'obtenir des taux de transformation plus importants et de meilleures ventes.Dans cet article, une méthode originale de segmentation comportementale de la clientèle est présentée. Elle permet de visualiser les segments de clients à travers des réseaux de communautés et de déceler aisément des mutations soudaines ou graduelles dans les comportements de quelques individus ou d'un ensemble plus important. L'analyste bénéficie alors d'une meilleure visibilité et peut adapter l'offre à tout moment.
**** *annee_2016  *idArt_1169
Dans cet article nous présentons une approche couplant une courbe remplissant l'espace et une chaîne de Markov pour analyser des données spatiales concernant la localisation de haies. Du fait de l'hétérogénéité spatiale des données, nous utilisons une courbe adaptative de Hilbert qui permet de linéariser l'espace en s'ajustant localement à la densité des données. Pour ensuite exploiter la séquence produite, il est nécessaire de caractériser la distance entre un point et son prédécesseur sur la courbe ainsi que la densité locale. Nous proposons de calculer un temps d'accès à un point à partir du point précédent en utilisant la notion de profondeur de découpe. Cette variable, couplée avec les variables caractérisant les haies est ensuite analysée avec un modèle de Markov. Nous présentons et interprétons les résultats obtenus sur un jeu de données d'environ10000 segments de haies d'une zone de la Basse vallée de la Durance.
**** *annee_2016  *idArt_1172
Dans le cadre du défi proposé à l'édition 2016 de la conférence EGC, nous exploitons les articles qui y ont été publiés de 2004 à 2015, avec pour but d'expliquer sa structure et son évolution. A partir des thématiques latentes découvertes et d'autres propriétés des articles (e.g.auteurs, affiliations), nous mettons en lumière des caractéristiques intéressantes des structures thématique et collaborative d'EGC. A l'aide d'une méthode d'extraction d'itemsets dans les hyper-graphes nous mettons aussi en avant des liens latents entre auteurs ou entre thématiques.De plus, nous proposons des recommandations d'auteurs ou de thématiques. Enfin, nous décrivons une interface Web pour explorer les connaissances découvertes.
**** *annee_2016  *idArt_1175
Dans cet article nous présentons CoSC, un cadre collaboratif pour la segmentation et la classification d'images de télédétection permettant d'extraire les objets d'une classe thématique donnée. Le processus de collaboration est guidé par la qualité des données évaluée par des critères d'homogénéité ainsi que des critères implicitement liés à la sémantique des objets afin d'extraire une classe thématique donnée. Nos expériences montrent que CoSC atteint des bons résultats en termes de classification, et améliore notablement la segmentation de l'image de manière globale.
**** *annee_2016  *idArt_1176
Dans le cadre du défi EGC 2016, nous avons développé une application web pour explorer les données décrivant les articles publiés depuis 2004 lors des conférences EGC. L'outil permet de découvrir les thèmes importants qui ont été abordés dans ces papiers. De plus, il permet de déterminer automatiquement les articles sémantiquement similaires à des thèmes donnés.
**** *annee_2016  *idArt_1177
Nous présentons une recherche sur la distribution et la classification non-supervisée des graphèmes. Nous visons à réduire l'écart entre les résultats de recherches récentes qui montrent la capacité des algorithmes d'apprentissage et de classification non-supervisée pour détecter les propriétés de phonèmes, et les possibilités actuelles de la représentation textuelle d'Unicode. Nos procédures doivent assurer la reproductibilité des expériences et garantir que l'information recherchée n'est pas implicitement présente dans le pré-traitement des données. Notre approche est capable de catégoriser correctement de potentiels graphèmes, ce qui montre que les propriétés phonologiques sont présentes dans les données textuelles, et peuvent être automatiquement extraites à partir des données textuelles brutes en Unicode, sans avoir besoin de les traduire en représentations phonologiques.
**** *annee_2016  *idArt_1178
Depuis 2001, les conférences EGC ont rassemblé 1 782 chercheurs autour de l'extraction et la gestion de connaissances. En 2016, l'association EGC réfléchit à son histoire et se projette en lançant un défi à sa communauté.Que peut-on révéler sur la communauté EGC via des approches développées en EGC ? Notre étude lexico-scientométrique apporte un éclairage sur les thématiques du congrès, les lieux de publication investis par ses auteurs, ou encore les auteurs sollicitables comme évaluateurs. Les résultats sont intégrés à un site web sous-tendu par un système d'information décisionnel.
**** *annee_2016  *idArt_1179
Mettre en place un dispositif de détection de pannes représente de nos jours l'un des défis majeurs pour les constructeurs des systèmes robotisés.Le processus de détection nécessite l'utilisation d'un certain nombre de capteurs afin de surveiller le fonctionnement de ces systèmes. Or, le coût ainsi queles contraintes liées à la mise en place de ces capteurs conduisent souvent les concepteurs à optimiser leurs nombres, ce qui mène à un manque de mesures nécessaires pour la détection de défaillances. L'une des méthodes pour combler ce manque est d'estimer les paramètres non mesurables à partir d'un modèle mathématique décrivant la dynamique du système réel. Cet article présente une approche basée sur des données mixtes (données mesurées et données estimées)pour la détection de défaillances dans les systèmes robotisés. Cette détection est effectuée en utilisant un classifieur de type arbre de décision. Les données utilisées pour son apprentissage proviennent des mesures prises sur le système réel.Ces données sont ensuite enrichies par des données estimées en provenance d'un observateur basé sur un modèle analytique. Cet enrichissement sous forme d'attributs supplémentaires a pour but d'augmenter la connaissance du classifieur sur le fonctionnement du système et par conséquent améliorer le taux de bonne détection de défaillances. Une expérience sur un système d'actionnement d'un siège robotisé, montrant l'intérêt de notre approche, sera présentée à la fin de l'article.
**** *annee_2016  *idArt_1180
Cet article porte sur l'étiquetage automatique de documents décrivant des produits, avec des concepts très spécifiques traduisant des besoins précis d'utilisateurs. La particularité du contexte est qu'il se confronte à une triple difficulté: 1) les concepts utilisés pour l'étiquetage n'ont pas de réalisations terminologiques directes dans les documents, 2) leurs définitions formelles ne sont pas connues au départ, 3) toutes les informations nécessaires ne sont pas forcément présentes dans les documents mêmes. Pour résoudre ce problème, nous proposons un processus d'annotation en deux étapes, guidé par une ontologie.La première consiste à peupler l'ontologie avec les données extraites des documents,complétées par d'autres issues de ressources externes. La deuxième est une étape de raisonnement sur les données extraites qui recouvre soit une phase d'apprentissage de définitions de concepts, soit une phase d'application des définitions apprises. L'approche SAUPODOC est ainsi une approche originale d’enrichissement d'ontologie qui exploite les fondements du Web sémantique,en combinant les apports du LOD et d'outils d'analyse de texte, d’apprentissage automatique et de raisonnement. L'évaluation, sur deux domaines d'application,donne des résultats de qualité et démontre l'intérêt de l'approche.
**** *annee_2016  *idArt_1182
Nous présentons une méthode de réduction de dimensionnalité pour des données de préférences multicritères lorsque l'espace des évaluations est un treillis distributif borné. Cette méthode vise à réduire la complexité des procédures d'apprentissage d'un modèle d'agrégation sur des données qualitatives.Ainsi nous considérons comme modèle d'agrégation l'intégrale de Sugeno. L’apprentissage d'un tel modèle à partir de données empiriques est un problème d'optimisation à 2n paramètres (où n est le nombre de critères considérés).La méthode de réduction que nous proposons s'appuie sur l'observation de certaines relations entre les éléments de ces données, et nous donnons des premiers résultats d'applications.
**** *annee_2016  *idArt_1183
Nous proposons une nouvelle approche pour le calcul de similarité sémantique entre phrases en utilisant les noyaux sémantiques qui les composent.Ces noyaux, sous la forme de triplets (sujet, verbe et objet) sont supposés porteurs de l'information des phrases dont ils sont extraits. Sur la base de la comparaison sémantique de noyaux, on extrait un ensemble d'indicateurs descriptifs.Nous utilisons ensuite un apprentissage automatique, sur un benchmark contenant des phrases dont la similarité sémantique a été évaluée par des experts humains,afin de déterminer l'importance de chaque indicateur et de construire ainsi un modèle capable de fournir une mesure de similarité sémantique entre phrases. Les expérimentations et les études comparatives, effectuées avec d'autres approches permettant l'estimation des similarités sémantiques entre phrases,montrent les bonnes performances de notre approche. En se basant sur cette dernière,un outil de navigation sémantique est en cours de développement.
**** *annee_2016  *idArt_1184
Les traces de mobilité générées par les divers capteurs qui nous entourent peuvent être analysées à des fins prédictives et explicatives pour répondre à divers problèmes du quotidien. Si de nombreuses méthodes ont été proposées pour décrire le comportement d'un individu de manière globale à partir des transitions entre ses différents points d'intérêts (par exemple via un modèle de Markov), peu de travaux cherchent à l'expliquer de manière locale. Nous proposons dans cet article une méthode qui permet d'extraire pour un individu dont on a une trace de mobilité conséquente des motifs de mobilité dits contextualisés.Chaque motif est composé d'une description sur l'ensemble des visites aux différents points d'intérêt de l'individu qui maximise une ou plusieurs mesures avec une sémantique particulière (le motif décrit une phase sédentaire ou exceptionnel de la mobilité de l'individu). Une expérimentation a été menée à partir de traces de mobilité de véhicules et donne des résultats encourageants.
**** *annee_2016  *idArt_1185
Au cours des dernières années, la classification à base de clustering s'est imposée comme un sujet de recherche important. Cette approche vise à décrire et à prédire un concept cible d'une manière simultanée. Partant du fait que le choix des centres pour l'algorithme des K-moyennes standard a un impact direct sur la qualité des résultats obtenus, cet article vise alors à tester à quel point une méthode d'initialisation supervisée pourrait aider l'algorithme des K-moyennes standard à remplir la tâche de la classification à base des K-moyennes.
**** *annee_2017  *idArt_1191
Dans cet article, nous proposons une méthodologie pour anonymiser une table de données multidimensionnelles contenant des données individuelles(soit n individus décrits par m variables). L'objectif est de publier une table anonyme construite à partir d'une table initiale qui protège contre le risque de ré-identification. En d'autres termes, on ne doit pas pouvoir retrouver dans les don-nées publiées un individu présent dans la table originale. La solution proposée consiste à agréger les données à l'aide d'une technique de co-clustering, puis à utiliser le modèle produit pour générer une table de données synthétiques du même format que les données initiales. Les données synthétiques, qui contiennent des individus fictifs, peuvent maintenant être publiées. Les données produites sontévaluées en termes d'utilité pour différentes tâches de fouille (analyse exploratoire, classification) et de niveau de protection.
**** *annee_2017  *idArt_1192
La classification croisée est une technique d'analyse non supervisée qui permet d'extraire la structure sous-jacente existante entre les individus et les variables d'une table de données sous forme de blocs homogènes. Cette technique se limitant aux variables de même nature, soit numériques soit catégorielles, nous proposons de l'étendre en proposant une méthodologie en deux étapes. Lors de la première étape, toutes les variables sont binarisées selon un nombre de parties choisi par l'analyste, par discrétisation en fréquences égales dans le cas numérique ou en gardant les valeurs les plus fréquentes dans le cas catégoriel. La deuxième étape consiste à utiliser une méthode de co-clustering entre individus et variables binaires, conduisant à des regroupements d'individus d'une part, et de parties de variables d'autre part. Nous appliquons cette méthodologie sur plusieurs jeux de donnée en la comparant aux résultats d'une analyse par correspondances multiples ACM, appliquée aux même données binarisées.
**** *annee_2017  *idArt_1194
Nous présentons dans cet article une méthode supervisée de structuration (en DAG) d'un ensemble d'éléments. Étant donnés une structure cible et un ensemble de relations sur ces éléments, il s'agit d'apprendre un modèle de structuration par combinaison des relations initiales. Nous formalisons ce problème dans le cadre de la théorie de la prétopologie qui permet d'atteindre des modèles de structuration complexes.Nous montrons que la non-idempotence de la fonction d'adhérence rentre dans le cadre du formalisme de l'apprentissage (supervisé) multi-instance et nous pro-posons un algorithme d'apprentissage reposant sur le dénombrement des «sacs»positifs et négatifs plutôt que sur un ensemble d'apprentissage standard.Une première expérimentation de cette méthode est présentée dans un cadre applicatif de fouille de textes, consistant à apprendre un modèle de structuration taxonomique d'un ensemble de termes.
**** *annee_2017  *idArt_1195
Dans cet article nous présentons une étude exploitant des méthodes d'apprentissage automatique de structures séquentielles pour extraire des relations sémantiques dans des textes issus de bases d'appels d'offres. L'une des relations que nous considérons concerne l'emprise d'un projet d'aménagement,caractérisée par une association entre les concepts qui définissent les infrastructures (bâtiments) et les concepts qui définissent leur(s) surface(s) d'implantation.L'étude propose une analyse comparée d'approches à base de champs conditionnels aléatoires (CRF), de CRF d'ordre supérieur (H-CRF), de CRF semi-Markoviens, Modèles de Markov cachés (HMM) et de perceptrons structurés.
**** *annee_2017  *idArt_1196
Dans un contexte de traitement de flux de données, il est important de garantir à l'utilisateur des propriétés de performance, qualité des résultats et passage à l'échelle. Mettre en adéquation ressources et besoins, pour n'allouer que les ressources nécessaires au traitement efficace des flux, est un défi d'actualité majeur au croisement des problématiques du Big Data et du Green IT. L'approche que nous suggérons permet d'adapter dynamiquement et automatiquement le degré de parallélisme des différents opérateurs composant une requête continue selon l'évolution du débit des flux traités. Nous proposons i) une métrique permettant d'estimer l'activité future des opérateurs selon l'évolution des flux en entrée, ii) l'approche AUTOSCALE évaluant a priori l'intérêt d'une modification du degré de parallélisme des opérateurs en prenant en compte l'impact sur le traitement des données dans sa globalité iii) grâce à une intégration de notre proposition à Apache Storm, nous exposons des tests de performance comparant notre approche par rapport à la solution native de cet outil.
**** *annee_2017  *idArt_1198
La formule de Lance et Williams permet d'unifier plusieurs méthodes de classification ascendante hiérarchique (CAH). Dans cet article, nous supposons que les données sont représentées dans un espace euclidien et nous établis-sons une nouvelle expression de cette formule en utilisant les similarités cosinus au lieu des distances euclidiennes au carré. Notre approche présente les avantages suivants. D'une part, elle permet d'étendre naturellement les méthodes classiques de CAH aux fonctions noyau. D'autre part, elle permet d'appliquer des méthodes d'écrêtage permettant de rendre la matrice de similarités creuse afin d'améliorer la complexité de la CAH. L'application de notre approche sur des tâches de classification automatique de données textuelles montre d'une part,que le passage à l'échelle est amélioré en mémoire et en temps de traitement;d'autre part, que la qualité des résultats est préservée voire améliorée.
**** *annee_2017  *idArt_1199
Dans cet article, nous proposons une nouvelle approche de classification d'objets 3D inspirée des Time Series Shapelets de Ye et Keogh (2009).L'idée est d'utiliser des sous-surfaces discriminantes pour la classification concernée afin de prendre en compte la nature locale des éléments pertinents. Cela permet à l'utilisateur d'avoir connaissance des sous-parties qui ont été utiles pour déterminer l'appartenance d'un objet à une classe. Les résultats obtenus confirment l'intérêt de la sélection aléatoire de caractéristiques candidates pour la pré-sélection d'attributs en classification supervisée.
**** *annee_2017  *idArt_1201
Dans le présent papier, nous proposons l'étude et l'application d’une nouvelle approche pour l'aide à la reconnaissance automatique de cibles (ATR, pour Automatic Target Recognition) à partir des images à synthèse d’ouverture inverse (ISAR, pour Inverse Synthetic Aperture Radar). Cette approche est com-posée de deux phases principales. Dans la première phase, nous utilisons deux méthodes statistiques pour extraire les caractéristiques discriminants à partir des images ISAR. Nous nous intéressons dans ce travail aux deux descripteurs multi-échelles issus des deux méthodes SIFT (Scale-Invariant Feature Transform) et la décomposition en ondelettes complexes DT-CWT (Dual-Tree Complex Wa-velet Transform) qui sont calculées disjointement. Ensuite, nous modélisons séparément les descripteurs issus des deux méthodes précédentes (SIFT et DT-CWT) par la loi Gamma. Les paramètres statistiques estimés sont utilisés pour la deuxième phase dédiée à la classification. Dans cette deuxième phase, une classification parcimonieuse (SRC, pour Sparse Representation-based Classification) est proposée. Afin d'évaluer et valider notre approche, nous avons eu recours aux données réelles d'images issues d'une chambre anéchoïque. Les résultats expérimentaux montrent que l'approche proposée peut atteindre un taux de reconnaissance élevé et dépasse largement l'utilisation du même descripteur avec le classifieur machine à vecteurs de support (SVM, pour Support VectorMachine).
**** *annee_2017  *idArt_1202
La classification croisée (co-clustering) est une technique non super-visée qui permet d'extraire la structure sous-jacente existante entre les lignes et les colonnes d'une table de données sous forme de blocs. Plusieurs approches ont été étudiées et ont démontré leur capacité à extraire ce type de structure dans une table de données continues, binaires ou de contingence. Cependant, peu de travaux ont traité le co-clustering des tables de données mixtes. Dans cet article,nous étendons l'utilisation du co-clustering par modèles à blocs latents au cas des données mixtes (variables continues et variables binaires). Nous évaluons l'efficacité de cette extension sur des données simulées et nous discutons ses limites potentielles.
**** *annee_2017  *idArt_1203
Cet article se situe dans le cadre de l'analyse de concepts formels(ACF) qui fournit des classes (les extensions) d'objets partageant des caractères similaires (les intentions), une description par des attributs étant associée à chaque classe. Dans un article récent, une nouvelle mesure de similarité entre deux concepts dans un treillis de concepts a été introduite, permettant une normalisation par la taille du treillis. Dans cet article, nous comparons cette mesure de similarité avec des mesures existantes, soit basées sur la cardinalité des en-sembles ou issues de la conception d'ontologies et basées sur la structure hiérarchique du treillis. Une comparaison statistique avec des méthodes existantes est effectuée et testée pour leur consistance.
**** *annee_2017  *idArt_1205
Découvrir des règles qui distinguent clairement une classe d'une autre reste un problème difficile. De tels motifs permettent de suggérer des hypothèses pouvant expliquer une classe. La découverte de sous-groupes (Subgroup Discovery, SD), un cadre qui définit formellement cette tâche d'extraction de motifs,est toujours confrontée à deux problèmes majeurs: (i) définir des mesures de qualité appropriées qui caractérisent la singularité d'un motif et (ii) choisir une heuristique d'exploration de l'espace de recherche correcte lorsqu'une énumération complète est irréalisable. À ce jour, les algorithmes de SD les plus efficaces sont basés sur une recherche en faisceau (Beam Search, BS). La collection de motifs extraits manque cependant de diversité en raison de la nature gloutonne de l'exploration. Nous proposons ici d'utiliser une technique d'exploration récente,la recherche arborescente de Monte Carlo (Monte Carlo Tree Search, MCTS).Le compromis entre l'exploitation et l'exploration ainsi que la puissance de la recherche aléatoire permettent d'obtenir une solution disponible à tout moment et de surpasser généralement les approches de type BS. Notre étude empirique,avec plusieurs mesures de qualité, sur divers jeux de données de référence et du monde réel démontre la qualité de notre approche.
**** *annee_2017  *idArt_1207
La conférence EGC'2017 propose un défi dont le contexte est la gestion des espaces verts pour la ville de Grenoble, et notamment des arbres qui y sont présents. L'objectif est de proposer un modèle basé sur des données fournies qui permettrait de prédire au mieux les arbres malades, ainsi que la localisation potentielle de la maladie. Après avoir obtenu quelques résultats intéressants avec des modèles standards, notre approche utilisant un modèle Cost-Sensitive One Against All (CSOAA) nous permet d'obtenir une exactitude de 0,86, une précision de 0,88, et un rappel de 0,91 sur la prédiction uni-label, et une précision/rappel micro de 0,82/0,74 ainsi qu'une précision/rappel macro de 0,66/0,46pour la prédiction multi-label. L'extraction de connaissances pour la tâche 2 nous a permis de mettre en relief l'intérêt de l'ajout de données sur la nature des maladies et la concentration de la pollution dans la ville.
**** *annee_2017  *idArt_1211
La problématique de ce papier est d'identifier dans un graphe dynamique les communautés les plus représentatives sur une période donnée, de mesurer leur stabilité, et d'en visualiser les évolutions majeures. Notre cas d'usage concerne l'étude de la visibilité médiatique des communautés et des individus grâce aux données relatives aux émissions télévisuelles et radiophoniques entre2011 et 2015. A partir d'une détection de communautés sur l'intégralité de la période, nous proposons des mesures de stabilité et d'activité des communautés et proposons une visualisation de leur évolution temporelle.
**** *annee_2017  *idArt_1213
Nous présentons ici une méthode originale pour l'automatisation de la détection de paysages dans une image satellite. Deux enjeux majeurs apparaissent dans ce processus. Le premier réside dans la faculté à prendre en compte l'ensemble des connaissances expertes tout au long du travail d'analyse de l'image. Le second est de réussir à structurer et pérenniser ces connaissances de façon à les rendre interopérables et exploitables dans le cadre du web de données. Nous présentons en quoi la collaboration de plusieurs stratégies alliant les traitements de l'image, le calcul de caractéristiques spécifiques et la programmation logique inductive (PLI), vient alimenter le processus d'automatisation,et comment l'intégration de la connaissance, au travers de la construction d'ontologies dédiées, permet de répondre pleinement à ces enjeux.
**** *annee_2017  *idArt_1214
L'extraction de motifs séquentiels vise à extraire des comportements récurrents dans un ensemble de séquences. Lorsque ces séquences sont étiquetées, l'extraction de motifs discriminants engendre des motifs caractéristiques de chaque classe de séquences. Cet article s'intéresse à l'extraction des chroniques discriminantes où une chronique est un type de motif temporel représentant des durées inter-évènements quantitatives. L'article présente l'algorithme DCM dont l'originalité réside dans l'utilisation de méthodes d'apprentissage automatique pour extraire les intervalles temporels. Les performances computationnelles et le pouvoir discriminant des chroniques extraites sont évalués sur des données synthétiques et réelles.
**** *annee_2017  *idArt_1215
Dans une base de connaissance, les entités se veulent pérennes mais certains événements induisent que les relations entre ces entités sont instables.C'est notamment le cas pour des relations entre organisations, produits, ou marques,entités qui peuvent être rachetées. Dans cet article, nous proposons une approche permettant d'extraire des relations d'appartenance entre deux entités afin de peupler une base de connaissance. L'extraction des relations à partir d'une source dynamique d'informations telle que Twitter permet d'atteindre cet objectif en temps réel. L'approche consiste à modéliser les événements en s'appuyant su rune ressource lexico-sémantique. Une fois les entités liées au Web des données ouvertes (en particulier DBpedia), des règles linguistiques sont appliquées pour finalement générer les triplets RDF qui représentent les événements.
**** *annee_2017  *idArt_1216
Un grand nombre d'applications nécessitent d'analyser un unique graphe attribué évoluant dans le temps. Cette tâche est particulièrement complexe car la structure du graphe et les attributs associés à chacun de ses nœuds ne sont pas figés. Dans ce travail, nous nous focalisons sur la découverte de motifs récurrents dans un tel graphe. Ces motifs, des séquences de sous-graphes connexes, représentent les évolutions récurrentes de sous-ensembles de nœuds et de leurs attributs.Différentes contraintes ont été définies (e.g. fréquence, volume, connectivité,non redondance, continuité) et un algorithme original a été proposé. Les expérimentations réalisées sur des jeux de données synthétiques et réelles démontrent l'intérêt de l'approche proposée et son passage à l'échelle.
**** *annee_2017  *idArt_1221
Contrairement à ce que promeut le Web des données, les données exposées par la plupart des organisations sont dans des formats non RDF tels que CSV, JSON, ou XML. De plus sur le Web des objets, les objets contraints préféreront des formats binaires tels que EXI ou CBOR aux formats RDF textuels.Dans ce contexte, RDF peut toutefois servir de lingua franca pour l'interopérabilité sémantique, l'intégration de données aux formats hétérogènes, le raisonnement,et le requêtage. Dans ce but, plusieurs outils et formalismes permettent de transformer des documents non-RDF vers RDF, les plus flexibles étant basés sur des langages de transformation ou de correspondance (GRDDL, XSPARQL,R2RML, RML, CSVW, etc.). Cet article définit un nouveau langage, SPARQL-Generate,qui permet de générer du RDF à partir: (i) d'une base de données RDF,et (ii) d'un nombre quelconque de documents aux formats arbitraires. L'originalité de SPARQL-Generate est qu'il étend SPARQL 1.1, et peut donc (i) être appris facilement par les ingénieurs de la connaissance familiers de SPARQL,(ii) être implémenté au dessus de n'importe quel moteur SPARQL existant, (iii)tirer parti des mécanismes d'extension de SPARQL pour prendre en compte de futurs formats.
**** *annee_2017  *idArt_1224
Nous nous intéressons à la classification non supervisée de séries chronologiques. Pour ce faire, nous utilisons l'algorithme K-Spectral Centroïd (K-SC), une variante des K-Means. K-Spectral Centroïd utilise une mesure de dis-similarité entre séries chronologiques, invariante par translation et par change-ment d'échelle. Cet algorithme est coûteux en temps de calcul : lors de la phase d'affectation, il nécessite de tester toutes les translations possibles pour identifier la meilleure ; lors de la phase de représentation, le calcul du nouveau barycentre nécessite l'extraction de la plus petite valeur propre d'une matrice. Nous proposons dans ce travail trois optimisations de K-SC. L'identification de la meilleure translation peut être réalisée efficacement en utilisant la transformée de Fourier discrète. Chaque matrice peut être calculée incrémentalement. Le calcul du nouveau barycentre peut s'effectuer à moindre coût grâce à la méthode de la puissance itérée. Ces trois optimisations fournissent exactement la même classification que K-SC.
**** *annee_2017  *idArt_1227
Ces dernières années, la prolifération rapide des capteurs et des objets communicants de tous types a significativement enrichi le contenu des systèmes d'information. Cependant, cela suscite de nouvelles questions quant à la confiance que l'on peut accorder aux informations et aux sources d'informations. En effet, ces sources peuvent être leurrées ou sous l'emprise d'un tiers qui falsifie ou altère les informations. Cet article propose donc d'aborder la sécurité des systèmes d'informations sous l'angle de la confiance dans les sources d'informations.En premier lieu, la définition puis l'évaluation de la confiance dans un réseau hétérogène sont introduits. Une modélisation des sources est ensuite proposée. La confiance dans ces sources d'informations est abordée au travers de deux caractéristiques: la compétence et la sincérité. L'extraction de la confiance est réalisée via un ensemble de mesures de ces deux caractéristiques. Une expérience basée sur plusieurs sources simulées à partir d'un jeu de données réelles montrent la pertinence de l'approche; approche qui peut être transposée à d'autres systèmes d'information. Cette étude est appliquée à l'analyse des données de navigation et de positionnement d'un navire.
**** *annee_2017  *idArt_1228
Ce document se situe dans le cadre de l'analyse de concepts formels(ACF), une méthode de hiérarchisation algébrique des données basée sur la notion d’intention / extension, partageant maximalement attributs et objets. Nous présentons ici une mesure de similarité basée sur des correspondances entre deux treillis de Galois, définie par un modèle expressif utilisant des correspondances entre objets et entre attributs des deux treillis. Un point clé de notre approche est que ces correspondances peuvent ne pas être des fonctions, associant un objet (resp. attribut) d'un treillis avec plusieurs objets (resp. attributs) de l'autre treillis.
**** *annee_2017  *idArt_1230
Le modèle NoSQL orienté colonnes propose un schéma de données flexible et hautement dénormalisé. Dans cet article, nous proposons une méthode d'implantation d'un entrepôt de données dans un système NoSQL en colonnes. Notre méthode est basée sur une stratégie de regroupement des attributs issus des tables de faits et de dimensions, sous forme de familles de colonnes. Nous utilisons deux algorithmes OEP et k-means. Pour évaluer notre méthode, nous avons effectué plusieurs tests sur le benchmark TPC-DS au sein du SGBD NoSQL orienté colonnes Hbase, avec une architecture de type MapReduce sur une plate-forme Hadoop.
**** *annee_2017  *idArt_1233
Nous décrivons dans cet article notre réponse au défi EGC 2017. Une analyse exploratoire des données a tout d'abord permis de comprendre les distributions des différentes variables et de détecter de fortes corrélations. Nous avons défini deux variables supplémentaires à partir des variables du jeu de données.Plusieurs algorithmes de classification supervisée ont été expérimentés pour ré-pondre à la tâche numéro 1 du défi. Les performances ont été évaluées par validation croisée. Cela nous a permis de sélectionner les meilleurs classifieurs uni-label et multi-label. Autant sur la tâche uni-label que multi-label, le meilleur classifieur dépasse les références d'environ 2%. Nous avons également exploré la tâche numéro 2 du défi. D'une part, des règles d'association ont été recherchées. D'autre part, le jeu de données a été enrichi avec des connaissances telles que des données climatiques (pluviométrie, température, vent) ou des données taxonomiques dans le domaine de la botanique (famille, ordre, super-ordre). En outre, des données géographiques et cartographiques sont exploitées dans un outil de visualisation d'une partie des données sur les arbres.
**** *annee_2017  *idArt_1234
Le financement participatif est un mode de financement d'un projet faisant appel à un grand nombre de personnes, contrairement aux modes de financement traditionnels. Il a connu une forte croissance avec l'émergence d'Internet et des réseaux sociaux. Cependant plus de 60 %des projets ne sont pas financés, il est donc important de bien préparer sa campagne de financement. De plus, en cours de campagne, il est crucial d'avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration, communication) : des outils de prédiction sont alors indispensables. Nous proposons dans cet article une méthode de prédiction du montant final levé lors d'une campagne de financement participatif utilisant l'algorithme k-NN : en utilisant l'historique de campagnes passées, nous déterminons celles qui sont les plus similaires à une campagne en cours. Nous utilisons alors les montants finaux pour faire une estimation. Nous comparons plusieurs mesures de distance pour dé-terminer les plus proches voisins. Nos résultats indiquent que le dernier état d'une campagne seul est suffisant pour obtenir une bonne prédiction.
**** *annee_2017  *idArt_1238
Une décision de justice est un document textuel rapportant le dénouement d'une affaire judiciaire. Les juristes s'en servent régulièrement comme source d'interprétation de la loi et de compréhension de l'opinion des juges.La masse disponible de décisions exige des solutions automatiques pour aider les acteurs du droit. Nous proposons d'adresser certains des défis liés à la recherche et l'analyse du volume croissant de décisions de justice en France dans un projet plus global. La première phase de ce projet porte sur l'extraction d'information des décisions dans l'objectif de construire une base de connaissances jurisprudentielles structurant et organisant les décisions. Une telle base facilite l'analyse descriptive et prédictive de corpus de décisions. Cet article présente une application des modèles probabilistes pour la segmentation des décisions et la reconnaissance d'entités dans leur contenu (lieu, date, participants, règles de loi, ...). Nos tests montrent l'avantage d'approches basées sur les champs aléatoires conditionnels (CRF) par rapport à des modèles plus simples et rapides basés sur les modèles cachés de Markov (HMM). Nous présentons ici les aspects techniques de la sélection et l'annotation du corpus d'apprentissage, et la définition de descripteurs discriminants. La spécificité des textes est importante et doit être prise en compte lors de l'application de méthodes d'extraction d'information dans un domaine spécifique.
**** *annee_2017  *idArt_1240
La classification multi-label est une extension de la classification supervisée au cas de plusieurs labels. Elle a connu un regain d'intérêt récent dans la communauté du machine learning de par son utilité dans plusieurs domaines.Comme pour tout problème de machine learning, le besoin de prétraiter les données multi-label est apparu comme une nécessité afin d'améliorer les performances des classifieurs. Dans cet article, nous introduisons une nouvelle méthode permettant de pré-traiter des variables descriptives par discrétisation ou groupement de valeur, dans le cas de plusieurs labels à prédire. Le choix du meilleur prétraitement est posé comme un problème de sélection de modèle, et est résolu au moyen d'une approche bayésienne. Une étude comparative est réalisée avec d'autres méthodes de l'état de l'art afin de positionner la nouvelle méthode et de montrer l'intérêt de la sélection de variables pour la classification.
**** *annee_2017  *idArt_1241
Dans ce papier nous proposons une nouvelle approche de subspace clustering pour les flux de données, permettant à l'utilisateur de suivre visuellement le changement dans le comportement du flux. Cette approche détecte l'impact des variables sur l'évolution du flux, Tout en visualisant les étapes du subspace clustering en temps réel. En premier lieu nous appliquons un clustering sur l'ensemble de variables afin d’identifier les sous-espaces. Ensuite un clustering est appliqué sur les individus dans chaque sous-espace.
**** *annee_2017  *idArt_1242
De nombreuses méthodes ont été proposées pour extraire des clusters des réseaux sociaux. Si un travail important est aujourd'hui mené sur la conception de méthodes innovantes capables de rechercher des clusters de nature différente, la plupart des approches font l'hypothèse de réseaux statiques.L'une des récentes méthodes concerne notamment la recherche de liens conceptuels. Il s'agit d'une nouvelle approche de clustering de liens, qui exploite à la fois la structure du réseau et les attributs des nœuds dans le but d'identifier des liens fréquents entre des groupes de nœuds au sein desquels les nœuds partagent des attributs communs. Dans ce travail, nous nous intéressons au suivi des liens conceptuels dans des réseaux dynamiques, c'est-à-dire des réseaux qui connaissent des changements structurels importants. Nous cherchons en particulier à comprendre comment les liens conceptuels se forment et évoluent au cours du développement du réseau. Pour ce faire, nous proposons un ensemble de mesures qui visent à capturer des comportements caractérisant l'évolution de ces clusters. Notre approche est ainsi utilisée pour comprendre l'évolution des liens conceptuels extraits sur deux réseaux réels : un réseau de co-auteurs d'articles scientifiques et un réseau de communications mobiles. Les résultats obtenus permettent de mettre en lumière des tendances significatives dans l'évolution des clusters sur ces deux réseaux.
**** *annee_2017  *idArt_1245
L'algorithme des K-moyennes prédictives est un des algorithmes de clustering prédictif visant à décrire et à prédire d'une manière simultanée. Contrairement à la classification supervisée et au clustering traditionnel, la performance de ce type d'algorithme est étroitement liée à sa capacité à réaliser un bon compromis entre la description et la prédiction. Or, à notre connaissance,il n'existe pas dans la littérature un critère analytique permettant de mesurer ce compromis. Cet article a pour objectif de proposer une version modifiée de l'indice Davies-Bouldin, nommée SDB, permettant ainsi d'évaluer la qualité des résultats issus de l'algorithme des K-moyennes prédictives. Cette modification se base sur l'intégration d'une nouvelle mesure de dissimilarité permettant d'établir une relation entre la proximité des observations en termes de distance et leur classe d'appartenance. Les résultats expérimentaux montrent que la version modifiée de l'indice DB parvient à mesurer la qualité des résultats issus de l'algorithme des K-moyennes prédictives.
**** *annee_2017  *idArt_1248
La fouille de motifs graduels a pour but la découverte de co-variations fréquentes entre attributs numériques dans une base de données. Plusieurs algorithmes d'extraction automatique de tels motifs ont été proposés. La principale différence entre ces algorithmes réside dans la sémantique de variation considérée. Dans certains domaines d'application, on trouve des bases de données dont les objets sont munis d'une relation d'ordre temporel. Ainsi, du fait de leur sémantique de variation, les algorithmes de la littérature sont inadaptés pour de telles données. Dans ce contexte, nous proposons une approche de fouille de motifs graduels sous contrainte d'ordre temporel, qui réduit le nombre de motifs générés. Une étude expérimentale sur des bases de données paléoécologiques permet d'apprendre les groupements d'indicateurs qui modélisent l'évolution de la biodiversité. Les connaissances apportées par ces groupements montre l'intérêt de notre approche pour le domaine environnemental.
**** *annee_2017  *idArt_1250
La découverte de règles d'association à partir de données transactionnelles est une tâche largement étudiée en fouille de données. Les algorithmes proposés dans ce cadre partagent la même méthodologie en deux étapes à savoir l'énumération des itemsets fréquents suivie par l'étape de génération de règles.Dans cet article, nous proposons une nouvelle approche basée sur la satisfiabilité propositionnelle pour extraire les règles d'association en une seule étape. Pour montrer la flexibilité et la déclarativité de notre approche, nous considérons également deux autres variantes, à savoir la fouille de règles d'association fermées et la fouille de règles indirectes. Les expérimentation sur plusieurs jeux de don-nées montrent que notre approche offre de meilleures performances comparée à des approches spécialisées.
**** *annee_2017  *idArt_1252
Le crowdsourcing, un enjeu économique majeur, est le fait d'externaliser une tâche interne d'une entreprise vers le grand-public, la foule. C'est ainsi une forme de sous-traitance digitale destinée à toute personne susceptible de pouvoir réaliser la tâche demandée généralement rapide et non automatisable.L'évaluation de la qualité du travail des participants est cependant un problème majeur en crowdsourcing. En effet, les contributions doivent être contrôlées pour assurer l'efficacité et la pertinence d'une campagne. Plusieurs méthodes ont été proposées pour évaluer le niveau d'expertise des participants. Ce travail a la particularité de proposer une méthode de calcul de degrés d'expertise en présence de données dont l'ordre de classement est connu. Les degrés d'expertise sont ensuite considérés sur des données sans ordre pré-établi. Cette méthode fondée surla théorie des fonctions de croyance tient compte des incertitudes des réponses et est évaluée sur des données réelles d'une campagne réalisée en 2016.
**** *annee_2017  *idArt_1253
La F-Mesure de trait est une métrique de sélection de variables statistique sans paramètres qui a montré de bonnes performances pour la classification, l'étiquetage de clusters ou encore la mesure de qualité des clusters. Dans cet article, nous proposons d'évaluer son utilisation dans le contexte des graphes de terrain et de leur structure communautaire pour bénéficier de son système sans paramètres et de ses performances bien évaluées. Nous étudions donc sur des graphes synthétiques réalistes les corrélations qui existent entre la F-Mesure de trait et certaines mesures de centralité, mais surtout avec des mesures destinées à caractériser le rôle communautaire des nœuds. Nous montrons ainsi que cette mesure est liée à la centralité des nœuds du réseau, et qu'elle est particulièrement adaptée à la mesure de leur connectivité au regard de la structure de communautés. Nous observons par ailleurs que les mesures usuelles de détection des rôles communautaires sont fortement dépendantes de la taille des communautés alors que celles que nous proposons sont par définition liées à la densité de la communauté, ce qui rend les résultats comparables d'un réseau à un autre. Ceci offre donc la possibilité d'applications comme le suivi temporel de la structure des communautés. Enfin, le processus de sélection appliqué aux nœuds permet de disposer d'un système universel, contrairement aux seuils fixés auparavant empiriquement pour l'établissement des rôles communautaires.
**** *annee_2018  *idArt_1261
La classification croisée (co-clustering) est une technique qui permet d'extraire la structure sous-jacente existante entre les lignes et les colonnes d'une table de données sous forme de blocs. Plusieurs applications utilisent cette technique, cependant de nombreux algorithmes de co-clustering actuels ne passent pas à l'échelle. Une des approches utilisées avec succès est la méthode MODL, qui optimise un critère de vraisemblance régularisée. Cependant, pour des tailles plus importante, cette méthode atteint sa limite. Dans cet article, nous présentons un nouvel algorithme de co-clustering à deux niveaux, qui compte tenu du critère MODL permet de traiter efficacement de données de très grande taille, ne pouvant pas tenir en mémoire. Nos expériences montrent que l'approche proposée gagne en temps de calcul tout en produisant des solutions de qualité.
**** *annee_2018  *idArt_1262
La recommandation de points d'intérêts est devenue une caractéristique essentielle des réseaux sociaux géo-localisés qui a accompagné l'émergence des échanges massifs de données digitales. Cependant les faibles densités de points d'intérêts visités par les utilisateurs rendent le problème difficile à traiter, d'autant plus que les espaces de mobilité des utilisateurs sont très hétérogènes, allant de la ville au monde entier.Dans ce papier nous explorons l'impact d'une approche de clustering spatial sur la qualité de la recommandation. Notre approche est basée sur un modèle de factorisation de matrices de Poisson et un réseau social inféré des différents comportements de mobilité. Nous avons conduit une évaluation comparative des performances de notre approche sur unjeu de données réaliste. Les résultats expérimentaux montrent que notre approche permet une précision supérieure aux techniques de recommandation alternatives.
**** *annee_2018  *idArt_1263
L'analyse des sentiments est un processus pendant lequel la polarité (positive, négative ou neutre) d'un texte donné est déterminée. Nous nous intéressons dans ce travail à l'analyse des sentiments à partir des commentaires Facebook, réels, partagés en arabe standard ou dialectal marocain par une approche basée sur l'apprentissage automatique. Ce processus commence par la collecte des commentaires et leur annotation à l'aide du crowdsourcing suivi d'une phase de prétraitement du texte afin d'extraire des mots arabes réduits à leur racine. Ces mots vont être utilisés pour la construction des variables d'entrée en utilisant plusieurs combinaisons de schémas d'extraction et de pondération.Pour réduire la dimensionnalité, une méthode de sélection de variables est appliquée.Les résultats obtenus des expérimentations sont très prometteurs.
**** *annee_2018  *idArt_1264
Cet article présente une approche visant à extraire les informations exprimées dans un corpus de textes et en produire un résumé. Plusieurs variantes de méthodes extractives de résumé de texte ont été implémentées et évaluées. Leur principale originalité réside dans l'exploitation de structures appelées CDS (pour Clause Description Structure) issues d'un composant d'annotation en rôles sémantiques et non directement des phrases composant les textes. Le résumé obtenu est un sous-ensemble des CDS issus du corpus d'origine ; ce format permettra dans la suite la détection d'incohérences textuelles. Dans ce travail, nous retransformons les CDS résumés en texte pour permettre la comparaison de notre approche avec celles de la littérature. Les premiers résultats sont très encourageants: les variantes que nous proposons obtiennent généralement de meilleurs scores que des implémentations de méthodes de référence.
**** *annee_2018  *idArt_1266 
Avec plus de 800 000 décès par an dans le monde, le suicide est la troisième cause de décès évitable. Il y a 20 fois plus de tentatives, impliquant de nombreuses hospitalisations, des coûts humains et sociétaux énormes. Ces dernières années, les modalités de collecte de données, sociologiques et cliniques,concernant les patients reçus en consultation après une tentative, ont connu de profonds changements liés aux outils numériques. Nous présentons les principaux résultats d'un processus complet de fouille de données sur un échantillon de suicidants de deux hôpitaux européens. Le premier objectif est d'identifier des groupes de patients similaires et le second d'identifier des facteurs de risque associés au nombre de tentatives. Des méthodes non supervisées (ACM et clustering)et supervisées (arbres de régression) sont appliquées pour y répondre.Les résultats mettent en lumière l'apport de la fouille de données à des fins descriptives ou explicatives.
**** *annee_2018  *idArt_1267
Dans le cadre du clustering prédictif, pour attribuer la classe aux groupes formés à la fin de la phase d'apprentissage, le vote majoritaire est la méthode communément utilisée. Cependant, cette approche comporte certaines limitations qui influent directement sur la qualité des résultats obtenus en termes de prédiction. Pour surmonter ce problème, nous proposons d'incorporer des modèles prédictifs localement dans les clusters formés afin d'améliorer la qualité prédictive du modèle global. Les résultats expérimentaux montrent que cette incorporation permet d'obtenir des résultats (en termes de prédiction) significativement meilleurs par rapport à ceux obtenus en utilisant le vote majoritaire ainsi que des résultats très compétitifs avec ceux obtenus par des algorithmes performants d'apprentissage supervisé similaires. Ceci est effectué sans dégrader le pouvoir descriptif (explicatif) du modèle global.
**** *annee_2018  *idArt_1268
En classification multi-labels, chaque instance est associée àun ou plusieurs labels. Par exemple, un morceau de musique peut être associé aux labels 'heureux' et 'relaxant'. Des relations de co-occurrence peuvent exister entre les labels : par exemple, les labels 'heureux' et 'triste' ne peuvent pas être associés au même morceau de musique. Les labels peuvent aussi avoir des relations de préférence : par exemple, pour un morceau de musique contenant plusieurs piques, le label 'heureux' est préféré par rapport au label 'relaxant'. Les relations entre les labels peuvent aider à mieux prédire les labels associés aux instances. Les approches existantes peuvent apprendre soit les relations de co-occurrence, soit les relations de préférence. Ce travail introduit une approche permettant de combiner l'apprentissage des deux types de relations. Les expérimentations menées montrent que la nouvelle approche introduite offre les meilleurs résultats de prédiction par rapports à cinq approches de l'état de l'art.
**** *annee_2018  *idArt_1269
Les tests A/B sont des procédures utilisées par les entreprises du web et de la santé entre autres, pour mesurer l'impact d'un changement de version d'une variable par rapport à un objectif. Bien qu'un nombre de plus en plus important de données soit disponible, la mise en place concrète d'un tel test peut impliquer un coût important relatif à l'observation et à l'évaluation d'une variation lorsque celle-ci n'est pas optimale.Dans ce papier, nous présentons une nouvelle approche intégrant le principe d'un bandit contextuel prenant en compte ces variables via une procédure de stratification.
**** *annee_2018  *idArt_1271
Le Clustering Collaboratif (CC) vise à faire ressortir les structures communes présentes dans plusieurs vues indépendantes en se basant sur une première étape de clustering locale, effectuée dans notre cas à l'aide de Cartes Auto-Organisatrices (SOM pour Self Organizing Maps en anglais). Pour faire face à la quantité toujours croissante de données disponibles, l'utilisation de méthodes de clustering incrémentales est devenue nécessaire. Ce papier présente un algorithme de SOM incrémentales compatibles avec les contraintes du CC. Les expérimentations conduites sur plusieurs jeux de données démontrent la validité de cette méthode et présentent l'influence de la taille du batch utilisé lors de l'apprentissage.
**** *annee_2018  *idArt_1273
La plupart des méthodes de classification sont conçues pour des types particuliers de données: données numériques, textuelles, catégoriques, fonctionnelles, probabilistes ou encore de type graphes. Cependant, les données générées dans notre quotidien sont en général composées de données de types mixtes. Par exemple, si nous considérons la prévention cardiaque dans le domaine de la santé, les applications vont combiner des données issues de capteurs avec d'autres données telles que l'âge, le niveau d'effort, la fréquence cardiaque maximale, des histogrammes de fréquences cardiaques moyennes lors de précédents efforts, etc. Ceci nous amène à la problématique de construire des classes en tenant compte de ces différentes données, et de définir une mesure de similarité à partir des similarités de paires d'objets sur les différents types de variables. Dans cet article nous proposons une méthode de classification basée sur la fusion des matrices de similarité à l'aide des moyennes quasi-arithmétiques qui permet de choisir les différentes dimensions des données à considérer, et ce quel que soit le type de données, pour autant qu'une mesure, de similarité ou de dissimilarité existe pour chacun des types de données, ce qui est très souvent le cas.
**** *annee_2018  *idArt_1275
Définir l'importance des nœuds dans les réseaux statiques est une question de recherche très étudiée depuis de nombreuses années. Dernièrement,des adaptations des métriques classiques ont été proposées pour les réseaux dynamiques.Ces méthodes reposent sur des approches très différentes dans leur façon d'évaluer l'importance des nœuds à un instant donné. Il est donc nécessaire de pouvoir les évaluer et les comparer. Dans cet article, nous comparons trois approches existes pour mieux comprendre ce qui les différencie. Nous montrons que la nature des jeux de données influe grandement sur le comportement des méthodes, et que pour certains d'entre eux, la notion d'importance n'est pas toujours pertinente.
**** *annee_2018  *idArt_1276
La tâche de similarité sémantique textuelle consiste à exprimer automatiquement un nombre reflétant la similarité sémantique de deux fragments de texte. Chaque année depuis 2012, les campagnes de SemEval déroulent cette tâche de similarité sémantique textuelle. Cet article présente une méthode associant différentes représentations vectorielles de phrases dans l'objectif d'améliorer les résultats obtenus en similarité sémantique. Notre hypothèse est que différentes représentations permettraient de représenter différents aspects sémantiques,et par extension, d'améliorer les similarités calculées, la principale difficulté étant de sélectionner les représentations les plus complémentaires pour cette tâche. Notre système se base sur le système vainqueur de la campagne de2015 ainsi que sur notre méthode de sélection par complémentarité. Les résultats obtenus viennent confirmer l'intérêt de cette méthode lorsqu'ils sont comparé saux résultats de la campagne de 2016.
**** *annee_2018  *idArt_1277
L'émergence de l'IoT et du traitement en temps-réel oblige les entreprises à considérer la détection d'anomalies comme un élément clé de leur activité. Afin de garantir une haute précision dans le processus de détection, des métadonnées fournissant un contexte spatio-temporel sur les mesures des capteurs sont nécessaires. Dans cet article, nous présentons un système générique qui aide à capturer, analyser, qualifier et stocker les informations contextuelles d'un domaine d'application donné. L'approche proposée est basée sur des méthodes sémantiques qui exploitent des ontologies pour évaluer la pertinence de l'information contextuelle. Après une description des composants principaux de l'architecture, la performance et la pertinence du système sont démontrées par une évaluation sur des ensembles de données du monde réel.
**** *annee_2018  *idArt_1278
L'article définit les contraintes prescriptives comme des règles permettant aux moteurs d'inférence de vérifier que certains objets formels sont réellement utilisés  pas seulement inférés  ou non, dans certaines conditions. Il montre que ces contraintes nécessitent de ne pas exploiter de mécanisme d'héritage (ou autres mécanismes ajoutant des relations à des objets) durant les tests des conclusions des règles. Il donne une méthode générale pour effectuer cela et des commandes SPARQL pour implémenter cette méthode lorsque les règles sont représentées via des relations sous-classe-de entre conditions et conclusions. L'article illustre ces commandes avec la vérification de patrons de conception d'ontologies. Plus généralement, l'approche peut être utilisée pour vérifier la complétude d'une ontologie, ou représenter dans une ontologie (plutôt que par des requêtes ou des procédures ad hoc) des contraintes permettant de calculer un degré de complétude d'ontologie. L'approche peut ainsi aider l'élicitation, la modélisation ou la validation de connaissances.
**** *annee_2018  *idArt_1279
Nous nous intéressons aux treillis distributifs dans le cadre de l'analyse formelle de concepts (FCA). La motivation primitive vient de la phylogénie et des graphes médians pour représenter les dérivations biologiques et les arbres parcimonieux. La FCA propose des algorithmes efficaces de construction de treillis de concepts. Cependant, un treillis de concepts n'est pas en correspondance avec un graphe médian sauf s'il est distributif, d'où l'idée d'étudier la transformation d'un treillis de concepts en un treillis distributif. Pour ce faire, nous nous appuyons sur le théorème de représentation de Birkhoff qui nous permet de systématiser la transformation d'un contexte quelconque en un contexte de treillis de concepts distributif. Ainsi, nous pouvons bénéficier de l'algorithmique de FCA pour construire mais aussi visualiser les treillis de concepts distributifs, et enfin étudier les graphes médians associés.
**** *annee_2018  *idArt_1280
Les données séquentielles sont aujourd'hui omniprésentes et concernent divers domaines d'application. La fouille de données de séquences permet d'extraire des informations et des connaissances pouvant être à forte valeur ajoutée. Cependant, lorsque les données de séquences sont riches en données numériques, des méthodes de fouille de données plus fines sont nécessaires pour extraire des connaissances plus expressives représentant la variabilité des valeurs numériques ainsi que leur éventuelle interdépendance. Dans cet article,nous présentons une nouvelle méthode de découverte de séquences graduelles fréquentes représentées par des graphes à partir d'une source de données de séquence sen RDF (Resource Description Framework). Ces dernières sont transformées en graphes graduels partiellement ordonnés, gpo. Nous proposons un algorithme permettant de découvrir les sous-graphes gpo fréquents. Une expérimentation sur deux jeux de données réelles ont montré la faisabilité et la pertinence de notre approche.
**** *annee_2018  *idArt_1281
DBpédia, qui encode les connaissances de Wikipédia, est devenue une base de référence pour le web des données. Les ressources peuvent y être répertoriées par des catégories définies manuellement, dont la sémantique n'est pas directement accessible par des machines. Dans cet article, nous proposons de remédier à cette lacune au moyen de méthodes de fouille de données, à savoir la recherche de règles d'associations et de motifs apparentés. Nous présentons une étude comparative de ces variantes sur une partie de DBpédia et discutons le potentiel des différentes approches.
**** *annee_2018  *idArt_1283
L'échantillonnage de motifs est une méthode non-exhaustive pour découvrir des motifs pertinents qui assure une bonne interactivité tout en offrant des garanties statistiques fortes grâce à sa nature aléatoire. Curieusement, une telle approche explorée pour les motifs ensemblistes et les sous-graphes ne l'a pas encore été pour les données séquentielles. Dans cet article, nous proposons la première méthode d'échantillonnage de motifs séquentiels. Outre le passage aux séquences, l'originalité de notre approche est d'introduire une contrainte sur la norme pour maîtriser la longueur des motifs tirés et éviter l'écueil de la « longue traîne ». Nous démontrons que notre méthode fondée sur une procédure aléatoire en deux étapes effectue un tirage exact. Malgré le recours à un échantillonnage avec rejet, les expérimentations montrent qu'elle reste performante.
**** *annee_2018  *idArt_1286
Un lien inter-langue dans Wikipédia est un lien qui mène d'un article appartenant à une édition linguistique à un autre article décrivant le même concept dans une autre langue. Ces liens sont ajoutés manuellement par les utilisateurs de Wikipédia et ainsi ils sont susceptibles d'être erronés. Dans ce papier,nous proposons une approche pour l'élimination automatique des liens inter-langues. Le principe de base est que la présence d'un lien erroné est révélée par l'existence d'un chemin de liens inter-langues reliant deux articles appartenant à une même édition linguistique. Notre approche élimine des liens inter-langues,à partir de ceux qui ont un faible score de correction, jusqu'à ce qu'il n'y ait plus de chemins entre deux articles d'une même édition linguistique. Les résultats de notre évaluation sur un sous-graphe de Wikipédia consistant en 8 langues montre que l'approche est prometteuse.
**** *annee_2018  *idArt_1287
Dans cet article, nous présentons une méthode d'analyse de corpus afin de générer deux interfaces originales de visualisation dans le domaine de l'e-recrutement. Notre approche s'appuie sur des millions de profils issus de plusieurs réseaux sociaux et sur des milliers d'offres d'emploi collectées sur Internet. Nous décrivons dans ces travaux les étapes nécessaires pour leur réalisation.La première visualisation est une carte dynamique indiquant les métiers qui recrutent, dans quel domaine, dans quelle région tandis que la seconde met en avant les parcours professionnels et permet d'observer les perspectives ainsi que les antécédents à plus ou moins long terme pour chaque métier considéré.
**** *annee_2018  *idArt_1288
Dans les corpus de textes scientifiques, certains articles issus de communautés de chercheurs différentes peuvent ne pas être décrits par les mêmes mots-clés alors qu'ils partagent la même thématique. Ce phénomène cause des problèmes dans la recherche d'information, ces articles étant mal indexés, et limite les échanges potentiellement fructueux entre disciplines scientifiques.Notre modèle permet d'attribuer automatiquement une étiquette thématique aux articles au moyen d'un apprentissage des représentations sémantiques d'articles du corpus déjà étiquetés. Passant bien à l'échelle, cette méthode a pu être testée sur une bibliothèque numérique d'articles scientifiques comportant des millions de documents. Nous utilisons un réseau sémantique de synonymes pour extraire davantage d'articles sémantiquement similaires et nous les fusionnons avec ceux obtenus par un modèle de classement thématique. Cette méthode combinée présente de meilleurs taux de rappel que les versions utilisant soit le réseau sémantique seul, soit la seule représentation sémantique des textes.
**** *annee_2018  *idArt_1290
Cet article présente les investigations menées sur les données mesurées par des capteurs positionnés dans cinq villes de l'île de la Réunion. Des analyses exploratoires préalables permettent de comparer les caractéristiques statistiques des villes considérées relativement aux différentes variables météorologiques mesurées (flux solaires diffus et global, pression atmosphérique, humidité, température, force et direction du vent). Nous appliquons diverses transformations sur les données avant d'analyser les séries uni-variées ou multivariées agrégées au pas de l'heure ou de la journée afin de construire des modèles de prédiction. Une approche classique de clustering de séries temporelles est testée. Deux algorithmes de bi-clustering appliqués successivement ont permis de grouper les journées d'observations partageant des paramètres météorologiques horaires. Une caractérisation des bi-clusters, une visualisation calendaire de leur succession ainsi qu'une recherche de séquences fréquentes permettent d'exploiter les résultats et de faciliter leur interprétation.
**** *annee_2018  *idArt_1291
Sur Internet, l'information se propage en particulier au travers des documents textuels. Cette propagation soulève de nombreux défis : identifier une information, suivre son évolution dans le temps, comprendre les mécanismes qui régissent sa propagation, etc. Étant donné un document parmi un grand corpus dans lequel de nombreuses informations circulent, pouvons-nous retrouver les chemins empruntés par l'information pour arriver à ce document ? Nous proposons de définir la notion de trajectoire comme l'ensemble des chemins le long desquels de l'information s'est propagée et nous proposons une méthode pour l'estimer. Nous avons mis en œuvre une évaluation humaine pour juger de la qualité des chemins calculés. Nous montrons que les évaluations concordent la plupart du temps et que notre algorithme est efficace pour retrouver les bons chemins.
**** *annee_2018  *idArt_1292
Cet article propose une méthode d'analyse pour des enregistrements opérationnels d'un ensemble de compteurs d'essieux, qui constituent un élément central à l'infrastructure ferroviaire. Notre objectif est de fournir une façon efficace d'extraire automatiquement des éléments de connaissance concernant les défaillances de ces systèmes.Puisque les données fournies ne contiennent pas de vérité de terrain sur les causes de défaillances, les informations et leurs causes doivent être extraites des relations sous-tendant les événements enregistrés. Après une phase de prétraitement,les événements sont groupés en fonction des relations qui ont été mises en lumière entre eux. Ces regroupements peuvent ensuite être utilisés pour créer des classes d'événements en utilisant un système de classification adapté.Au delà de cette application spécifique, cette approche est une façon nouvelle d'aborder les problèmes d'analyse de fiabilité.
**** *annee_2018  *idArt_1294
Dans cet article nous étudions le problème de l'extraction de motifs fréquents contenant des événements positifs, des événements négatifs spécifiant l'absence d'événement ainsi que des informations temporelles sur le délai entrec es événements. Nous définissons la sémantique de tels motifs et proposons la méthode NTGSP basée sur des approches de l'état de l'art. Les performances de la méthode sont évaluées sur des données commerciales fournies par EDF (Électricité de France).
**** *annee_2018  *idArt_1295
Les systèmes orientés documents permettent de stocker tout document, quel que soit leur schéma. Cette flexibilité génère une potentielle hétérogénéité des documents qui complexifie leur interrogation car une même entité peut être décrite selon des schémas différents. Cet article présente une approche d'interrogation transparente des systèmes orientés documents. Pour cela, nous proposons de générer un dictionnaire de façon automatique lors de l'insertion des documents, et qui associe à chaque attribut tous les chemins permettant d'y accéder. Ce dictionnaire permet de réécrire la requête utilisateur à partir de dis-jonctions de chemins afin de retrouver tous les documents quelles que soient leurs structures. Nos expérimentations montrent des coûts d'exécution de la requête réécrite largement acceptables comparés au coût d'une requête sur schémas homogènes.
**** *annee_2018  *idArt_1297
Les systèmes de recommandation ont pour rôle d'aider les utilisateurs submergés par la quantité d'information à faire de bons choix à partir de vastes catalogues de produits. Le déploiement de ces systèmes dans l'industrie hôtelière est confronté à des contraintes spécifiques, limitant la performance des approches traditionnelles. Les systèmes de recommandation d'hôtels souffrent en particulier d'un problème de démarrage à froid continu à cause de la volatilité des préférences des voyageurs et du changement de comportements en fonction du contexte. Dans cet article, nous présentons le problème de recommandation d'hôtels ainsi que ses caractéristiques distinctives. Nous proposons de nouvelles méthodes contextuelles qui prennent en compte les dimensions géographique et temporelle ainsi que la raison du voyage, afin de générer les listes de recommandation.Nos expérimentations sur des jeux de données réels soulignent la contribution des données contextuelles à l'amélioration de la qualité de recommandation.
**** *annee_2018  *idArt_1298
L'intégration des données hétérogènes en Sciences de la Vie est un sujet de recherche majeur. L'importance et le volume considérable des informations sur les milieux de vie des micro-organismes dans tous les domaines tels que la santé, l'agriculture ou l'environnement justifie le développement de traitements automatisés. Nous proposons ici l'ontologie OntoBiotope dont nous décrivons les principes de construction ainsi que des exemples d'utilisation pour l'annotation et l'indexation sémantique des habitats microbiens décrits en langue naturelle dans les documents scientifiques.
**** *annee_2018  *idArt_1302
La multiplicité des enquêtes d'opinion sur un même sujet nécessite la construction de synthèses qui agrègent les résultats obtenus dans des conditions indépendantes. Dans cet article, nous proposons une nouvelle approche ordinale de méta-analyse qui consiste à rechercher un ordre consensus qui rend compte « au mieux » des ordres partiels entre les modalités issus des résultats des différentes enquêtes. Nous modélisons ce problème par une variante d'une recherche d'un ordre médian sur les sommets d'un graphe orienté pondéré et nous développons un algorithme de séparation-évaluation pour le résoudre. Notre approche est appliquée sur un ensemble d'enquêtes internationales portant sur les motivations et les freins à l'intégration de l'Internet des Objets dans les entreprises.
**** *annee_2018  *idArt_1303
Ce papier propose une méthode basée sur la théorie des ensembles approximatifs et dédiée à l'apprentissage supervisé incrémental dans un contexte de données déséquilibrées. Cette méthode consiste en trois phases : la construction d'une table de décision, l'inférence d'un ensemble de règles de décision et la classification de chaque action potentielle dans l'une des classes de décision prédéfinies. La méthode MAI2P est validée dans le contexte des MOOCs (Massive Open Online Courses).
**** *annee_2018  *idArt_1305
Avec l'avènement des méga-données, l'informatique décisionnelle a dû trouver des solutions pour gérer des données de très grands volume et variété.Les lacs de données (data lakes) répondent à ces besoins du point du vue du stockage, mais nécessitent la gestion de métadonnées adéquates pour garantir un accès efficace aux données. Sur la base d'un modèle multidimensionnel de métadonnées conçu pour un lac de données présentant un défaut d'évolutivité de schéma, nous proposons l'utilisation d'un data vault pour traiter ce problème. Pour montrer la faisabilité de cette approche, nous instancions notre modèle conceptuel de métadonnées en modèles logiques et physiques relationnel et orienté document. Nous comparons également les modèles physiques en termes de stockage et de temps de réponse aux requêtes sur les métadonnées.
**** *annee_2018  *idArt_1308
Dans cet article, nous nous intéressons à l'optimisation du processus de recherche de clusters de liens. Nous proposons en particulier l'algorithme PALM (Stattner et al., 2017), qui vise à améliorer l'efficacité du processus d'extraction par l'exploration conjointe de plusieurs zones de l'espace de recherche. Ainsi, nous commençons par démontrer que l'espace des solutions forme un treillis de concepts. Nous proposons ensuite une approche qui explore en parallèle les branches de ce treillis tout en réduisant l'espace de recherche en s'appuyant sur différentes propriétés. Les bonnes performances de notre algorithme sont démontrées en le comparant avec l'algorithme d'extraction d'origine.
**** *annee_2018  *idArt_1311
Cet article décrit une approche flexible pour la prédiction à court terme de variables météorologiques. En particulier, nous nous intéressons à la prédiction du rayonnement solaire à une heure. Cette tâche est d'une grande importance pratique dans l'optique d'optimiser les ressources énergétiques solaires. Comme le défi EGC 2018 nous fournit des données météorologiques enregistrées sur cinq sites géographiques de l'île de la Réunion, nous utilisons ces données historiques comme base pour créer des modèles de prédiction, et nous testons la performance de ces modèles selon le site considéré. Après avoir décrit notre méthode de nettoyage de données et de  normalisation, nous combinons une méthode de sélection de variables basée sur les modèles ARIMA (AutoRegressive Integrated Moving Average) à l'utilisation de méthodes de régression génériques, telles que les arbres de régression et les réseaux de neurones.
**** *annee_2018  *idArt_1312
Dans un problème de classification supervisée, les données d'apprentissage proviennent souvent d'inventaires acquis sur le terrain par des experts du domaine. Toutefois, la localisation de ces inventaires est approximative (en raison de la précision intrinsèque des GPS portables utilisés). Cette imprécision spatiale est particulièrement problématique lorsque ces données sont utilisées pour entraîner un classifieur sur des images satellitaires très haute résolution(THR). En effet, la précision spatiale des inventaires peut être dans certains cas bien inférieure à celles de ces images. Dans ce papier, nous proposons trois approches visant à améliorer la précision spatiale des données terrain via des prétraitements.Le principe est d'exploiter les images satellitaires THR disponibles pour corriger spatialement les données terrain. Nos expérimentations mettent en avant l'intérêt de ces pré-traitements sur un jeu de données constitué de 24inventaires d'habitats coralliens et une image satellitaire THR (WorldView-2).
**** *annee_2018  *idArt_1314
Le financement participatif est un mode de financement d'un projet faisant appel à un grand nombre de personnes qui a connu une forte croissance avec l'émergence d'Internet et des réseaux sociaux. Cependant plus de 60 % des projets ne sont pas financés, il est donc important de bien préparer sa campagne de financement. De plus, en cours de campagne, il est crucial d'avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration, communication) : des outils de prédiction sont alors indispensables. Nous proposons dans cet article plusieurs pistes d'amélioration pour la prédiction du montant levé lors d'une campagne de financement participatif en utilisant l'algorithme k-NN. La première proposition consiste à utiliser un algorithme de clustering afin de segmenter l'ensemble d'apprentissage et faciliter le passage à l'échelle. La seconde proposition consiste à extraire des caractéristiques pertinentes depuis les séries temporelles et les informations sur les campagnes pour avoir une représentation vectorielle.
**** *annee_2018  *idArt_1316
Récemment, la recherche par mots-clés dans les bases de données relationnelles a suscité un intérêt grandissant en raison de sa facilité d'utilisation. Bien que des recherches approfondies fussent dernièrement effectuées dans ce contexte, la plupart de ces recherches non seulement nécessitent un accès préalable aux données, ce qui restreint leur applicabilité si cette condition n'est pas vérifiée,mais aussi renvoient des réponses très génériques. Cependant, fournir aux utilisateurs des réponses personnalisées est devenu plus que jamais nécessaire en raison de la surabondance de données qui peut déranger l'utilisateur. Le défi de retourner des réponses pertinentes et personnalisées qui satisfont les besoins des utilisateurs demeure. Inspiré par l'application réussie de la technique de filtrage collaboratif dans les systèmes de recommandation, nous proposons une nouvelle approche basée sur les mots-clés pour fournir aux utilisateurs des résultats personnalisés basés sur l'hypothèse que seulement une information sur le schéma de la base de données est disponible.
**** *annee_2018  *idArt_1318
Les modèles de classification discriminante supposent que les données de formation et de déploiement ont les mêmes distributions d'attributs de données. Ces modèles donnent des performances très variées lorsqu'ils sont déployés dans des conditions variées avec différentes distributions de données. Ce phénomène est appelé Dataset Shift. Dans cet article, nous avons fourni une méthode qui détermine d'abord s'il y a un changement significatif dans les distributions d'attributs entre les ensembles de données d'apprentissage et de déploiement. S'il existe un changement dans les données, la méthode proposée utilise ensuite une approche de Hill climbing pour cartographier ce décalage, quelle que soit sa nature, c'est-à-dire (linéaire ou non linéaire) à l'équation pour la transformation quadratique. Les résultats expérimentaux sur trois jeux de données réels montrent de forts gains de performance obtenus par la méthode proposée par rapport aux méthodes précédemment établies telles que le reconditionnement et le recadrage linéaire.
**** *annee_2018  *idArt_1319
La date de pose est souvent un facteur principal d'explication de la dégradation des conduites d'assainissement. Pour les gestionnaires de ces réseaux, connaître cette information permet ainsi (par l'utilisation de modèles de détérioration)de prédire l'état de santé actuel des conduites non encore inspectées.Cette connaissance est primordiale pour prendre des décisions dans un contexte de forte contrainte budgétaire. L'objectif est ainsi de reconstituer ces dates de pose à partir des caractéristiques du patrimoine et de son environnement. Les données à manipuler présentent plusieurs niveaux de complexité importants.Leurs sources sont hétérogènes, leur volume est important et les informations sur leur étiquetage (dates) sont limitées : seulement 24 % du linéaire est connu pour les réseaux d'assainissement de la métropole de Lyon. La base de données sous-jacente contient les caractéristiques connues des conduites (profil géométrique,matériau utilisé, etc.). Dans ce papier, nous proposons de mesurer l'effet et l'impact de quelques méthodes d'apprentissage statistique semi-supervisé, et de proposer ainsi une approche alternative adaptée à la reconstitution de ce type de données.
**** *annee_2018  *idArt_1320
L'évaluation périodique du risque de chute des personnes âgées requiert des informations fiables et nombreuses. Comme il n'est pas possible de recueillir régulièrement toutes ces informations, les observations sont faites au fil du temps et conservées, ce qui entraîne une problématique liée au vieillissement des informations. Cet article traite de la détection des informations obsolètes dans une base d'informations sur une personne âgée. Nous proposons une solution comportant un modèle de connaissances sur les personnes âgées sous forme d'un réseau bayésien et un module de raisonnement chargé de la détection et de la gestion des contradictions et des doutes sur les informations.
**** *annee_2018  *idArt_1322
L'objectif de ce travail est de décrire avec une approche réaliste la signification des données d'observation en neuro-imagerie sous un format formel pour faciliter leur interprétation par les cliniciens et leur réutilisation dans d'autres contextes.
**** *annee_2018  *idArt_1323
Dans le champ des sciences patrimoniales, la dimension temporelle de l'information joue un rôle à l'évidence majeur tant pour l'interpréter et l'analyser que pour relier des faits isolés. Mais la façon dont cette dimension est verbalisée pose des problèmes de formalisation non triviaux. Pourtant, cette verbalisation, que l'on associe souvent au terme-chapeau d'incertitude, peut être lue en dissociant d'une part le caractère mal connu d'un fait documenté, irréductible, et les choix faits par le producteur de l'information pour la relativiser. Dans cette contribution nous proposons un modèle formel permettant d'observer et d'analyser de façon systématique cette couche de verbalisation. L'expérience est menée sur des données fortement hétérogènes, souvent d'origine citoyenne, documentant le petit patrimoine matériel et immatériel. Ce cas d'étude est donc limité, mais il apparait néanmoins comme portant une question de fond allant au-delà du cas d'espèce. La contribution détaille d'abord la grille d'analyse d'indices temporels proposée, puis relate l'expérimentation concrète associée (ontologie OWL). Il n'est pas fait état d'une quelconque prétention à un résultat généralisable stricto sensu, mais cette expérience peut contribuer à nourrir de façcon pragmatique un débat nécessaire sur la formalisation d'indices temporels dans les sciences historiques.
**** *annee_2018  *idArt_1324
Nous proposons un modèle de co-clustering de données mixtes et un critère Bayésien de sélection du meilleur modèle. Le modèle infère automatiquement les discrétisations optimales de toutes les variables et effectue un co-clustering en minimisant un critère Bayésien de sélection de modèle. Un avantagede cette approche est qu'elle ne nécessite aucun paramètre utilisateur. De plus, le critère proposé mesure de façon exacte la qualité d'un modèle tout en étant régularisé. L'optimisation de ce critère permet donc d'améliorer continuellement les modèles trouvés sans pour autant sur-apprendre les données. Les expériences réalisées sur des données réelles montrent l'intérêt de cette approche pour l'analyse exploratoire des grandes bases de données.
**** *annee_2018  *idArt_1325
Les portails d'actualités en ligne produisent un flux d'information ayant un volume et une vélocité importants. Dans ce contexte, il devient plus difficile de proposer en temps réel des recommandations dynamiques adaptées aux intérêts de chaque utilisateur. Dans cet article, nous présentons une approche hybride pour la recommandation des articles d'actualité reposant sur l'analyse sémantique du contenu disponible. L'approche est basée sur l'hybridation de plusieurs approches personnalisées et non personnalisées pour remédier au problème de démarrage à froid. L'expérimentation de notre approche dans un environnement à large échelle et à fortes contraintes temps réel dans le cadre du challenge NEWSREEL a permis d'évaluer la qualité de ses recommandations etde confirmer l'apport de la sémantique dans le processus de recommandation.
**** *annee_2018  *idArt_1326
Nous présentons dans ce travail une méthode de désagrégation pour l'estimation de population à l'échelle locale à partir de données ouvertes globales. Notre but est d'estimer notamment le nombre de personnes résidant dans chaque bâtiment de la zone d'intérêt, à partir de données à plus grande échelle. Une description fine à l'échelle résidentielle est tout d'abord effectuée à partir des données d'OpenStreetMap. Les surfaces des bâtiments d'habitation ou d'usage mixte (habitation et activités) sont notamment identifiées. Nous effectuons ensuite une désagrégation à partir de données de grille de population à grande échelle (1km2 par carreau), guidée par les surfaces des bâtiments compris dans chaque carreau de la grille. Ensuite, nous effectuons une désagrégation à partir de données de grille de population à grande échelle (1km2 par carreau), guidée par les distributions spatiales découvertes à l'étape précédente. Nous utilisons exclusivement des données ouvertes pour favoriser la réplicabilité et pour pouvoir appliquer notre méthode à toute région d'intérêt, pour peu que la qualité des données soit suffisante. L'évaluation et la validation du résultat dans le cas de plusieurs villes Françaises sont effectuées à l'aide de données de recensement INSEE.
**** *annee_2018  *idArt_1329
Avec l'avènement des réseaux sociaux et la multiplication des messages produits au sujet des entreprises, mieux comprendre les retours clients est devenu un enjeu primordial. Des techniques de classification automatique et de modélisation thématique permettent d'ores et déjà d'observer les principales tendances observées dans ces données. Il est intéressant, dans une optique d'anticipation,d'observer les thématiques émergentes et de les identifier avant qu'elles ne prennent de l'ampleur. Afin de résoudre cette problématique, nous avons étudié la piste de l'utilisation de modèles LDA pour détecter les documents relatifs à ces thématiques émergentes. Nous avons testé trois systèmes sur plusieurs scénarios d'arrivées de la nouveauté dans le flux de données. Nous montrons que les modèles thématiques permettent de détecter cette nouveauté mais que cela dépend du scénario envisagé.
**** *annee_2015  *numText_2
La segmentation et annotation de maillages utilisant la sémantique a été l'objet d'un intérêt grandissant avec la démocratisation des techniques de reconstruction 3D. Une approche classique consiste à réaliser cette tâche en deux étapes, tout d'abord en segmentant le maillage, puis en l'annotant. Cependant, cette approche ne permet pas à chaque étape de profiter de l'autre. En traitement d'images, quelques méthodes combinent la segmentation et l'annotation, mais ces approches ne sont pas génériques, et nécessitent des ajustements d'implémentation ou des réécritures pour chaque modification des connaissances expertes. Dans ce travail, nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d'étapes de segmentation, et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche.Notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés d'un objet (géométrie, topologie, etc.), ainsi que des algorithmes permettant de détecter ces concepts. Cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique. La description formelle du domaine est alors utilisée pour réaliser automatiquement l'assemblage de la segmentation et de l'annotation d'objets et de leurs propriétés, en sélectionnant à chaque étape l'algorithme le plus pertinent, étant données les information sémantiques déjà détectées. Cette approche originale comporte plusieurs avantages. Tout d'abord, elle permet de segmenter et d'annoter des objets sans aucune connaissance en traitement d'images ou de maillages, en décrivant uniquement les propriétés de l'objet en terme de concepts ontologiques. De plus, ce cadre de fonctionnement peut facilement être réutilisé et appliqué à différents contextes, dès lors qu'une ontologie de domaine a été définie. Finalement, la réalisation conjointe de la segmentation et de l'annotation permet d'utiliser d'une manière efficace la connaissance experte, en réduisant les erreurs de segmentation et le temps de calcul, en lançant toujours l'algorithme le plus pertinent.
**** *annee_2015  *numText_3
Cet article présente une analyse détaillée d'un ensemble de 2 millions de résultats de recherche d'information obtenus par différents paramétrages de systèmes de recherche d'information. Plus spécifiquement, nous avons utilisé la plate-forme Terrier et l'interface RunGeneration pour créer différentes exécutions (run en anglais) en modifiant les modèles d'indexation et de recherche. Nous avons ensuite évalué chacun des résultats obtenus selon différentes mesures de performance de recherche d'information. Une analyse systématique a été menée sur ces données afin de déterminer d'une part quels étaient les paramètres qui ont le plus d'influence, d'autre part quels étaient les valeurs de ces paramètres les plus susceptibles de conduire à de bonnes performances du système.
**** *annee_2015  *numText_4
L'analyse d'opinions est une tâche qui consiste en l'identification et la classification de textes subjectifs. Dans ce travail, nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille sur le Web. Nous proposons une approche pour visualiser les résultats d'analyse d'opinions, basée sur l'utilisation de termes clés. Nous décrivons également la plate-forme de veille sur leWeb AMIEI, au sein de laquelle notre approche a été implémentée. La démonstration consistera en une expérimentation de la plate-forme de veille AMIEI et du module d'analyse d'opinions sur un corpus de tweets politiques.
**** *annee_2015  *numText_5
Le projet ANR IMAGIWEB dans lequel s'inscrit ce travail s'est donné pour mission d'étudier les images véhiculées sur Internet en se basant sur la détection d'opinions. Deux cas d'étude ont été définis : (1) le premier vise à répondre aux besoins d'analyse de chercheurs en science politique grâce à des données issues de Twitter durant la campagne présidentielle de 2012 ; (2) le second doit permettre à l'entreprise française EDF d'évaluer l'opinion du public en matière de sécurité, d'emploi et de prix à partir de billets de blogs. Dans cet article, nous présentons un retour d'expérience sur l'usage de l'analyse en ligne OLAP (OnLine Analytical Processing) pour des données textuelles, mettant en avant l'intérêt de ce type d'analyse pour les membres du projet.
**** *annee_2015  *numText_6
La démocratisation d'Internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les états et les entreprises. Le côté déplaisant de cette interconnexion mondiale des systèmes d'information réside dans un phénomène appelé Cybercriminalité. Nous proposons une méthode de visualisation de grands graphes et l'exploitation d'analyses statiques des flux permettant de détecter les comportements anormaux et dangereux afin d'appréhender les risques d'une façon compréhensible par tous les acteurs.
**** *annee_2015  *numText_12
Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local.Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la meilleure mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la meilleure mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique.Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la meilleur mesure de proximité discriminante.
**** *annee_2015  *numText_13
Ce papier propose une version améliorée de l'algorithme de classification automatique évidentielle semi-supervisée SECM. Celui-ci bénéficie de l'introduction de données étiquetées pour améliorer la pertinence de ses résultats et utilise la théorie des fonctions de croyance afin de produire une partition crédale qui généralise notamment les concepts de partitions dures et floues. Le pendant de ce gain d'expressivité est une complexité qui est exponentielle avec le nombre de classes, ce qui impose en retour l'utilisation de schémas efficaces pour optimiser la fonction objectif. Nous proposons dans cet article une heuristique qui relâche la contrainte classique de positivité liée aux masses de croyances des méthodes évidentielles. Nous montrons sur un ensemble de jeux de données de test que notre méthode d'optimisation permet d'accélérer sensiblement l'algorithme SECM avec un schéma d'optimisation classique, tout en améliorant également la qualité de la fonction objectif.
**** *annee_2015  *numText_14
Cet article présente une solution centrée sur les ontologies pour la classification multi-label automatique d'information nécessaire à un système de recommandation d'informations économiques.
**** *annee_2015  *numText_15
Actuellement, le clustering de flux de données devient le moyen le plus efficace pour partitionner un très grand ensemble de données. Dans cet article, nous présentons une nouvelle approche topologique, appelée G-Stream, pour le clustering de flux de données évolutives. La méthode proposée est une extension de l'algorithme GNG (Growing Neural Gas) pour gérer le flux de données. G-Stream permet de découvrir de manière incrémentale des clusters de formes arbitraires en ne faisant qu'une seule passe sur les données. Les performances de l'algorithme proposé sont évaluées à la fois sur des données synthétiques et réelles.
**** *annee_2015  *numText_16
Le maintien de la qualité et de la fiabilité de bases de connaissances RDF du Web Sémantique est un problème courant. De nombreuses propositions pour l'intégration de « bonnes » données ont été faites, se basant soit sur les ontologies de ces bases, soit sur des méta-données additionnelles. Dans cet article, nous proposons une approche originale, basée exclusivement sur l'étude des données de la base. Le principe est de déterminer si les modifications apportées par la mise à jour candidate rendent la partie ciblée de la base plus similaire - selon certains critères - à d'autres parties existantes dans la base. La mise à jour est considérée cohérente avec cette base et peut être appliquée.
**** *annee_2015  *numText_17
La modularisation de grands graphes ou recherche de communautés est abordée comme l'optimisation d'un critère de qualité, l'un des plus utilisés étant la modularité de Newman-Girvan. D'autres critères, ayant d'autres propriétés, aboutissent à des solutions différentes. Dans cet article, nous présentons une réécriture relationnelle de six critères linéaires: Zahn-Condorcet, Owsi´nski- Zadro˙zny, l'Ecart à l'Uniformité, l'Ecart à l'Indétermination et la Modularité Equilibrée. Nous utilisons une version générique de l'algorithme d'optimisation de Louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles. Les partitions obtenues présentent des caractéristiques différentes, concernant notamment le nombre de classes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. En outre, cette notation permet d'identifier facilement les critères ayant une limite de résolution (phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes). Une étude de la qualité des partitions trouvées dans les graphes synthétiques LFR permet de confirmer ces résultats.
**** *annee_2015  *numText_18
Dans de nombreux problèmes d'apprentissage automatique la performance des algorithmes est évaluée à l'aide des mesures précision et rappel. Or ces deux mesures peuvent avoir une importance très différente en fonction du contexte. Dans cet article nous étudions le comportement des principaux indices de performance en fonction du couple précision-rappel. Nous proposons un nouvel outil de visualisation de performances et définissons l'espace de compromis qui représente les différents indices en fonction du compromis précision-rappel. Nous analysons les propriétés de ce nouvel espace et mettons en évidence ses avantages par rapport à l'espace précision-rappel.
**** *annee_2015  *numText_19
L'opérateur skyline est devenu un paradigme dans les bases de données. Il consiste à localiser Sky l'ensemble des points d'un espace vectoriel qui ne sont pas dominés. Cet opérateur est utile lorsqu'on n'arrive pas à se décider dans les situations conflictuelles. Le calcul des requêtes skyline est pénalisé par le nombre de points que peuvent contenir les bases de données. Dans ce papier, nous présentons une solution analytique pour la réduction de l'espace candidat et nous proposons une méthode efficace pour le calcul de ce type de requêtes
**** *annee_2015  *numText_20
Ce travail se situe dans le domaine de la Cybersécurité, le projet D113 permet de visualiser en temps réel les flux transitant sur des équipements de filtrage sans avoir recours au traitement manuel des journaux d'événements. Nous centrerons notre démonstration sur la visualisation de grands graphes et l'exploitation d'analyses statiques des flux.
**** *annee_2015  *numText_21
Cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue une première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».e première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».
**** *annee_2015  *numText_22
Dans le cadre de la détection du plagiat, la phase de comparaison de deux documents est souvent réduite à une comparaison mot à mot, une recherche de « copier/coller ». Dans cet article, nous proposons une approche naïve de comparaison de deux documents dans le but de détecter automatiquement aussi bien les phrases copiées de l'un des textes dans l'autre que les paraphrases et reformulations, ceci en se focalisant sur l'existence des mots porteurs de sens, ainsi que sur leurs mots de substitution possibles. Nous comparons trois algorithmes utilisant cette approche afin de déterminer la plus efficace pour ensuite l'évaluer face à des méthodes existantes. L'objectif est de permettre la détection des similitudes entre deux textes en utilisant uniquement des mots clefs. L'approche proposée permet de détecter des reformulations non paraphrastiques impossibles à détecter avec des approches conventionnelles faisant appel à une phase d'alignement.
**** *annee_2015  *numText_23
La détection de plagiat extrinsèque devient vite inefficace lorsque l'on n'a pas accès aux documents potentiellement sources du plagiat ou lorsque l'on se confronte à un espace aussi vaste que leWeb, ce qui est souvent le cas dans les logiciels anti-plagiat actuels. Dès lors la détection intrinsèque devient nettement plus efficace. Dans cet article, nous traitons justement de la détection automatique d'auteurs qui permet de savoir si un passage d'un texte n'appartient pas au même auteur que le reste du texte et donc en théorie de repérer les passages plagiés d'un document. Nous expliquons notre contribution aux procédures déjà existantes et évaluons les limites de notre approche. L'objectif est de permettre la détection et le regroupement de passages d'un document par auteur.
**** *annee_2015  *numText_24
Le risque chimique ou alimentaire couvre les situations où les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Les experts qui assurent le contrôle et la gestion de ces substances se retrouvent face à de gros volumes de littérature scientifique, qui doit être analysée pour appuyer la prise de décisions. Nous proposons une aide automatique pour l'analyse de cette littérature. Nous abordons la tâche comme une problématique de catégorisation: il s'agit de catégoriser les phrases des textes dans les classes du risque lié aux substances. Nous utilisons deux approches: par apprentissage supervisé et la recherche d'information. Les résultats obtenus avec l'apprentissage supervisé (toute classe confondue, F-mesure autour de 0,8 pour le risque alimentaire, entre 0,61 et 0,64 pour le risque chimique) sont meilleurs que ceux obtenus avec par recherche d'information (toute classe confondue, F-mesure entre 0,18 et 0,226 pour le risque alimentaire, entre 0,20 et 0,32 pour le risque chimique). Le rappel est compétitif avec les deux approches.
**** *annee_2015  *numText_27
Un nouveau domaine de motifs appelé chemins pondérés condensés a été introduit en 2013 lors de la conférence IJCAI. Le contexte de fouille est alors un graphe acyclique orienté (DAG) dont les sommets sont étiquetés par des attributs. Nous avons travaillé à une implémentation efficace de ce type de motifs et nous montrons que l'algorithme proposé était juste mais incomplet. Nous établissons ce résultat d'incomplétude et nous l'expliquons avant de trouver une solution pour réaliser une extraction complète. Nous avons ensuite développé des structures complémentaires pour calculer efficacement tous les chemins pondérés condensés. L'algorithme est amélioré en performance de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons à des données réelles pour motiver qualitativement l'usage des chemins pondérés.
**** *annee_2015  *numText_28
Les systèmes de recommandation ont pour objectif de sélectionner et présenter d'abord les informations susceptibles d'intéresser les utilisateurs. Ce travail expose un système de recommandation qui s'appuie sur deux concepts: des relations sémantiques sur les données et une technique de filtrage collaboratif distribué basée sur la factorisation des matrices (MF). D'une part, les techniques sémantiques peuvent extraire des relations entre les données, et par conséquent, améliorer la précision des recommandations. D'autre part, MF donne des prévisions très précises avec un algorithme facilement parralélisable. Notre proposition utilise cette technique en ajoutant des relations sémantiques au processus. En effet, nous analysons en profondeur les intérêts cachés des utilisateurs dans les attributs des items à recommander. Nous utilisons dans nos expérimentations le jeu de données MovieLens enrichi par la base de données IMDb. Nous comparons notre travail à une technique MF classique. Les résultats montrent une précision dans les recommandations, tout en préservant un niveau élevé d'abstraction du domaine. En outre, nous améliorons le passage à l'échelle du système en utilisant des techniques parallélisables.
**** *annee_2015  *numText_29
L'apprentissage automatique a fait son apparition dans l'écosystème Hadoop créant, de par la puissance promise, une opportunité sans précédent pour ce domaine. Dans cet écosystème, Apache Mahout est une réponse à la question du temps de calcul et/ou de la volumétrie: il consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous portés afin de s'exécuter sur Map/Reduce. Ce rapport se concentre sur le portage et l'utilisation de l'algorithme des Random Forest dans Mahout. Il montre à travers notre retour d'expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration.
**** *annee_2015  *numText_30
Les données manquantes sont problématiques en hydrologie, car elles gênent le calcul de statistiques inter annuelles et sur de longues périodes, ainsi que l'analyse et l'interprétation de la variabilité des données. Dans cet article, nous présentons gapIT, une plate-forme d'analyse de données permettant d'inspecter visuellement les données manquantes et ensuite de choisir la méthode de correction adéquate. Nous avons utilisé l'outil pour estimer les données manquantes dans des séries temporelles correspondant aux débits mesurés par des stations hydrométriques du Luxembourg.
**** *annee_2015  *numText_32
Les correspondances sémantiques entre ontologies (mappings) jouent un rôle essentiel dans les systèmes d'information. Cependant, en vertu de l'évolution des connaissances, les éléments ontologiques sont sujets à modification invalidant potentiellement les alignements préalablement établis. Des techniques de maintenance sont donc nécessaires pour maintenir la validité des mappings. Dans cet article, nous présentons un ensemble d'heuristiques guidant leur adaptation. Notre approche s'appuie sur l'explication des mappings existants, les informations provenant de l'évolution des ontologies ainsi que les adaptations possibles applicables aux mappings. Nous proposons une validation expérimentale à partir d'ontologies du domaine médical et des mappings qui leur sont associés.
**** *annee_2015  *numText_33
Malgré des performances très satisfaisantes, l'approche sociale de la recommandation ne fournit pas de bonnes recommandations à un sous-ensemble des utilisateurs. Nous supposons ici que certains de ces utilisateurs ont des préférences différentes de celles des autres, nous les qualifions d'atypiques. Nous nous intéressons à leur identification, en amont de la tâche de recommandation, et proposons plusieurs mesures représentant l'atypicité des préférences d'un utilisateur. L'évaluation de ces mesures sur un corpus de l'état de l'art montre qu'elles permettent d'identifier de façon fiable des utilisateurs recevant de mauvaises recommandations.
**** *annee_2015  *numText_34
Etant donné un ensemble de documents rédigés par un même auteur, le problème d'authentification d'auteurs consiste à décider si un nouveau texte a été rédigé ou non par cet auteur. Pour résoudre ce problème, nous avons proposé et implémenté différentes approches : comptage de similarité, techniques de vote et apprentissage supervisé qui exploitent différents modèles de représentation des documents. Les expérimentations réalisées à partir des collections de la compétition PAN-CLEF 2013 et 2014 ont confirmé l'intérêt de nos approches et leur performance en termes de temps de traitement.
**** *annee_2015  *numText_36
Dans cet article nous présentons une approche de fusion de données fondée sur l'utilisation d'informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs.
**** *annee_2015  *numText_37
Le repérage des Entités Nommées (REN) en langue amazighe est un prétraitement éventuellement essentiel pour de nombreuses applications du traitement automatique des langues (TAL), en particulier pour la traduction automatique. Dans cet article, nous présentons une chaîne de repérage des entités nommées en amazighe fondée sur une étude synthétique des spécificités de la langue et des entités nommées en amazighe. L'article met l'accent sur les choix méthodologiques à résoudre les ambiguïtés dues à la langue, en exploitant les technologies existantes pour d'autres langues.
**** *annee_2015  *numText_40
La détection du plagiat passe le plus souvent par la phase de recherche de similitudes la plus naïve, la détection de « copier/coller ». Dans cet article, nous proposons une méthode alternative à l'approche standard de comparaison mot à mot. Le principe étant d'effectuer une intersection des deux textes à comparer, récupérant ainsi un tableau des mots qu'ils ont en commun et de ne conserver que les séquences maximales des mots se suivant dans l'un des textes et existant également dans l'autre. Nous montrons que cette méthode est plus rapide et moins coûteuse en ressources que les méthodes de parcours de textes habituellement utilisées. L'objectif étant de détecter les passages identiques entre deux textes plus rapidement que les méthodes de comparaison mot à mot, tout en étant plus efficace que les méthodes n-grammes.
**** *annee_2015  *numText_44
Pour la prédiction automatique des items préférés par des utilisateurs sur le Web, différents systèmes de filtrage collaboratif ont été proposés. La plupart d'entre eux sont basés sur la factorisation matricielle et les approches de type k plus proches voisins. Malheureusement ces deux approches requièrent un temps de calcul important. Une partie de ces problèmes a pu être surmontée par la classification croisée ou co-clustering qui s'avère pertinente du fait qu'elle permet par nature une gestion simultanée des ensembles correspondant aux utilisateurs et aux items. Cependant, des travaux doivent encore être menés pour une meilleure prise en compte des données manquantes. Dans ce travail, nous proposons donc une gestion efficace des données non observées permettant une meilleure exploitation du potentiel de la classification croisée dans le domaine des systèmes de recommandation. Nous montrons de plus qu'elle permet d'obtenir des représentations à base de graphes bipartis facilitant l'interprétation interactive des affinités entre des groupes d'utilisateurs et des groupe d'items.
**** *annee_2015  *numText_45
Cet article présente un nouvel outil visuel de clustering interactif. Il utilise une technique de réduction de dimensionnalité pour permettre une représentation 2D des données et des classes associées, initialement établies de manière non-supervisée. L'originalité de l'outil consiste à autoriser des modifications itératives à la fois du clustering et de la projection 2D. Grâce à des contrôles adaptés, l'utilisateur peut ainsi injecter ses préférences, et observer le changement induit en temps réel. La méthode de projection utilisée suit une métaphore physique, qui facilite le suivi des changements par l'utilisateur. Nous montrons un exemple illustrant l'intérêt pratique de l'outil.
**** *annee_2015  *numText_46
Remplacer des hypothèses sur le modèle de données par des informations mesurées sur les données réelles est l'une des forces de la fouille de données. Cet article étudie cet ajustement entre les données et les méthodes de découverte de motifs pour en évaluer la qualité et la complexité. Nous formalisons ce lien entre données et mesures d'intérêt en identifiant les motifs liés qui sont ceux nécessaires pour l'évaluation d'une mesure ou d'une contrainte. Nous formulons alors trois axiomes que devraient satisfaire ces motifs liés pour qu'une méthode d'extraction se comporte bien. En outre, nous définissons la complexité en évaluation qui quantifie finement l'interrelation entre les motifs au sein d'une méthode d'extraction. A la lumière de ces axiomes et de cette complexité en évaluation, nous dressons une typologie de multiples méthodes de découverte de motifs impliquant la fréquence.
**** *annee_2015  *numText_47
Trouver les liens manquants dans un grand réseau social est une tâche difficile, car ces réseaux sont peu denses, et les liens peuvent correspondre à des environnements structurels variés. Dans cet article, nous décrivons RankMerging, une méthode d'apprentissage supervisé simple pour combiner l'information obtenue par différentes méthodes de classement. Afin d'illustrer son intérêt, nous l'appliquons à un réseau d'utilisateurs de téléphones portables, pour montrer comment un opérateur peut détecter des liens entre les clients de ses concurrents. Nous montrons que RankMerging surpasse les méthodes à disposition pour prédire un nombre variable de liens dans un grand graphe épars.
**** *annee_2015  *numText_48
La prédiction de séquences de symboles est une tâche ayant de multiples applications. Plusieurs modèles de prédiction ont été proposés tels que DG, All-k-order markov et PPM. Récemment, il a été montré qu'un nouveau modèle nommé Compact Prediction Tree (CPT) utilisant une structure en arbre et un algorithme de prédiction plus complexe, offre des prédictions plus exactes que plusieurs approches de la littérature. Néanmoins, une limite importante de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ce problème en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est 4.5 fois plus rapide que CPT, tout en conservant une exactitude très élevée par rapport à All-K-order Markov, DG, Lz78, PPM et TDAG.
**** *annee_2015  *numText_49
Dans les systèmes d'apprentissage supervisé par construction de règles de classification floues, un nombre élevé d'attributs descriptifs conduit à une explosion du nombre de règles générées et peut affecter la précision des algorithmes d'apprentissage. Afin de remédier à ce problème, une solution est de traiter séparément des sous-groupes d'attributs. Cela permet de décomposer le problème d'apprentissage en des sous-problèmes de complexité inférieure, et d'obtenir des règles plus intelligibles car de taille réduite. Nous proposons une nouvelle méthode de regroupement des attributs qui se base sur le concept des règles d'association. Ces règles découvrent des relations intéressantes entre des intervalles de valeurs des attributs. Ces liaisons locales sont ensuite agrégées au niveau des attributs mêmes en fonction du nombre de liaisons trouvées et de leur importance. Notre approche, testée sur différentes bases d'apprentissage et comparée à l'approche classique, permet d'améliorer la précision tout en garantissant une réduction du nombre de règles.
**** *annee_2015  *numText_50
Dans le domaine de la fouille de séries temporelles, plusieurs travaux récents exploitent des noyaux construits à partir de distances élastiques de type Dynamic Time Warping (DTW) au sein d'approches à base de noyaux. Pourtant les matrices, apparentées aux matrices de Gram, construites à partir de ces noyaux n'ont pas toujours les propriétés requises ce qui peut les rendre in fine impropres à une telle exploitation. Des approches émergeantes de régularisation de noyaux élastiques peuvent être mises à profit pour répondre à cette insuffisance. Nous présentons l'une de ces méthodes, KDTW, pour le noyau DTW, puis, autour d'une analyse en composantes principales non-linéaire (K-PCA), nous évaluons la capacité de quelques noyaux concurrents (élastiques v.s non élastiques, définis v.s. non définis) à séparer les catégories des données analysées tout en proposant une réduction dimensionnelle importante. Cette étude montre expérimentalement l'intérêt d'une régularisation de type KDTW.
**** *annee_2015  *numText_51
Dans cet article, nous nous intéressons à la recherche des points les plus intéressants au sens de l'ordre de Pareto, dans les bases de données évidentielles. Nous présentons le modèle skyline évidentiel qui est adapté à la nature des données incertaines. Ensuite, nous présentons une évaluation expérimentale de notre approche.
**** *annee_2015  *numText_54
Nous introduisons une mesure d'ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure. Ensuite, nous étudions l'influence de l'ultramétricité sur la comportement de deux classes d'algorithmes d'exploration de données (le kNN algorithme de classification et l'algorithme de regroupement PAM) appliqués sur les espaces de dissimilarité. On montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour les clusters, une augmentation d'ultramétricité génère regroupements avec une meilleure séparation. Une diminution de la ultramétricité produit groupes plus compacts.
**** *annee_2015  *numText_55
Nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. La pénalité `1 imposée sur les composantes principales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que d'un nombre restreint de variables initiales. Un algorithme EM, simple de mise en œuvre, est proposé pour l'estimation des paramètres du modèle. La méthode de l'heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation.
**** *annee_2015  *numText_56
Dans cet article nous proposons une modification pour l'algorithme Iterated Conditional Modes (ICM) appliqué à la segmentation d'images à très haute résolution. Pour ce faire, nous introduisons un nouveau critère de convergence basé sur la compacité des clusters et qui repose sur une fonction d'énergie adaptée aux modèles de voisinages irréguliers de ce type d'images. Grâce à cette méthode, nos premières expériences ont montré que nous obtenons des résultats plus fiables en terme de convergence et de meilleure qualité qu'en utilisant l'énergie globale comme critère d'arrêt.
**** *annee_2015  *numText_57
Cet article propose un langage générique d'interrogation pour le modèle des graphes conceptuels. D'abord, nous introduisons les graphes d'interrogation. Un graphe d'interrogation est utilisé pour exprimer un « ou » entre deux sous-graphes, ainsi qu'une « option » sur un sous-graphe optionnel. Ensuite, nous proposons quatre types de requêtes (interrogation, sélection, description et construction) en utilisant les graphes d'interrogation. Enfin, les réponses à ces requêtes sont calculées à partir d'une opération basée sur l'homomorphisme de graphe.
**** *annee_2015  *numText_58
Nous nous intéressons dans ce travail au problème de détection de communautés dans les réseaux multiplexes. Le modèle de réseau multiplexe a été récemment introduit afin de faciliter la modélisation des réseaux multi-relationnels, des réseaux dynamiques et/ou des réseaux attribués. Les approches existantes pour la détection de communautés dans ce genre de graphes sont, pour la plupart, basées sur des schémas d'agrégation de couches ou d'agrégation de partitions. Nous proposons ici une nouvelle approche centrée graine qui permet de prendre en compte directement la nature multi-couche d'un réseau multiplexe. Des expérimentations effectuées sur différents réseaux multiplexes montrent que notre approche surpasse les approches de l'état de l'art en termes de qualité des communautés identifiées.
**** *annee_2015  *numText_59
Les modèles de propagation d'informations, d'influence et d'actions dans les réseaux sociaux sont nombreux et diversifiés rendant le choix de celui approprié à une situation donnée potentiellement difficile. La sélection d'un modèle pertinent pour une situation exige de pouvoir les comparer. Cette comparaison n'est possible qu'au prix d'une traduction des modèles dans un formalisme commun et indépendant de ceux-ci. Nous proposons l'utilisation de la réécriture de graphes afin d'exprimer les mécanismes de propagation sous la forme d'un ensemble de règles de transformation locales appliquées selon une stratégie donnée. Cette démarche prend tout son sens lorsque les modèles ainsi traduits sont étudiés et simulés à partir d'une plate-forme de visualisation analytique dédiée à la réécriture de graphe. Après avoir décrit les modèles et effectué différentes simulations, nous exhibons comment la plate-forme permet d'interagir avec ces formalismes, et comparer interactivement les traces d'exécution de chaque modèle grâce à diverses mesures soulignant leurs différences.
**** *annee_2015  *numText_60
L'évolution d'une ontologie est un processus indispensable dans son cycle de vie. Elle est exprimée et définie par des changements ontologiques de différents types : élémentaires, composés et complexes. Les changements complexes et composés sont très utiles dans le sens où ils aident l'utilisateur à adapter son ontologie sans se perdre dans les détails des changements élémentaires. Cependant, ils cachent derrière une formalisation sophistiquée puisqu'ils affectent, à la fois, plusieurs entités ontologiques et peuvent causer des inconsistances à l'ontologie évoluée. Pour adresser cette problématique, cet article présente une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés. Cette formalisation s'appuie sur l'approche algébrique Simple Pushout (SPO) de transformation de graphes et possède deux principaux avantages : (1) fournir une nouvelle formalisation permettant de contrôler les transformations de graphes et éviter les incohérences d'une manière a priori, (2) simplifier la définition des changements composés et complexes en réduisant le nombre de changements élémentaires nécessaires à leur application.
**** *annee_2015  *numText_61
L'objectif de nos travaux est de proposer une méthode d'analyse automatique du comportement des utilisateurs à des fins de prédiction de leur propension à réaliser une action suggérée. Nous proposons dans cet article une nouvelle méthode de Web Usage Mining basée sur une étude sémiotique des styles perceptifs, considérant l'expérience de l'utilisateur comme élément déterminant de sa réaction à une sollicitation. L'étude de ces styles nous a amené à définir de nouveaux indicateurs (des descripteurs sémiotiques) introduisant un niveau supplémentaire à l'approche sémantique d'annotation des sites. Nous proposons ensuite un modèle neuronal adapté au traitement de ces nouveaux indicateurs. Nous expliquerons en quoi le modèle proposé est le plus pertinent pour traiter ces informations.
**** *annee_2015  *numText_62
Nous nous intéressons, dans ce papier, à l'impact des données massives dans un environnement décisionnel et plus particulièrement sur la phase d'intégration des données. Dans ce contexte, nous avons développé une plateforme, baptisée P-ETL (Parallel-ETL), destinée à l'entreposage de données massives selon le paradigme MapReduce. P-ETL permet le paramétrage de processus ETL (workflow) et un paramétrage avancé relatif à l'environnement parallèle et distribué. Ce papier décrit la plateforme P-ETL en vue d'une démonstration. Face à des jeux de données allant de 244 * 106 à 7, 317 * 109 tuples, les expérimentations menées ont montré l'amélioration significative des performances de P-ETL lorsque la taille du cluster et le nombre des tâches parallèles augmentent.
**** *annee_2015  *numText_63
Dans ce travail, nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux. Cette méthode est basée sur l'analyse et l'enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs. Nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques, et nous avons obtenu des bons résultats.
**** *annee_2015  *numText_65
Issue d'un phénomène complexe partant d'une molécule odorante jusqu'à la perception dans le cerveau, l'olfaction reste le sens le plus difficile à appréhender par les neuroscientifiques. L'enjeu principal est d'établir des règles sur les propriétés physicochimiques des molécules (poids, nombre d'atomes, etc.) afin de caractériser spécifiquement un sous-ensemble de qualités olfactives (fruité, boisé, etc.). On peut trouver de telles règles descriptives grâce à la découverte de sous-groupes (subgroup discovery). Cependant les méthodes existantes permettent de caractériser soit une seule qualité olfactive ; soit toutes les qualités olfactives à la fois (exceptional model mining) mais pas un sousensemble. Nous proposons alors une approche de découverte de sous-groupes caractéristiques de seulement certains labels, par une nouvelle technique d'énumération, issue de la fouille de redescriptions. Nous avons expérimenté notre méthode sur une base de données d'olfaction fournie par des neuroscientifiques et pu exhiber des premiers sous-groupes intelligibles et réalistes.
**** *annee_2015  *numText_66
Hotspots, à laquelle de nombreuses photographies ont été prises, pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme. Visualisation des hotspots révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et du marketing touristiques. Bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés, un hotspot a une relation à d'autres hotspots dans certains cas. Pour organiser ces hotspots, nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots. Notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques. Notre approche extrait les relations à l'aide de sous-hotspots, qui sont fendus d'un hotspot qui comprend des photographies de différents types.
**** *annee_2014  *numText_68
SAX (Symbolic Aggregate approXimation) est une des techniques majeures de symbolisation des séries temporelles. La non prise en compte des tendances dans la symbolisation est une limitation bien connue de SAX. Cet article présente 1d-SAX, une méthode pour représenter une série temporelle par une séquence de symboles contenant des informations sur la moyenne et la tendance des fenêtres successives de la série segmentée. Nous comparons l'efficacité de 1d-SAX vs SAX dans une tâche de classification de séries temporelles d'images satellites. Les résultats montrent que 1d-SAX améliore les taux de classification pour une quantité d'information identique utilisée.
**** *annee_2014  *numText_69
Cet article étudie l'intérêt de représenter les documents textuels non plus comme des sacs-de-mots, mais comme des sacs-de-sacs-de-mots. Au cœur de l'utilisation de cette représentation, le calcul de similarité entre deux objets nécessite alors d'agréger toutes les similarités entre sacs de chacun des objets.Nous évaluons cette représentation dans un cadre de recherche d'information,et étudions les propriétés attendues de ces fonctions d'agrégation. Les expériences rapportées montrent l'intérêt de cette représentation lorsque les opérateurs d’agrégation respectent certaines propriétés, avec des gains très importants par rapport aux représentations standard.
**** *annee_2014  *numText_70
Nous proposons dans cet article une méthode d'alignement d'une ontologie source avec des ontologies cibles déjà publiées et liées sur le web de données. Nous présentons ensuite un retour d'expérience sur l'alignement d'une ontologie dans le domaine des sciences du vivant et de l'environnement avec AGROVOC et NALT.
**** *annee_2014  *numText_71
La numérisation de documents administratifs est un enjeu économique et écologique prioritaire dans le contexte sociétal actuel. La dématérialisation massive de document n'est pas sans conséquence et soulève les problèmes d'organisation,de stockage et d'accès à l'information. Le défi n'est donc plus la numérisation du document, mais l'extraction des informations qu'ils contiennent.Les documents sont produits par l'Homme et pour l'Homme. Cette propriété permet de localiser des informations dans les zones saillantes du document (logos).La saillance et la reconnaissance sont deux éléments essentiels pour la classification rapide de documents. A l'opposé, la recherche d'un document ou d'un ensemble de documents repose presque toujours sur le texte brut, il est donc nécessaire de faire une correspondance entre une requête textuelle et le document. Cet article présente une nouvelle approche d'annotation automatique de documents administratifs qui utilise une approche visuel et une approche de fouille de texte.
**** *annee_2014  *numText_72
Le modèle MapReduce est aujourd'hui l'un des modèles de programmation parallèle les plus utilisés. Définissant une architecture Maître-Esclave,il permet le traitement parallèle de grandes masses de données. Dans ce papier,nous proposons un algorithme basé sur MapReduce qui permet, à partir des données publiques du Ministère Français de la Communication et de la Culture, de définir un classement des galeries et musées nationaux selon leurs degré d’accessibilité aux personnes handicapées. Tout en profitant de la puissance et de la flexibilité du paradigme MapReduce, les décideurs pourront mettre en place des stratégies efficaces à moindre coût et avoir ainsi une vision plus précise sur les établissements culturels et leurs limites relatives à cette catégorie de personnes.L'algorithme que nous proposons peut être exploité et appliqué à d'autres casd'études avec des jeux de données plus volumineux.
**** *annee_2014  *numText_73
Les fonctions biologiques dans la cellule mettent en jeu des interactions3D entre protéines et ARN. Les avancées des techniques expérimentales restent insuffisantes pour de nombreuse applications. Il faut alors pouvoir prédire in silico les interactions protéine-ARN. Dans ce contexte, nos travaux sont focalisés sur la construction de fonctions de score permettant d'ordonner les solutions générées par le programme d'amarrage protéine-ARN RosettaDock. La méthodologie d'évaluation utilisée par RosettaDock impose de trouver une fonction de score s'exprimant comme une combinaison linéaire de mesures physicochimiques. Avec une approche d'apprentissage supervisé par algorithme génétique,nous avons appris différentes fonctions de score en imposant des contraintes sur la nature des poids recherchés. Les résultats obtenus montrent l’importance de la signification des poids à apprendre et de l'espace de recherche associé.
**** *annee_2014  *numText_74
Nous considérons le problème de classification supervisée pour des flux de données présentant éventuellement un très grand nombre de variables explicatives. Le classifieur Bayésien naïf se révèle alors simple à calculer et relativement performant tant que l'hypothèse restrictive d'indépendance des variables conditionnellement à la classe est respectée. La sélection de variables et le moyennage de modèles sont deux voies connues d'amélioration qui reviennent à déployer un prédicteur Bayésien naïf intégrant une pondération des variables explicatives. Dans cet article, nous nous intéressons à l'estimation directe d'un tel modèle Bayésien naïf pondéré. Nous proposons une régularisation parcimonieuse de la log-vraisemblance du modèle prenant en compte l'informativité de chaque variable. La log-vraisemblance régularisée obtenue étant non convexe,nous proposons un algorithme de gradient en ligne qui post-optimise la solution obtenue afin de déjouer les minima locaux. Les expérimentations menées s'intéressent d'une part à la qualité de l'optimisation obtenue et d'autre part aux performances du classifieur en fonction du paramétrage de la régularisation.
**** *annee_2014  *numText_75
L'apprentissage de dépendances est une tâche consistant à établir, à partir des phrases d'un texte, un modèle de construction d'arbres traduisant une hiérarchie syntaxique entre les mots. Nous proposons un modèle intermédiaire entre l'analyse syntaxique complète de la phrase et les sacs de mots. Il est basé sur une grammaire stochastique hors-contexte se traduisant par des relations de dépendance entre les catégories grammaticales d'une phrase. Les résultats expérimentaux obtenus sur des benchmarks attestés dépassent pour cinq langues sur dix les scores de l'algorithme de référence DMV, et pour la première fois des scores sont obtenus pour le français. La très grande simplicité de la grammaire permet un apprentissage très rapide, et une analyse presque instantanée.
**** *annee_2014  *numText_76
L'article propose une approche formelle de fusion d'ontologies se reposant sur les grammaires de graphes typés. Elle se décompose en trois étapes :1) la recherche de similarités entre concepts ; 2) la fusion des ontologies par l’approche algébrique SPO (Simple Push Out) ; 3) l'adaptation d'une ontologie globale par le biais de règles de réécriture de graphes. Contrairement aux solutions existantes, cette méthode offre une représentation formelle de la fusion d’ontologies ainsi qu'une implémentation fonctionnelle basée sur l'outil AGG.
**** *annee_2014  *numText_77
Dans cet article nous nous intéressons aux approches pour l’analyse de graphes pouvant évoluer dans le temps et tel qu'un sommet à un temps donné peut correspondre à plusieurs sommets au temps suivant et où les sommets sont associés à un ensemble d'attributs catégoriels. Dans ce type de données, nous proposons une nouvelle classe de motifs basée sur des contraintes permettant de décrire l'évolution de structures homogènes. Ce type d'approche est particulièrement adaptée pour l'analyse d'images multi-résolution sans perte d'information.Nous présentons un résultat qualitatif dans ce domaine.
**** *annee_2014  *numText_80
Dans cet article, nous proposons un nouveau descripteur spatio-temporel appelé ST-SURF pour l'analyse et la reconnaissance d’actions dans des flux vidéo. L'idée principale est d'enrichir le descripteur Speed UpRobust Feature (SURF) en intégrant l'information de mouvement issue du flot optique. Seuls les points d'intérêts qui ont subi un déplacement sont pris en compte pour générer un dictionnaire de mots visuels (DMV) robuste basé sur l’algorithme des k-moyennes (K-means). Le dictionnaire est utilisé lors du processus d’apprentissage et de reconnaissance d'actions basé sur la méthode des machines à vecteurs supports (SVM). Les résultats obtenus confirment l’intérêt du descripteur proposé ST-SURF pour l'analyse de scènes et en particulierpour la reconnaissance d'actions. La méthode atteind une précision de reconnaissance de l'ordre de 80.7%, équivalente aux performances des descripteurs spatio-temporels de l'état de l'art.
**** *annee_2014  *numText_81
La prédiction du rayonnement solaire horaire dans une journée est un enjeu primordial pour la production d'énergie de type photovoltaïque. Nous présentons deux stratégies de classification des jours selon leurs rayonnements solaires puis une méthode de prédiction du flux solaire cohérente avec la classification.
**** *annee_2014  *numText_82
Un classifieur naïf de Bayes est un classifieur probabiliste basé sur l'application du théorème de Bayes avec l'hypothèse naïve, c'est-à-dire que les variables explicatives (Xi) sont supposées indépendantes conditionnellement à la variable cible (C). Malgré cette hypothèse forte, ce classifieur s'est avéré très efficace sur de nombreuses applications réelles et est souvent utilisé sur les flux de données pour la classification supervisée. Le classifieur naïf de Bayes nécessite simplement en entrée l'estimation des probabilités conditionnelles par variable P(Xi|C) et les probabilités a priori P(C). Pour une utilisation sur les flux de données, cette estimation peut être fournie à l'aide d'un « résumé supervisé en-ligne de quantiles ». L'état de l'art montre que le classifieur naïf de Bayes peut être amélioré en utilisant une méthode de sélection ou de pondération des variables explicatives. La plupart de ces méthodes ne peuvent fonctionner que hors-ligne car elles nécessitent de stocker toutes les données en mémoire et/ou de lire plus d'une fois chaque exemple. Par conséquent, elles ne peuvent être utilisées sur les flux de données. Cet article présente une nouvelle méthode basée sur un modèle graphique qui calcule les poids des variables d'entrée en utilisant une estimation stochastique. La méthode est incrémentale et produit un classifieur Naïf de Bayes Pondéré pour flux de données. Cette méthode est comparée au classique classifieur naïf de Bayes sur les données utilisées lors du challenge « Large Scale Learning ».
**** *annee_2014  *numText_83
Les approches existantes pour structurer automatiquement un flux de télévision (i.e. reconstituer un guide de programme exact et complet), sont supervisées.Elles requièrent de grandes quantités de données annotées manuellement,et aussi de définir a priori les types d'émissions (publicités, bandes annonces,programmes, sponsors...). Pour éviter ces deux contraintes, nous proposons une classification non supervisée. La nature multi-relationnelle de nos données proscrit l'utilisation des techniques de clustering habituelles reposant sur des représentations sous forme attributs-valeurs. Nous proposons et validons expérimentalement une technique de clustering capable de manipuler ces données en détournant la programmation logique inductive (PLI) pour fonctionner dans ce cadre non supervisé.
**** *annee_2014  *numText_84
Nous proposons une nouvelle méthode de clustering et d'analyse de séquences temporelles basée sur les modèles en grille à trois dimensions. Les séquences sont partitionnées en clusters, la dimension temporelle est discrétisée en intervalles et la dimension événement est partitionnée en groupes. La grille de cellules 3D forme ainsi un estimateur non-paramétrique constant par morceaux de densité jointe des séquences et des dimensions des événements temporels.Les séquences d'un cluster sont ainsi groupés car elles suivent une distribution similaire d’événements au cours du temps. Nous proposons aussi une méthode d’exploitation du clustering par simplification de la grille ainsi que des indicateurs permettant d'interpréter les clusters et de caractériser les séquences qui les composent. Les expériences sur des données artificielles ainsi que sur des données réelles issues de DBLP démontrent le bien-fondé de notre approche.
**** *annee_2014  *numText_85
La recherche de liens conceptuels fréquents (FCL) est une nouvelle approche de clustering de réseaux, qui exploite à la fois la structure et les attributs des nœuds. Bien que les travaux récents se soient déjà intéressés à l'optimisation des algorithmes de recherche des FCL, peu de travaux sont aujourd'hui menés sur la complémentarité qui existe entre les liens conceptuels et l'approche classique de clustering qui consiste en l'extraction de communautés. Ainsi dans ce papier, nous nous intéressons à ces deux approches. Notre objectif est d'évaluer les relations potentiellement existantes entre les communautés et les FCL pour comprendre la façon dont les motifs obtenus par chacune des méthodes peuvent correspondre ou s'intersecter ainsi que la connaissance utile résultant de la prise en compte de ces deux types de connaissance. Nous proposons pour cela un ensemble de mesures originales, basées sur la notion d'homogénéité, visant à évaluer le niveau d'intersection des FCL et des communautés lorsqu'ils sont extraits d'un même jeu de données. Notre approche est appliquée à deux réseaux et démontre l'importance de considérer simultanément plusieurs types de connaissance et leur intersection.
**** *annee_2014  *numText_86
Nous avons tous déjà eu l'occasion d'effectuer des recherches d’ordre médical sur Internet. Si certains sites spécialisés se refusent à tout diagnostic en ligne, préférant le renvoi vers des professionnels de santé, d'autres en revanche conduisent souvent à des déclarations alarmistes faisant état de situations humaines difficiles. Dans ce travail, nous étudions l'ampleur de ce phénomène et montrons que quel que soit le syndrome recherché, les résultats obtenus conduisent toujours à l'énoncé des mots cancer ou tumeur.
**** *annee_2014  *numText_87
Le clustering incrémental en une passe repose sur l'affectation efficace de chaque nouveau point aux clusters existants. Dans le cas général, où les clusters ne peuvent être représentés par une moyenne, la détermination exhaustive du cluster le plus proche possède une complexité quadratique avec le nombre de données. Nous proposons dans ce papier une nouvelle méthode d’affectation stochastique à chaque cluster qui minimise le nombre de comparaisons à effectue rentre la donnée et chaque cluster pour garantir, étant donné un taux d’erreur acceptable, l'affectation au cluster le plus proche. Plusieurs bornes théoriques(Bernstein, Hoeffding et Student) sont comparées dans ce papier. Les résultats sur des données artificielles et réelles montrent que la borne de Bernstein donne globalement les meilleurs résultats (notamment lorsqu'elle est réduite) car elle permet une accélération forte du processus de clustering, tout en conservant un nombre très faible d'erreurs.
**** *annee_2014  *numText_88
Cet article compare deux représentations de données spatiales, les graphes de voisinages et les chemins de Hilbert-Peano, utilisées par des algorithmes de fouille. Cette comparaison s'appuie sur la mise en oeuvre d'une méthode d'énumération de « sacs de nœuds », qui permet d'obtenir des caractérisations homogènes à partir des deux représentations. La méthode est appliquée à la caractérisation de parcellaires agricoles et les résultats tendent à montrer que la linéarisation de l'espace capte la majorité de l'information, à l'exception des éléments rares, sur cet exemple particulier.
**** *annee_2014  *numText_89
Sur les sites Web communautaires, les utilisateurs échangent des connaissances,en étant à la fois auteurs et lecteurs. Nous présentons une méthode pour construire notre propre compréhension de la sémantique de la communauté,sans recours à une base de connaissances externe. Nous effectuons une extraction de la connaissance présente dans les contributions analysées. Nous proposons une évaluation de la confiance imputable à cette compréhension déduite,afin d'évaluer la qualité du contenu, avec application à un site Web de partage de recettes de cuisine.
**** *annee_2014  *numText_90
L'optimisation de la construction de cubes OLAP 1 a été jusqu'à présent axée sur le développement d'algorithmes de calcul performants. Ces derniers opèrent sur des données extraites de l'entrepôt de données qui est généralement implémenté selon le modèle relationnel qui adopte l'architecture orientée lignes. Or, pour les requêtes décisionnelles, l'architecture orientée colonnes offre de meilleures performances. Cependant, les SGBDR 2 selon cette architecture ne disposent pas d'opérateurs appropriés pour le calcul de cube OLAP. Nous proposons dans cet article une nouvelle méthode de calcul de cube OLAP. Les résultats obtenus à partir des expérimentations que nous avons menées démontrent que notre approche optimise considérablement le temps de construction de cube OLAP et réduit le temps de réponse relatif à l'exploitation du cube comparé à l'approche orientée lignes.
**** *annee_2014  *numText_91
L'utilisation de préférences suscite un intérêt croissant pour personnaliser des réponses et effectuer des recommandations. En amont, l'étape essentielle est l'élicitation des préférences qui consiste à construire un profil de préférences en sollicitant le moins possible l'utilisateur. Dans cet article, nous présentons une méthode basée sur l'extraction de motifs séquentiels afin de générer des règles de préférences contextuelles à partir d'une base de paires de transactions. À partir de ces règles générées, qui ont une expressivité plus riche que celle des approches existantes, nous montrons comment construire et utiliser un profil modélisant les préférences de l'utilisateur. De plus, notre approche a l'avantage de bénéficier des nombreux algorithmes efficaces d'extraction de séquences fréquentes. L'évaluation de notre méthode sur des données réelles montre que les modèles de préférences construits permettent d'effectuer des recommandations justes à un utilisateur.
**** *annee_2014  *numText_92
L'extraction de connaissances à partir de données issues du génie logiciel est un domaine qui s'est beaucoup développé ces dix dernières années, avec notamment la fouille de référentiels logiciels (Mining Software Repositories) et l'application de méthodes statistiques (partitionnement, détection d'outliers) à des thématiques du processus de développement logiciel. Cet article présente la démarche de fouille de données mise en œuvre dans le cadre de Polarsys, un groupe de travail de la fondation Eclipse, de la définition des exigences à la proposition d'un modèle de qualité dédié et à son implémentation sur un prototype.Les principaux concepts adoptés et les leçons tirées sont également passés en revue.
**** *annee_2014  *numText_93
Un des défis actuels dans le domaine de la classification supervisée de documents est de pouvoir produire un modèle fiable à partir d'un faible volume de données. Avec un volume conséquent de données, les classifieurs fournissent des résultats satisfaisants mais les performances sont dégradées lorsque celui-ci diminue. Nous proposons, dans cet article, de nouvelles méthodes de pondérations résistant à une diminution du volume de données. Leur efficacité, évaluée en utilisant des algorithmes de classification supervisés existants (Naive Bayes et Class-Feature-Centroid) sur deux corpus différents, est supérieure à celle des?autres algorithmes lorsque le nombre de descripteurs diminue. Nous avons étudié en parallèle les paramètres influençant les différentes approches telles que le nombre de classes, de documents ou de descripteurs.
**** *annee_2014  *numText_94
Les Humanités Numériques, aussi contestable et critiquable que soit le terme, font maintenant partie du paysage de la recherche en sciences humaines, institutionnalisées par la Très Grande Infrastructure de Recherche Huma-Num du CNRS. Elles sont généralement définies comme la convergence de disciplines autour d'un matériau numérique, matériau inévitablement accompagné d'un outillage tout aussi numérique. Ce matériau, suivant la discipline qui l’observe pourra être considéré comme un objet éditorial, un objet analysable ou un objet calculable. Nous tenterons de montrer que ce matériau peut aussi être perçu, voire construit,comme un dépôt voire un entrepôt de connaissances. Notre présentation s'appuiera sur divers projets de recherche en humanités numériques auxquels nous contribuons afin de mettre en exergue le lien qui peut être fait entre extraction et gestion de connaissances d'une part et humanités numériques d'autre part : le premier peut trouver un terrain expérimental dans le second tandis que le second peut tirer profit des méthodes et outils développés par le premier.Nous égrènerons par ailleurs d'autres problématiques inhérentes aux Humanités numériques :de la constitution à l'analyse du corpus en passant par la formalisation et la normalisation des données. Enfin, nous tenterons de montrer par l'exemple que les questions posées par les humanités numériques ne sont pas sans rappeler celles des industries de la connaissance.
**** *annee_2014  *numText_95
Pour mieux analyser et extraire de la connaissance de flots de données,des approches spécifiques ont été proposées ces dernières années. L'un des challenges auquel elles doivent faire face est la détection de changement dansles données. Alors que de plus en plus de données qualitatives sont générées,peu de travaux de recherche se sont intéressés à la détection de changement dans ce contexte et les travaux existants se sont principalement focalisés sur la qualité d’un modèle appris plutôt qu'au réel changement dans les données. Dans cet article nous proposons une nouvelle méthode de détection de changement non supervisée, appelée CDCStream (Change Detection in Categorical DataStreams), adaptée aux flux de données qualitatives.
**** *annee_2014  *numText_96
Avec le vieillissement de la population dans les décennies à venir, la prise en charge de la dépendance est devenu un enjeu majeur. Les nouvelles technologies permettent d'améliorer le confort et la sécurité des personnes dépendantes à domicile. Dans cet article nous proposons une méthode de détection de situations à risques basée sur le seuillage automatique des intervalles d’inactivité des capteurs de mouvement de type infrarouge passif. Notre contribution consiste à apprendre de façon automatique la durée maximale d'inactivité, par pièce et par plage horaire. La méthode est évaluée sur des données réelles provenant de l'activité d'une personne réelle dans un appartement équipé de capteurs domotiques. Notre approche permet de réduire le temps d'appel des secours.
**** *annee_2014  *numText_97
Twitter est à l'heure actuelle un des réseau sociaux les plus utilisé au monde et analyser les opinions qui y sont contenues permet de fournir de précieuses informations notamment aux entreprises commerciales. Dans cet article,nous décrivons une méthode permettant de déterminer l'opinion d'un tweet en détectant dans un premier temps sa subjectivité, puis sa polarité.
**** *annee_2014  *numText_98
Avec la prolifération des données géographiques, il y a un fort besoin de concevoir des outils automatiques pour l'exploitation des connaissances géographiques incarnées dans les documents textuels. C'est dans ce contexte, que nous proposons une approche permettant de générer une base de données géographiques(BDG) à partir de textes. Notre approche s'articule autour de deux grandes phases : la génération du schéma de la BDG et la détermination des données qui serviront au remplissage de cette base. L'implémentation de notre approche a donné naissance à un outil que nous avons baptisé GDB Generatoret que nous avons intégré dans le SIG : OpenJUMP.
**** *annee_2014  *numText_99
Dans cet article, nous proposons une approche générale de prédiction des communautés basée sur un modèle d'apprentissage automatique pour la prédiction des interactions. En effet, nous pensons que, si on peut prédire avec précision la structure du réseau, alors on a juste à rechercher les communautés surle réseau prédit. Des expérimentations sur des jeux de données réels montrent lafaisabilité de cette approche.
**** *annee_2014  *numText_100
De nos jours dans les secteurs commerciaux et financiers la veille est cruciale et complexe, car la charge d'informations est importante. Pour répondre à cette problématique, nous proposons un système novateur de recommandation d'articles basé sur une modélisation ontologique des connaissances. Nous présentons également une nouvelle méthode d'évaluation de la pertinence utilisant le modèle vectoriel intrinsèquement efficace et adapté afin de pallier la confusion native de ces modèles entre les notions de similarité et de pertinence.
**** *annee_2014  *numText_101
Dans cet article, nous présentons une approche de fouille de textes ainsi qu'une interface de visualisation afin d'explorer une large collection de chansons françaises à partir des paroles. Dans un premier temps, nous collectons paroles et métadonnées de différentes sources sur le Web. Nous utilisons une approche combinant clustering et analyse sémantique latente afin d'identifier différentes thématiques et de déterminer différents descripteurs significatifs. Nous transformons par la suite le modèle afin d'obtenir une visualisation interactive permettant d'explorer la collection de chansons.
**** *annee_2014  *numText_102
Les techniques de classification modernes permettent d'étiqueter les zones non couvertes des bases de données cartographiques, mais souffrent d'un manque de robustesse important. Dans cet article, nous proposons une méthode robuste d'extension d'étiquetage sur l'emprise d'une image satellite, par analyse hiérarchique des données existantes. Notre approche est fondée sur une sélection d'attributs par thème de la base de données, une sélection des pixels d'apprentissage et des classifications par objet de chaque thème. La décision finale d'étiquetage est prise après fusion des classifications par thème. Notre méthode est appliquée avec succès et comparée à plusieurs méthodes de classification,couplant données d'occupation du sol et imagerie spatiale très haute résolution.
**** *annee_2014  *numText_103
Les graphes orientés attribués sont des graphes orientés dans lesquels les nœuds sont associés à un ensemble d'attributs. De nombreuses données, issues du monde réel, peuvent être représentées par ce type de structure, mais encore peu d'algorithmes sont capables de les traiter directement. La fouille des graphes attribués est difficile, car elle nécessite de combiner l'exploration de la structure du graphe avec l'identification d'itemsets fréquents. De plus, du fait de l’explosion combinatoire des itemsets, les isomorphismes de sous-graphes, dont la présence impacte énormément les performances des algorithmes de fouille,sont beaucoup plus nombreux que dans les graphes étiquetés.Dans cet article, nous présentons une nouvelle méthode de fouille de données qui permet d'extraire des motifs fréquents à partir d'un ou de plusieurs graphes orientés attribués. Nous montrons comment réduire l'explosion combinatoire provoquée par les isomorphismes de sous-graphes en traitant de manière particulière les motifs automorphes.
**** *annee_2014  *numText_104
Les messages déposés quotidiennement sur les réseaux sociaux et les-blogs sont très nombreux et constituent une source d'informations précieuse.Leur fouille peut être utilisée dans un but de prédiction d'informations. Notre objectif dans cet article est de proposer un algorithme permettant la prédiction d’informations au plus tôt et de façon fiable, par le biais de l'identification de règles d'épisodes.
**** *annee_2014  *numText_105
Les représentations condensées ont fait l'objet de nombreux travaux depuis 15 ans. Tandis que les motifs maximaux des classes d'équivalence ont reçu beaucoup d'attention, les motifs minimaux sont restés dans l'ombre notamment à cause de la difficulté de leur extraction. Dans ce papier, nous présentons un cadre générique concernant l'extraction de motifs minimaux en introduisant la notion de système minimisable d'ensembles. Il permet de considérer des langages variés comme les motifs ensemblistes ou les chaînes de caractères, mais aussi différentes métriques dont la fréquence. Ensuite, pour n'importe quel système minimisable d'ensembles, nous introduisons un test de minimalité rapide permettant d'extraire en profondeur les motifs minimaux. Nous démontrons que l'algorithme proposé est polynomial-delay et polynomial-space. Des expérimentations sur les benchmarks traditionnels complètent notre étude.
**** *annee_2014  *numText_106
Nous présentons ici la plate-forme KD-Ariane, un déploiement d'outils pour la fouille de données dans l'environnement de programmation visuelle Ariane. Ce déploiement facilite la conception de chaînes structurées de traitements pour l'extraction de connaissance dans les données.
**** *annee_2014  *numText_107
Pour atteindre un but, tout agent en compétition élabore inévitablement des stratégies. Lorsque l'on dispose d'une certaine quantité de traces d’interactions entre agents, il est naturel d'utiliser la fouille de motifs séquentiels pour découvrir de manière automatique ces stratégies. Dans cet article, nous proposons une méthodologie qui permet l'élicitation de stratégies et leur capacité à discriminer une réussite ou un échec. La méthodologie s'articule en trois étapes :(i) les traces brutes sont transformées en une base de séquences selon des choix qui permettent, (ii) l'extraction de stratégies fréquentes, (iii) lesquelles sont munies d’une mesure originale d'émergence. C'est donc une méthodologie de découverte de connaissances que nous proposons. Nous montrons l'intérêt des motifs extraits et la faisabilité de l'approche à travers des expérimentations quantitatives et qualitatives sur des données réelles issues du domaine émergent du sport électronique.
**** *annee_2014  *numText_108
La recherche de groupes non-disjoints à partir de données non-étiquetées est une problématique importante en classification non-supervisée. La classification recouvrante (Overlapping clustering) contribue à la résolution de plusieurs problèmes réels qui nécessitent la détermination de groupes qui se chevauchent.Cependant, bien que les recouvrements entre groupes soient tolérés voire encouragés dans ces applications, il convient de contrôler leur importance.Nous proposons dans ce papier des généralisations de k-moyennes offrant le contrôle et le paramétrage des recouvrements. Deux principes de régulation sont mis en place, ils visent à contrôler les recouvrements relativement à leur taille et à la dispersion des classes. Les expérimentations réalisées sur des jeux de données réelles, montrent l'intérêt des principes proposés.
**** *annee_2014  *numText_109
Dans ce papier, nous présentons une approche dédiée à la transformation d'une base de données en un extrait textuel. L'idée sous-jacente à notre proposition est d'apporter plus de sémantique aux données de la base. Cet objectif est atteint moyennant l'utilisation des ontologies comme ressources sémantiques.Notre approche prend comme input un ensemble de bases de données et associe à chacune une ontologie. Une ontologie globale est générée, à partir de laquelle des règles d'association sont proposées pour mieux expliciter sa sémantique. Enfin, la génération d'un extrait textuel prend lieu.
**** *annee_2014  *numText_110
Découvrir des connaissances dans des graphes qui sont dynamiques et dont les sommets sont attribués est de plus en plus étudié, par exemple dans le contexte de l'analyse d'interactions sociales. Il est souvent possible d'expliciter des hiérarchies sur les attributs permettant de formaliser des connaissances a priori sur les descriptions des sommets. Nous proposons d'étendre des techniques de fouille sous contraintes récemment proposées pour l'analyse de graphes attribués dynamiques lorsque l'on exploite de telles hiérarchies et donc le potentiel de généralisation/spécialisation qu'elles permettent. Nous décrivons un algorithme qui calcule des motifs de co-évolution multi-niveaux, c'est-à-dire des ensembles de sommets qui satisfont une contrainte topologique et qui évoluent de la même façon selon un ensemble de tendances et de pas de temps. Nos expérimentations montrent que l'utilisation d'une hiérarchie permet d'extraire des collections de motifs plus concises sans perdre d'information.
**** *annee_2014  *numText_111
La classification recouvrante correspond à un enjeu important en classification non-supervisée en permettant à une observation d'appartenir à plusieurs clusters. Plusieurs méthodes ont été proposées pour faire face à cette problématique en utilisant plusieurs approches usuelles de classification. Cependant,malgré l'efficacité de ces méthodes à déterminer des groupes non-disjoints,elles échouent lorsque les données comportent des groupes de densités différentes car elles ignorent la densité locale de chaque groupe et ne considèrent que la distance Euclidienne entres les observations. Afin de détecter des groupes non-disjoints de densités différentes, nous proposons deux méthodes de classification intégrant la variation de densité des différentes classes dans le processus de classification. Des expériences réalisées sur des ensembles de données artificielles montrent que les méthodes proposées permettent d'obtenir de meilleures performances lorsque les données contiennent des groupes de densités différentes.
**** *annee_2014  *numText_112
La notion de structure de communautés est particulièrement utile pour étudier les réseaux complexes, car elle amène un niveau d'analyse intermédiaire,par opposition aux plus classiques niveaux local (voisinage des nœuds) et global(réseau entier). Le concept de rôle communautaire permet de décrire le positionnement d'un nœud en fonction de sa connectivité communautaire. Cependant,les approches existantes sont restreintes aux réseaux non-orientés, utilisent des mesures topologiques ne considérant pas tous les aspects de la connectivité communautaire, et des méthodes d'identification des rôles non-généralisables à tous les réseaux. Nous proposons de résoudre ces problèmes en généralisant les mesures existantes, et en utilisant une méthode non-supervisée pour déterminer les rôles. Nous illustrons l'intérêt de notre méthode en l'appliquant au réseau de Twitter. Nous montrons que nos modifications mettent en évidence les rôles spécifiques d'utilisateurs particuliers du réseau, nommés capitalistes sociaux.
**** *annee_2014  *numText_113
Dans ce travail, nous nous intéressons au problème de la prédiction d'attributs sur les nœuds dans un réseau social. La plupart des techniques sont hors ligne et ne sont pas adaptées à des situations où les données arrivent massivement en flux comme dans le cas des médias sociaux. Dans ce travail, nous utilisons les modèles de variables latentes pour prédire les attributs inconnus des nœuds dans un réseau social et proposer une méthode pour mettre à jour incrémentalement le modèle avec des nouvelles données. Des expérimentations sur un jeu de données issues des médias sociaux montrent que notre méthode est moins coûteuse en temps de calcul et peut garantir des performances acceptables en comparaison avec les techniques non-incrémentales de l'état de l'art.
**** *annee_2014  *numText_115
De nombreuses ressources publiées sur le Web des données sont décrites par une composante qui désigne d'une manière directe ou indirecte une localisation géographique. Comme toute autre propriété, cette information de localisation peut être mise à profit pour permettre l'interconnexion des données avec d'autres sources. Elle permet en outre leur représentation cartographique.Cependant, les informations de localisation utilisées dans les sources de données linked data peuvent parfois s'avérer imprécises ou hétérogènes d'une source à l'autre. Ceci rend donc leur exploitation pour réaliser une interconnexion difficile,voire impossible. Dans cet article, nous proposons de pallier ces difficultés en ancrant les données linked data thématiques aux objets d'un référentiel géographique. Nous mettons à profit le référentiel géographique afin de mettre en correspondance des données thématiques dotées d'indications de localisation hétérogènes. Nous exploitons enfin les relations de correspondance créées entre données thématiques et référentiel géographique dans une application de visualisation cartographique des données.
**** *annee_2014  *numText_116
Nous présentons une nouvelle méthode d'analyse exploratoire de grands flots de liens que nous appliquons à la détection d'événements significatifs dans plus de 2 millions d'interactions (pendant 4 mois) entre utilisateurs du réseau social en ligne Github. Nous combinons une méthode statistique de détection automatique d'événements dans une série temporelle,Outskewer, avec un système de visualisation de graphes. Outskewer identifie des instants de l'évolution du graphe d'interactions méritant d'être étudiés, et un analyste peut valider et interpréter ces événements par la visualisation de motifs anormaux dans les sous-graphes correspondants. Nous montrons par de multiples exemples que cette approche 1) permet de détecter des événements pertinents et de rejeter ceux qui ne le sont pas, 2) est adaptée à une démarche exploratoire car elle ne nécessite pas de connaissance a priori sur les données.
**** *annee_2014  *numText_117
Les acteurs et usagers du domaine médical (médecins, infirmiers, patients,internes, pharmaciens, etc.) ne sont pas issus de la même catégorie socioprofessionnelle et ne présentent pas le même niveau de maîtrise du domaine.Leurs écrits en témoignent et véhiculent, de plus, la subjectivité qui leur est propre. Nous nous intéressons à l'étude automatisée de la subjectivité dans le discours médical dans des textes en langue française. Nous confrontons le discours des médecins (articles scientifiques, rapports cliniques) à celui des patients(messages de forums de santé) en analysant contrastivement les différences d’emploi des descripteurs tels que les marqueurs d'incertitude et de polarité,les marques émotives non lexicales (smileys, ponctuations répétées, etc.)et lexicales, et les termes médicaux relatifs aux pathologies, traitements et procédures.Nous effectuons une annotation et catégorisation automatiques des documents afin de mieux observer les spécificités que présentent les discours médicaux ciblés.
**** *annee_2014  *numText_118
La notion d'incertitude a été longtemps un sujet de controverses. En particulier la prééminence de la théorie des probabilités dans les sciences tend à gommer les différences présentes dans les premières tentatives de formalisation, remontant au 17ème siècle, entre l'incertitude due à la variabilité des phénomènes répétables et l'incertitude due au manque d'information (dite épistémique). L'école Bayésienne affirme que quelle que soit l'origine de l'incertitude,celle-ci peut être modélisée par une distribution de probabilité unique. Cette affirmation a été beaucoup remise en cause dans les trente dernières années. En effet l'emploi systématique d'une distribution unique en cas d'information partielle mène à des utilisations paradoxales de la théorie des probabilités.Dans de nombreux domaines, il est crucial de distinguer entre l'incertitude due à la variabilité d'observations et l'incertitude due à l'ignorance partielle. Cette dernière peut être réduite par l'obtention de nouvelles informations, mais pas la première, dont on ne se prémunit que par des actions concrètes. Dans le cas des bases de données, il est souvent supposé qu'elles sont précises, et l'incertitude correspondante est souvent négligée. Quant elle est abordée on reste souvent dans une approche probabiliste orthodoxe. Néanmoins, les statisticiens ont développé des outils qui ne relèvent pas de la théorie de Kolmogorov pour pallier le manque de données (intervalles de confiance, principe de maximum de vraisemblance...).De nouvelles théories de l'incertain ont émergé, qui offrent la possibilité de représenter les incertitudes épistémiques et aléatoires de façon distincte, notamment l'incertitude épistémique,en remplaçant la distribution de probabilité unique par une famille de distributions possibles,cette famille étant d'autant plus grande que l'information est absente. Cette représentation complexe possède des cas particuliers plus simples à utiliser en pratique, comme les ensembles aléatoires (théorie des fonctions de croyance), les distributions de possibilité (représentant des ensembles flous de valeurs possibles) et les p-boxes, notamment.Le but de cet exposé est de susciter l'intérêt pour ces nouvelles théories de l'incertain, d'en donner les bases formelles, d'en discuter la philosophie sous-jacente, de faire le lien avec certaines notions en statistique, et de les illustrer sur des exemples.
**** *annee_2014  *numText_119
Du fait qu'elles apportent des solutions dans de nombreuses applications,les traverses minimales des hypergraphes ne cessent de susciter l'intérêt de la communauté scientifique et le développement d'algorithmes pour les calculer.Dans cet article, nous présentons une nouvelle approche pour l'optimisation de l'extraction des traverses minimales basée sur les notions d'hypergraphe partiel et de traverses minimales locales selon une stratégie diviser pour régner. Nous introduisons aussi un nouvel algorithme, appelé LOCAL-GENERATOR pour le calcul des traverses minimales. Les expérimentations effectuées sur divers jeux de données ont montré l'intérêt de notre approche, notamment sur les hypergraphes ayant un nombre de transversalité élevé et renfermant un nombre très important de traverses minimales.
**** *annee_2014  *numText_120
Les entités nommées sont des éléments intéressants pour les applications fondées sur le Traitement du Langage Naturel. Dans le cas de la recherche d'information, les entités nommées sont largement employées par les utilisateurs du web dans les requêtes de recherche, soit pour définir un concept de base, soit pour décrire un autre concept dans la requête. Du côté du modèle de recherche, les entités nommées sont des éléments riches en information qui aident à mieux cibler les documents pertinents. Dans cet article, nous étudions l'avantage d'étendre les entités nommées dans la requête. L'idée est d'utiliser une technique d'expansion sémantique sur une ontologie générale (Yago) pour désambiguïser les entités nommées et pour trouver leurs différentes appellations que l'on intègre dans la requête en utilisant 3 approches : sac de mots, dépendance séquentielle, et concept clé. Nous mesurons l'efficacité de ces expériences en termes de précision et rappel, et nous étudions l'effet du rôle des entités nommées sur l'expansion. Nous concluons que l'expansion des entités nommées est une méthode simple qui améliore significativement la qualité de la recherche quand elle est comparée à un modèle de référence sans expansion. De plus, cette méthode est assez compétitive par rapport à l'approche pseudo retour de pertinence souvent utilisée pour l'expansion de la requête.
**** *annee_2014  *numText_121
Pour parler, le locuteur met en mouvement un ensemble complexe d'articulateurs : la mâchoire qu'il ouvre plus ou moins, la langue à laquelle il fait prendre de nombreuses formes et positions, les lèvres qui lui permettent délaisser l'air s'échapper plus ou moins brutalement, etc. Le modèle articulatoire le plus connu est celui de Maeda (1990), obtenu à partir d'Analyses en Composantes Principales faites sur les tableaux de coordonnées des points des articulateurs d'un locuteur en train de parler. Nous proposons ici une analyse 3-way du même type de données, après leur transformation en tableaux de distances. Nous validons notre modèle par la prédiction des sons prononcés, qui s'avère presque aussi bonne que celle du modèle acoustique, et même meilleure quand on prend en compte la co-articulation.
**** *annee_2014  *numText_123
Le nombre de caméras de vidéosurveillance installées dans le monde augmente chaque jour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillance se basent sur les indications spatiales et temporelles de la victime et sur leur connaissance de la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête.Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu'à plusieurs jours de traitement) et (2) un risque important de perte de résultats à cause d'une mauvaise connaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche est de définir des outils d'assistance aux opérateurs qui puissent, à partir d'une trajectoire donnée,sélectionner de façon automatique les caméras pertinentes par rapport à la requête.
**** *annee_2014  *numText_124
Nous proposons une méthode originale pour extraire un résumé compact,représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquentielles. Notre approche consiste à extraire un nouveau type de motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l'aide d'un algorithme hiérarchique agglomératif nommé RepaMiner. Nous générons non pas un simple ensemble de motifs mais une véritable structure dérivée de dendrogrammes, le RPgraph.
**** *annee_2014  *numText_125
La classification recouvrante correspond à un domaine d'étude très actif ces dernières années et dont l'objectif est d'organiser un ensemble de données en groupes d'individus similaires avec la particularité d'autoriser des chevauchements entre les groupes. Parmi les approches étudiées nous nous intéressons aux extensions recouvrantes des modèles de type moindres carrés et constatons les difficultés théoriques et pratiques liées à leur adaptation aux noyaux. Nous formulons alors une nouvelle définition ensembliste pour caractériser un recouvrement de plusieurs classes, nous montrons que cette modélisation permet le recours aux noyaux et nous proposons une solution algorithmique efficace pour répondre au problème de la classification recouvrante à noyaux.
**** *annee_2014  *numText_126
Dans cet article, nous proposons une nouvelle approche permettant à la fois le bi-partitionnement topologique (bi-clustering) et la pondération de blocs variables. Le modèle que nous proposons FBR-BiTM (Feature Block Relevanceusing BiTM) permet de découvrir un espace topologique d'un ensemble d'observations et de variables en associant un nouveau score de pondération à chaque sous ensemble de variables. L'estimation des coefficients de pondération est réalisée dans le même processus d'apprentissage que le bi-partitionnement. Ces pondérations sont locales et associées à chaque prototype. Elles reflètent l'importance locale de chaque bloc de variables pour le bi-partitionnement. L'évaluation montre que l'approche proposée, comparée
**** *annee_2014  *numText_127
Cet article présente une méthode originale de prédiction de valeurs manquantes dans les bases de données relationnelles, fondée sur la notion de proportion analogique. Nous montrons en particulier comment un algorithme proposé dans le cadre de la classification automatique peut être adapté à cette fin.Deux cas sont considérés : celui d'une base de données transactionnelle (attributs booléens), et celui où les valeurs manquantes peuvent être de type numérique.
**** *annee_2014  *numText_128
Les forums de santé en ligne sont des espaces d'échanges où les patients partagent leurs sentiments à propos de leurs maladies, traitements, etc. Sous couvert d'anonymat, ils expriment très librement leurs expériences personnelles.Ces forums sont donc une source d'informations très utile pour les professionnels de santé afin de mieux identifier et comprendre les problèmes, les comportements et les sentiments de leurs patients. Dans cet article, nous proposons d'exploiter les messages des forums via des techniques de fouille de textes pour extraire des traces d'émotions (e.g. joie, colère, surprise , etc.).
**** *annee_2014  *numText_130
La reconstruction de chronologies d’événements cybercriminels (ou reconstruction d’événements) est une étape primordiale dans une investigation numérique. Cette phase permet aux enquêteurs d'avoir une vue des événements survenus durant un incident. La reconstruction d’événements requiert l'étude d'importants volumes de données en raison de l'omniprésence des nouvelles technologies dans notre quotidien. De plus, les conclusions produites se doivent de respecter les critères fixés par la justice. Afin de répondre à ces challenges,nous proposons une nouvelle méthodologie basée sur une ontologie permettant d'assister les enquêteurs tout au long du processus d'enquête.
**** *annee_2014  *numText_131
Dans cet article, nous proposons de montrer l'intérêt et l'utilité de déploiement des règles d'association inter-langues (RAILs) dans le domaine de la Recherche d'Information Multilingue (RIM). Ces règles sont des connaissances additionnelles résultantes d'un processus de fouille de grands corpus parallèles alignés au niveau de la phrase. En effet, leurs conclusions exprimées dans une langue cible représentent des traductions potentielles de leurs prémisses, exprimées dans une langue source. Nous illustrons l'utilisation des RAILs dans le contexte de la RIM à travers deux propositions, à savoir : (i) la traduction des requêtes et (ii) la traduction des termes de l'index. L'évaluation expérimentale a été menée sur la collection de documents MUCHMORE. Les résultats ont montré une amélioration significative de la pertinence système.
**** *annee_2014  *numText_133
Dans cet article, nous nous intéressons à la recherche des points les plus intéressants au sens de l'ordre de Pareto, i.e., à l'évaluation de requêtes« skyline » , dans des jeux de données présentant des anomalies. Il n'est pas rare que les données, de petites annonces par exemple, soient peuplées d'erreurs ou d’exceptions qui peuvent perturber la recherche des meilleurs points car celles ci sont susceptibles de dominer les autres points. L'approche présentée vise à calculer les requêtes skyline malgré la présence de ces exceptions, sans pour autant les écarter définitivement, et à présenter graphiquement les résultats de façon à identifier rapidement les points d'intérêt et les anomalies potentielles.
**** *annee_2014  *numText_134
La technique des K plus proches voisins (KNN) est une méthode d'apprentissage à base d'instances, elle a été appliquée dans la catégorisation de textes depuis de nombreuses années. En contraste avec ses performances de classification, il est reconnu que cet algorithme est lent pendant la classification d'un nouveau document. Les Techniques de sélection de prototypes sont apparues comme des méthodes très compétitives pour améliorer le KNN grâce à la réduction des données. L'étude contenue dans ce papier a pour objectif d'analyser l'impact de ces méthodes sur la performance de la classification de textes avec l'algorithme KNN.
**** *annee_2014  *numText_135
L'objectif de cet article est d'évaluer la capacité de 12 algorithmes de classification multi-label à apprendre, en peu de temps, avec peu d’exemples d’apprentissage. Les résultats expérimentaux montrent des différences importantes entre les méthodes analysées, pour les 3 mesures d'évaluation choisies:Log-Loss, Ranking-Loss et Temps d'apprentissage/prédiction, et les meilleurs résultats sont obtenus avec: multi-label k Nearest neighbours (ML-kNN), suivide Ensemble de Classifier Chains (ECC) et Ensemble de Binary Relevance (EBR).
**** *annee_2014  *numText_136
Dans le domaine de la reconnaissance de gestes isolés, bon nombre de travaux se sont intéressés à la réduction de dimension sur l'axe spatial pour réduire à la fois la complexité algorithmique et la variabilité des réalisations gestuelles. Il est assez étonnant de constater que peu de ces méthodes se sont explicitement penchées sur la réduction de dimension sur l'axe temporel. En matière de complexité, la réduction de dimension sur cet axe est un enjeu majeur quant à l'utilisabilité de distances élastiques en complexité quadratique. Par ailleurs, la prise en compte de la variabilité sur cet axe demeure une source avérée de gain de performance. Pour tenter d'apporter un éclairage en matière de réduction de dimension sur l'axe temporel, nous présentons dans cet article une approche basée sur un sous échantillonnage temporel associé à l'exploitation d'un apprentissage automatique à base de noyaux élastiques. Nous montrons expérimentalement, sur deux jeux de données très référencés dans la communauté et très opposés en matière de qualité de capture de mouvement, qu'il est possible de réduire sensiblement le nombre de postures sur les trajectoires temporelles tout en conservant, grâce à des noyaux élastiques, des performances de reconnaissance au niveau de l'état de l'art du domaine. Le gain de complexité obtenu rend une telle approche éligible pour des applications temps-réel.
**** *annee_2014  *numText_137
Dans cet article, nous proposons une approche argumentative visant à automatiser la résolution des conflits entre les décideurs qui ont des préférences contradictoires lors d'une classification multicritères collaborative des connaissances cruciales. Notre étude expérimentale a prouvé que cette approche peut résoudre jusqu'à 81% des conflits et améliorer la qualité d'approximation de décideurs d'un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur.
**** *annee_2014  *numText_138
Les symétries sont des propriétés structurelles qu'on détecte dans un grand nombre de bases de données. Dans cet article, nous étudions l'exploitation des symétries pour élaguer l'espace de recherche dans les problèmes d'extraction de motifs ensemblistes. Notre approche est basée sur une intégration dynamique des symétries dans les algorithmes de type Apriori permettant de réduire l'espace des motifs candidats. En effet, pour un motif donné, les symétries nous permettent de déduire les motifs qui lui sont symétriques et vérifiant par conséquent les mêmes propriétés. Nous détaillons notre approche en utilisant l'exemple des motifs fréquents. Ensuite, nous la généralisons au cadre unificateur de Mannila et Toivonen pour l'extraction des motifs ensemblistes. Les expériences menées montrent la faisabilité et l'apport de notre approche d'élagage basé sur les symétries.
**** *annee_2014  *numText_141
En classification non-supervisée, le consensus de partitions a pour objectif de produire une partition unique, représentant le consensus, à partir d’un ensemble de partitions où chacune est engendrée indépendamment des autres,voire avec des méthodologies différentes. En complément des techniques ayant leur qualité propre en terme de robustesse ou de passage à l'échelle, nous apportons un point de vue original sur le consensus de partitions, c'est-à-dire, par le biais de définitions algébriques qui permettent d'établir la nature des déductions pouvant être réalisées dans une approche systématique (p.ex. un système à base de connaissances). Nous fondons notre approche sur le treillis des partitions pour lequel nous montrons comment peuvent être adjoint des opérateurs dans le but de formuler une expression caractérisant le consensus à partir d'un ensemble de partitions.
**** *annee_2014  *numText_142
Dans le cadre des cartes topologiques, nous proposons une nouvelle approche d'ensemble clusters basée sur la méthode STATIS. Les méthodes d’ensemble clusters visent à améliorer la qualité de la partition d'un jeu de données à travers la combinaison de plusieurs partitions.Les différentes partitions peuvent être obtenues en faisant varier les paramètres d’un algorithme (choix des centres initiaux, du voisinage initial et final des cellules dans le cas des cartes topologiques auto-organisée SOM, etc). L’approche présentée dans cette communication repose sur la méthode d'analyse de données multi-tableaux STATIS pour déterminer une matrice compromis représentant au mieux la similarité entre les partitions issues des cartes topologiques. La fusion des cartes topologiques est alors obtenue à travers une classification basée sur cette matrice compromis. La méthode proposée est illustrée sur des données réelles issues de l'UCI et sur des données simulées.
**** *annee_2014  *numText_143
Nous proposons dans cet article une nouvelle approche croisant des techniques de programmation par contraintes et de fouille pour l'extraction de motifs séquentiels. Le modèle que nous proposons offre un cadre générique et déclaratif pour modéliser et résoudre des contraintes de nature hétérogène
**** *annee_2014  *numText_144
Au vu de l'émergence rapide des nouvelles technologies mobiles et la croissance des offres et besoins d'une société en mouvement, les travaux se multiplient pour identifier de nouvelles plate-formes d'apprentissage pertinentes afin d’améliorer et faciliter l'apprentissage à distance. La prochaine étape de l’apprentissage à distance est naturellement le port de l'e-learning (apprentissage électronique) vers les nouveaux systèmes mobiles. On parle de m-learning (apprentissage mobile). Nos travaux portent sur le développement d'une nouvelle architecture pour le m-learning dont l'objectif est d'adapter et recommander des parcours de formations selon les contraintes contextuelles de l'apprenant.
**** *annee_2014  *numText_145
Trouver le nombre optimal de groupes dans le contexte d'un algorithme de clustering est un problème notoirement difficile. Dans cet article,nous en décrivons et évaluons une solution approchée dans le cas de l’algorithme spectral. Notre méthode présente l'avantage d'être déterministe, et peu coûteuse. Nous montrons qu'elle fonctionne de manière satisfaisante dans beaucoup de cas, même si quelques limites amènent des perspectives à ce travail.
**** *annee_2014  *numText_146
Dans cet article, nous nous intéressons à la détection du profil des auteurs (âge, genre) à travers leurs discussions. La méthode proposée s’appuie sur la classification automatique qui utilise certaines données extraites d'une manière statistique à partir de corpus source. Nous présentons une méthode hybride qui combine l'analyse de surface dans les textes avec une méthode d’apprentissage automatique. A fin d'obtenir une meilleure gestion de ces données, nous nous sommes basés sur l'utilisation des arbres de décision. Notre méthode adonné des résultats intéressants pour la détection du genre.
**** *annee_2014  *numText_147
De nombreux systèmes complexes sont étudiés via l'analyse de réseaux dits complexes ayant des propriétés topologiques typiques. Parmi celles ci,les structures de communautés sont particulièrement étudiées. De nombreuses méthodes permettent de les détecter, y compris dans des réseaux contenant des attributs nodaux, des liens orientés ou évoluant dans le temps. La détection prend la forme d'une partition de l'ensemble des nœuds, qu'il faut ensuite caractériser relativement au système modélisé. Nous travaillons sur l'assistance à cette tâche de caractérisation. Nous proposons une représentation des réseaux sous la forme de séquences de descripteurs de nœuds, qui combinent les informations temporelles, les mesures topologiques, et les valeurs des attributs nodaux. Les communautés sont caractérisées au moyen des motifs séquentiels émergents lesplus représentatifs issus de leurs nœuds. Ceci permet notamment la détection de comportements inhabituels au sein d'une communauté. Nous décrivons une étude empirique sur un réseau de collaboration scientifique.
**** *annee_2014  *numText_148
L'explosion du volume de messages échangés via Twitter entraîne un phénomène de surcharge informationnelle pour ses utilisateurs. Il est donc crucial de doter ces derniers de moyens les aidant à filtrer l'information brute, laquelle est délivrée sous la forme d'un flux de messages. Dans cette optique, nous proposons une méthode basée sur la modélisation de l'anomalie dans la fréquence de création de liens dynamiques entre utilisateurs pour détecter les pics de popularité et extraire une liste ordonnée de thématiques populaires. Les expérimentations menées sur des données réelles montrent que la méthode proposée est capable d'identifier et localiser efficacement les thématiques populaires.
**** *annee_2014  *numText_149
La maximisation d'étiquetage (F-max) est une métrique non biaisée d’estimation de la qualité d'une classification non supervisée (clustering) qui favorise les clusters ayant une valeur maximale de F-mesure d'étiquetage. Dans cet article, nous montrons qu'une adaptation de cette métrique dans le cadrede la classification supervisée permet de réaliser une sélection de variables et de calculer pour chacune d'elles une fonction de contraste. La méthode est expérimentée sur différents types de données textuelles. Dans ce contexte, nous montrons que cette technique améliore les performances des méthodes de classification de façon très significative par rapport à l'état de l'art des techniques de sélection de variables, notamment dans le cas de la classification de données textuelles déséquilibrées, fortement multidimensionnelles et bruitées.
**** *annee_2014  *numText_150
Face à la complexité des nouvelles générations d'images médicales, les processus de recherche d'images basés sur leurs contenus visuels peuvent s'avérer insuffisants. Cet article propose une nouvelle approche basée sur l'annotation des images via des termes sémantiques pouvant pallier ce problème. Elle repose sur la combinaison d'une distance hiérarchique permettant de comparer les images en considérant les corrélations entre les termes utilisés pour les décrire et d'une mesure de similarité permettant d'évaluer la proximité sémantique entre des termes ontologiques. Cette approche est validée dans le cadre de la recherche d'images tomodensitométriques.
**** *annee_2014  *numText_152
L'induction d'arbre de décision est une technique puissante et populaire pour extraire de la connaissance. Néanmoins, les arbres de décision obtenus depuis des données issues du monde réel peuvent être très complexes et donc difficiles à exploiter. Dans ce cadre, cet article présente une solution originale pour adapter le résultat d'une classification non supervisée quelconque afin d’obtenir des arbres de décision simplifiés pour chaque cluster.
**** *annee_2014  *numText_153
La modularité, introduite par Newman pour mesurer la qualité d'une partition des sommets d'un graphe, ne prend pas en compte d'éventuelles valeurs associées à ces sommets. Dans cet article, nous introduisons une mesure de modularité complémentaire, basée sur l'inertie, et adaptée pour évaluer la qualité d'une partition d'éléments représentés dans un espace vectoriel réel. Cette mesure se veut un pendant pour la classification non supervisée de la modularité de Newman. Nous présentons également 2Mod-Louvain, une méthode utilisant ce critère de modularité basée sur l'inertie conjointement à la modularité de Newman pour détecter des communautés dans des réseaux d'information. Les expérimentations que nous avons menées ont montré qu'en exploitant à la fois les données relationnelles et vectorielles, 2Mod-Louvain détectait plus efficacement les communautés que des méthodes utilisant un seul type de données et qu'elle était robuste face à des dégradations des données.
**** *annee_2014  *numText_154
Dans cet article nous présentons deux approches de visualisation développées dans le cadre d'un projet collaboratif sur l'accès et l'exploitation des données prosopographiques de la Renaissance en France. L'objectif du projet est de modéliser et réaliser un portail sémantique assurant l'accès à différentes bases de données prosopographiques existantes afin de permettre une meilleure exploration et exploitation de ces données. Dans ce cadre, nous avons proposé deux interfaces de visualisation ProsoGraph et ProsoMap qui s'appuient respectivement sur la visualisation de graphes de réseaux sociaux et la visualisation de lieux géographiques et de trajectoires spatio-temporelles. Les deux interfaces communiquent avec le portail via une couche sémantique et lui offrent des fonctionnalités d'interrogation supplémentaires.
**** *annee_2013  *numText_155
Depuis deux décennies, la découverte de motifs a été l'un des champs de recherche les plus actifs de l'exploration de données. Cet article en établit une étude bibliographique quantitative en nous appuyant sur 1030 publications issues de 5 conférences internationales majeures : KDD, PKDD, PAKDD, ICDM et SDM. Nous avons d'abord mesuré depuis 2005 un sévère ralentissement de l'activité de recherche dédiée à la découverte de motifs. Puis, nous avons quantifié les principales contributions en terme de langages, de contraintes et de représentations condensées de sorte à comprendre ce ralentissement et à esquisser les directions actuelles.
**** *annee_2013  *numText_156
Si la 3D est un sujet de débat dans la communauté, les expériences sur lesquelles s'appuient les discussions concernent le plus souvent des restitutions visuelles basées sur une projection classique en perspective linéaire. L'objectif de cette communication est de renouveler le cadre expérimental en étudiant l'impact de l'ajout de la disparité binoculaire. Nous nous focalisons ici sur une tâche importante en analyse de réseaux : l'identification de communautés. Et nous comparons la 3D monoscopique et la 3D stéréoscopique à la fois pour la performance de résolution de la tâche et pour le comportement exploratoire à travers l'analyse du mouvement du pointeur de la souris et de la dynamique des modifications de points de vue sur les graphes. Nos résultats expérimentaux mettent en évidence des performances significativement meilleures pour la 3D stéréoscopique et des différences comportementales dans l'exploration avec un centrage plus important sur des zones restreintes en stéréoscopie.
**** *annee_2013  *numText_158
Parmi la panoplie de classificateurs utilisés dans la catégorisation de textes, nous nous intéressons à l'algorithme des k-voisins les plus proches. Ces performances le situent parmi les meilleures méthodes de catégorisation de textes. Toutefois, il présente certaines limites: (i) coût mémoire car il faut stocker l'ensemble d'apprentissage en entier et (ii) coût élevé de calcul car il doit explorer l'ensemble d'apprentissage pour classer un nouveau document. Dans ce papier, nous proposons une nouvelle démarche pour réduire ce temps de classification sans dégrader les performances de classification.
**** *annee_2013  *numText_159
Dans cet article nous présentons une approche conceptuelle d'aide à la décision dans la conception de systèmes complexes. Cette approche s'appuie sur le formalisme de l'analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l'exploration de données de simulation afin d'aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L'approche est illustrée sur un cas test de conception de cabine d'un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d'identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l'ACFS permet d'identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.
**** *annee_2013  *numText_160
L'analyse formelle de concepts (AFC) est un formalisme de représentation et d'extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).L'AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l'extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.Dans cette présentation, nous allons explorer le potentiel de l'AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l'analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex. détection de communautés et d'individus influents à partir d'un réseau d'amis) ou même de réseaux hétérogènes (ex. extraction de règles d'association d'un réseau bibliographique).
**** *annee_2013  *numText_161
La gestion des réclamations est un élément fondamental dans la relation client. C'est le cas en particulier pour la Caisse Nationale des Allocations Familiales qui veut mettre en place une politique nationale pour faciliter cette gestion. Dans cet article, nous décrivons la démarche que nous avons adoptée afin de traiter automatiquement les réclamations provenant d'allocataires de la CAF du Rhône. Les données brutes mises à notre disposition nécessitent une série importante de prétraitements pour les rendre utilisables. Une fois ces données correctement nettoyées, des techniques issues de l'analyse des données et de l'apprentissage non supervisé nous permettent d'extraire à la fois une typologie des réclamations basée sur leur contenu textuel mais aussi une typologie des allocataires réclamants. Après avoir présenté ces deux typologies, nous les mettons en correspondance afin de voir comment les allocataires se distribuent selon les différents types de réclamation.
**** *annee_2013  *numText_162
L'Analyse Relationnelle de Concepts (ARC) est une extension de l'Analyse Formelle de Concepts (AFC), une méthode de classification non supervisée d'objets sous forme de treillis de concepts. L'ARC supporte en plus la gestion de relations entre objets de différents contextes ce qui permet d'établir des liens entre les concepts des différents treillis. Cette particularité lui permet d'être plus intuitive à utiliser pour extraire des connaissances à partir de données relationnelles et de donner des résultats plus riches. Malheureusement lorsque les jeux de données présentent de nombreuses relations, les résultats obtenus sont difficilement exploitables et des problèmes de passages à l'échelle se posent. Nous proposons dans cet article une adaptation possible de l'ARC pour explorer les relations de manière supervisée pour augmenter la pertinence des résultats obtenus et réduire le temps de calcul. Nous prenons pour exemple des données hydrobiologiques ayant trait à la qualité des milieux aquatiques.
**** *annee_2013  *numText_163
La classification orientée objet (COO) prend de plus en plus de dimension dans les travaux de télédétection grâce à sa capacité d'intégrer des connaissances de haut niveau telles que la taille, la forme et les informations de voisinage. Cependant, les approches existantes restent tributaires de l'étape de construction des objets à cause de l'absence d'interaction entre celle-ci et celle de leur identification. Dans cet article, nous proposons une approche sémantique, hiérarchique et collaborative entre les algorithmes de croissances de régions et une classification orientée objet supervisée, permettant une coopération entre l'extraction et l'identification des objets de l'image. Les expériences menées sur une image de très haute résolution de la région de Strasbourg ont confirmé l'intérêt de l'approche introduite.
**** *annee_2013  *numText_164
Cet article présente une application de classification multi-étiquettes permettant de déterminer le programme à utiliser pour construire un alignement multiple d'un ensemble de séquences protéiques donné. Dans un premier temps, nous avons réussi à améliorer le système existant, Alexsys en ajoutant des attributs. Dans un second temps, nous déterminons pour un ensemble de séquences protéiques donné le ou les aligneurs capable de produire les alignements de meilleur score, à epsilon près. Les mesures de performances propres à la classification multi-étiquette nous permettent d'analyser l'influence de epsilon et de choisir une valeur assez petite pour distinguer les meilleurs aligneurs des autres.
**** *annee_2013  *numText_165
Le clustering (ou classification non supervisée) de trajectoires a fait l'objet d'un nombre considérable de travaux de recherche. La majorité de ces travaux s'est intéressée au cas où les objets mobiles engendrant ces trajectoires se déplacent librement dans un espace euclidien et ne prennent pas en compte les contraintes liées à la structure sous-jacente du réseau qu'ils parcourent (ex. réseau routier). Dans le présent article, nous proposons au contraire la prise en compte explicite de ces contraintes. Nous représenterons les relations entre trajectoires et segments routiers par un graphe biparti et nous étudierons la classification de ses sommets. Nous illustrerons, sur un jeu de données synthétiques, l'utilité d'une telle étude pour comprendre la dynamique du mouvement dans le réseau routier et analyser le comportement des véhicules qui l'empruntent.
**** *annee_2013  *numText_166
L'accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens.Le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C'est par exemple le cas pour l'intégration d'un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d'un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).Le deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l'objet d'un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L'historien doit donc les évaluer et les valider s'il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d'autres chercheurs.Dans les deux cas, il est nécessaire de développer des outils d'appropriation, qui permettent d'entrer dans le réel historique contenu dans les stocks de données. C'est là la fonction du projet Histobase, un système permettant d'entrer dans la structuration des gisements, d'en évaluer l'information, d'ajouter des couches d'interprétation (qualification de l'information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l'objet d'une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en œuvre.
**** *annee_2013  *numText_167
Nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles. Ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes : (i) à partir des données originales, nous générons de multiples nouvelles représentations simples ; (ii) sur chacune de ces représentations, nous appliquons un algorithme de coclustering ; (iii) à partir des résultats de co-clustering, nous construisons de nouveaux descripteurs pour les séries temporelles. Nous obtenons une nouvelle base de données objets-attributs dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées. Nous utilisons un classifieur Bayésien sur cette nouvelle base de données. Nous montrons expérimentalement que ce processus offre de très bonnes performances prédictives comparées à l'état de l'art.
**** *annee_2013  *numText_168
Les skypatterns sont des motifs traduisant des préférences de l'utilisateur selon une relation de dominance. Dans cet article, nous introduisons la notion de souplesse dans la problématique des skypatterns et nous montrons comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. Nous proposons une méthode efficace d'extraction de skypatterns ainsi que de soft-skypatterns, méthode fondée sur la programmation par contraintes. La pertinence de notre approche est illustrée à travers une étude de cas en chémoinformatique pour la découverte de toxicophores.
**** *annee_2013  *numText_170
L'extraction des traverses minimales d'un hypergraphe est une problématique réputée comme particulièrement difficile et qui a fait l'objet de plusieurs travaux dans la littérature. Dans cet article, nous établissons un lien entre les concepts de la fouille de données et ceux de la théorie des hypergraphes, proposant ainsi un cadre méthodologique pour le calcul des traverses minimales. Le nombre de ces traverses minimales étant, souvent, exponentiel même pour des hypergraphes simples, nous proposons d'en représenter l'ensemble de manière concise et exacte. Pour ce faire, nous introduisons la notion de traverses minimales irrédondantes, à partir desquelles nous pouvons retrouver l'ensemble global de toutes les traverses minimales, à l'aide de l'algorithme IMT-EXTRACTOR. Une étude expérimentale de ce nouvel algorithme a confirmé l'intérêt de l'approche introduite.
**** *annee_2013  *numText_171
Dans ce papier, nous présentons une nouvelle approche qui permet la détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution. S'agissant d'un domaine où la concurrence est très vive entre les différentes enseignes avec des enjeux financiers colossaux, les stratégies commerciales ont pour principal objectif de fidéliser la clientèle pour limiter leur défection. C'est là qu'intervient la détection des changements de tendances produits, qui va permettre d'anticiper l'attrition de la clientèle. Déceler des tendances suffisamment tôt permettra aux décideurs de mettre en place des stratégies préventives efficaces à moindre coût. Notre objectif est donc d'analyser et de modéliser clairement les changements de tendances et leurs impacts potentiels globaux sur les achats des clients. Nous illustrerons notre approche sur des données réelles d'achats de clients d'une grande enseigne.
**** *annee_2013  *numText_172
Nous présentons une approche pour enrichir automatiquement une ontologie à partir d'un ensemble de pages web structurées. Cette approche s'appuie sur un noyau d'ontologie initial. Son originalité est d'exploiter conjointement la structure des documents et des annotations sémantiques produites à l'aide du noyau d'ontologie pour identifier de nouveaux concepts et des spécialisations de relations qui enrichissent l'ontologie. Nous avons implémenté et évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage.
**** *annee_2013  *numText_173
Nous proposons dans cet article de présenter une application d'analyse d'une base de données de grande taille issue du secteur des télécommunications. Le problème consiste à segmenter un territoire et caractériser les zones ainsi définies grâce au comportement des habitants en terme de téléphonie mobile. Nous disposons pour cela d'un réseau d'appels inter-antennes construit pendant une période de cinq mois sur l'ensemble de la France. Nous proposons une analyse en deux phases. La première couple les antennes émettrices dont les appels sont similairement distribués sur les antennes réceptrices et vice versa. Une projection de ces groupes d'antennes sur une carte de France permet une visualisation des corrélations entre la géographie du territoire et le comportement de ses habitants en terme de téléphonie. La seconde phase découpe l'année en périodes entre lesquelles on observe un changement de distributions d'appels sortant des groupes d'antennes. On peut ainsi caractériser l'évolution temporelle du comportement des usagers de mobiles dans chacune des zones du pays.
**** *annee_2013  *numText_174
Cet article étudie les possibilités d'utilisation d'oubli dans l'apprentissage incrémental en-ligne de classifieurs évolutifs basés sur des systèmes d'inférence floue. Pour cela, nous étudions différentes possibilités, existant dans la littérature dédiée au contrôle, pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs. Nous présentons l'impact de ces différentes techniques dans le contexte de l'apprentissage incrémental de classifieurs évolutifs en environnement non stationnaire. Ces approches sont évaluées, pour l'optimisation des systèmes d'inférence floue, sur la problématique de la reconnaissance de gestes manuscrits sur surface tactile.
**** *annee_2013  *numText_175
Nous nous intéressons dans cet article à la problématique d'évolution d'une ontologie permettant de représenter des relations n-aires. Nous présentons la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence structurelle. Nous illustrerons nos propos sur une ontologie dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives.
**** *annee_2013  *numText_176
Les graphes orientés acycliques attribués peuvent être utilisés dans beaucoup de domaines applicatif. Dans ce papier, nous étudions un nouveau domaine de motif pour permettre leur analyse : les chemins pondérés fréquents. Nous proposons en conséquence des contraintes primitives permettant d'évaluer leur pertinence (par exemple, les contraintes de fréquence et de compacité), et un algorithme extrayant ces solutions. Nous aboutissons à une représentation condensée dont l'efficacité et le passage à l'échelle sont étudiés empiriquement.
**** *annee_2013  *numText_177
L'extraction de motifs fréquents est une tâche importante en fouille de données. Initialement centrés sur la découverte d'ensembles d'items fréquents, les premiers travaux ont été étendus pour extraire des motifs structurels comme des séquences, des arbres ou des graphes. Dans cet article, nous proposons une nouvelle méthode de fouille de données qui consiste à extraire de nouveaux types de motifs à partir d'une collection d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des ensembles d'attributs. L'extraction de ces motifs (appelés sous-arbres attribués) combine une recherche d'ensembles d'items fréquents à une recherche de sous-arbres et nécessite d'explorer un immense espace de recherche. Nous présentons plusieurs nouveaux algorithmes d'extraction d'arbres attribués et montrons que leurs implémentations peuvent efficacement extraire des motifs fréquents à partir de grands jeux de données.
**** *annee_2013  *numText_178
L'analyse exploratoire de données multidimensionnelles est un problème complexe. Nous proposons d'extraire certains invariants topologiques appelés nombre de Betti, pour synthétiser la topologie de la structure sous-jacente aux données. Nous définissons un modèle génératif basé sur le complexe simplicial de Delaunay dont nous estimons les paramètres par l'optimisation du critère d'information Bayésien (BIC). Ce Complexe Simplicial Génératif nous permet d'extraire les nombres de Betti de données jouets et d'images d'objets en rotation. Comparé à la technique géométrique des Witness Complex, le CSG apparaît plus robuste aux données bruitées.
**** *annee_2013  *numText_179
Nous proposons dans cet article un Système de Recherche d'Information (SRI) qui se base sur des techniques d'indexation de textes en langue naturelle. Nous présentons une méthode d'indexation de documents qui repose sur une approche hybride pour la sélection de descripteurs textuels. Cette approche emploie des traitements du langage naturel pour l'extraction des syntagmes nominaux et sur un filtrage statistique basé sur l'information mutuelle pour sélectionner les syntagmes nominaux les plus informatifs pour le processus d'indexation. Nous effectuons des expérimentations en utilisant le corpus Le Monde 94 de la collection CLEF 2001 et sur le SRI Lemur pour évaluer l'approche proposée.
**** *annee_2013  *numText_180
La littérature s'est beaucoup intéressée à l'extraction de règles d'association positives et peu à l'extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes. Dans cet article, nous nous sommes intéressés aux algorithmes d'extraction de RAPN (Règles d'Association Positives et Négatives) reposant sur l'algorithme fondateur Apriori. Nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients. A l'issue de cette étude, nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites et au niveau du parcours de recherche des règles. L'étude s'est terminée par une évaluation de cet algorithme sur plusieurs bases de données.
**** *annee_2013  *numText_181
Nous présentons une méthode en-ligne de détection de changement de concept dans un flux étiqueté. Notre méthode de détection est basée sur un critère supervisé bivarié qui permet d'identifier si les données de deux fenêtres proviennent ou non de la même distribution. Notre méthode a l'intérêt de n'avoir aucun a priori sur la distribution des données, ni sur le type de changement et est capable de détecter des changements de différentes natures (changement dans la moyenne, dans la variance...). Les expérimentations montrent que notre méthode est plus performante et robuste que les méthodes de l'état de l'art testées. De plus, à part la taille des fenêtres, elle ne requiert aucun paramètre utilisateur.
**** *annee_2013  *numText_182
Les travaux présentés dans cet article s'inscrivent dans le paradigme des recherches visant à acquérir des relations sémantiques à partir de folksonomies (ensemble de tags attribués à des ressources par des utilisateurs). Nous expérimentons plusieurs approches issues de l'état de l'art ainsi que l'apport de l'apprentissage automatique pour l'identification de relations entre tags. Nous obtenons dans le meilleur des cas un taux d'erreur de 23,7 % (relations non reconnues ou fausses), ce qui est encourageant au vu de la difficulté de la tâche (les annotateurs humains ont un taux de désaccord de 12%).
**** *annee_2013  *numText_183
Nous proposons une approche permettant de prédire des complexes impliquant trois protéines (appelés trimères) à partir de combinaison de classifieurs appris sur des complexes n'impliquant que deux protéines (dimères). La prédiction de ces trimères repose sur deux hypothèses biologiques : (i) deux protéines orthologues présentent des caractéristiques fonctionnelles similaires; (ii) deux protéines interagissant sous la forme d'un complexe sous-tendent une fonction biologique essentielle à l'espèce concernée. Ces deux hypothèses sont exploitées pour décrire chaque paire de protéines par l'ensemble des espèces pour lesquelles elles possèdent un orthologue. Un ensemble de mesures de qualité classiquement utilisées pour évaluer l'intérêt des règles d'association est utilisé pour évaluer la force du lien entre les deux protéines. L'organisme modèle Escherichia Coli a été utilisé pour évaluer notre approche.
**** *annee_2013  *numText_184
La réponse cellulaire d'un organisme vivant à un signal donné, hormone, stress ou médicament, met en jeu des mécanismes complexes d'interaction et de régulation entre les gènes, les ARN messagers, les protéines et d'autres éléments tels que les micro-ARNs. On parle de réseau d'interaction pour décrire l'ensemble des interactions possibles entre protéines et de réseau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d'envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d'observation de ces réseaux, ce problème d'identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s'inscrit dans le cadre général de la fouille de données et plus particulièrement de l'apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d'étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.En partant des deux problématiques distinctes que sont l'inférence de réseau d'interaction et l'inférence de réseau de régulation, je montrerai que ces deux tâches d'apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L'inférence de réseau d'interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d'apprentissage d'un noyau de sortie à partir d'un noyau d'entrée. L'inférence de réseau de régulation, impliquant la modélisation d'un système dynamique, peut être abordée par l'approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d'excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l'homme illustreront mes propos. En fin d'exposé, je tracerai quelques perspectives concernant les nouveaux défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées.
**** *annee_2013  *numText_185
Les réseaux sociaux tels que Twitter font partie du phénomène de Déluge des données, expression utilisée pour décrire l'apparition de données de plus en plus volumineuses et complexes. Pour représenter ces réseaux, des graphes orientés sont souvent utilisés. Dans cet article, nous nous focalisons sur deux aspects de l'analyse du réseau social de Twitter. En premier lieu, notre but est de trouver une méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utilisant des ressources informatiques raisonnables. Cet axe de recherche constitue un enjeu majeur puisqu'il est ainsi possible de traiter des graphes à échelle réelle sur des machines potentiellement accessibles par tous. Ensuite, nous étudions les capitalistes sociaux, un type particulier d'utilisateurs de Twitter observé par Ghosh et al. (2012). Nous proposons une méthode pour détecter et classifier efficacement ces utilisateurs.
**** *annee_2013  *numText_186
L'émergence des réseaux sociaux a révolutionné le Web en permettant notamment aux individus de prolonger leur connexion virtuelle en une relation plus réelle et de partager leurs connaissances. Ce nouveau contexte de diffusion de l'information sur le Web peut constituer un moyen efficace pour cerner les besoins en information des utilisateurs du Web, et permettre à la recherche d'information (RI) de mieux répondre à ces besoins en adaptant les modèles d'indexation et d'interrogation. L'exploitation des réseaux sociaux confronte la RI à plusieurs défis dont les plus importants concernent la représentation de l'information dans ce modèle social de RI et son évaluation, en l'absence de collections de test et de compétitions dédiées. Dans cet article, nous présentons un modèle de RI sociale dans lequel nous proposons de modéliser et d'exploiter le contexte social de l'utilisateur. Nous avons évalué notre modèle à l'aide d'une collection de test de RI sociale construite à partir des annotations du réseau social de bookmarking collaboratif Delicious.
**** *annee_2013  *numText_188
Dans ce papier, nous proposons une nouvelle approche topologique de bi-partitionnement (bi-clustering) appelée BiTM en utilisant les cartes autoorganisatrices. L'idée principale de l'approche est d'utiliser une seule carte pour le partitionnement simultané des lignes (observations) et des colonnes (variables). Contrairement aux approches utilisant les cartes topologiques, notre modèle ne nécessite pas de pré-traitement de la base de données. Ainsi, une nouvelle fonction de coût est proposée. De plus, BiTM fournit une visualisation topologique des blocs ou bi-clusters facilement interprétable. Les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique.
**** *annee_2013  *numText_189
Le nombre croissant d'ontologies rend le processus d'alignement une composante essentielle du Web sémantique. Plusieurs outils ont été conçus dans le but de produire des alignements. La qualité des alignements fournis par ces outils est étroitement liée à certains paramètres qui régissent leurs traitements. Dans ce papier, nous proposons une nouvelle approche permettant l'adaptation automatique des paramètres d'alignement d'ontologies par l'utilisation de l'intégrale de Choquet, comme un opérateur d'agrégation. Les expérimentations montrent une nette amélioration des résultats par rapport à un paramétrage statique et figé.
**** *annee_2013  *numText_190
Nous proposons dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif. L'approche utilise une méthode d'extraction de classes basée sur la détection de limite de classe, chaque classe étant définie par son centre. Nous avons également défini des critères d'évaluation adaptés à la méthode proposée. Plusieurs expérimentations ont montré l'intérêt de l'approche dans divers problèmes.
**** *annee_2013  *numText_191
Nous décrivons la deuxième phase de réalisation d'un système d'intégration qui minimise l'intervention humaine habituellement nécessaire. Après la phase de construction semi-automatique du schéma (ontologie) global décrite dans de précédents articles, nous présentons ici le processus de ré-écriture de requêtes globales en des requêtes adressées aux sources.
**** *annee_2013  *numText_192
La recherche de documents similaires est un processus qui consiste à trouver les documents présentant des similitudes, comme la copie ou la reformulation, sur des bases documentaires ou sur internet. Elle est utilisée notamment pour protéger la propriété intellectuelle de productions issues de l'enseignement, de la recherche ou de l'industrie. Dans cet article, nous définissons une approche automatique pour permettant d'extraire des mots-clés d'un document en effectuant un bouclage sur une succession de découpage de plus en plus petit. Cette approche permet d'obtenir des mots-clés impossibles à obtenir par une approche globale notamment quand la thématique, le style ou le contenu d'un document varient dans le document. L'objectif est de permettre la détection des documents présentant des similitudes en utilisant uniquement des mots-clés.
**** *annee_2013  *numText_193
Notre travail porte sur l'aide à l'observation de l'activité dans les simulateurs pleine échelle de centrale nucléaire pour assister les formateurs pendant les simulations. Notre approche consiste à représenter l'activité sous la forme de trace modélisée et à les transformer afin d'extraire et de visualiser des informations de haut niveau permettant aux formateurs de mieux retracer et analyser les simulations. Afin de valider notre approche, nous avons conçu le prototype D3KODE que nous avons évalué avec des experts formateurs d'EDF.
**** *annee_2013  *numText_194
La sélection des variables a un rôle très important dans la fouille de données lorsqu'un grand nombre de variables est disponible. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. Une méthode de sélection a pour objectif de mesurer la pertinence d'un ensemble utilisant principalement un critère d'évaluation. Nous présentons dans cet article un critère non supervisé permettant de mesurer la pertinence d'un sous-ensemble de variables. Ce dernier repose sur l'utilisation du score Laplacien auquel nous avons ajouté des contraintes hiérarchiques. Travailler dans le cadre non supervisé est un vrai challenge dans ce domaine dû à l'absence des étiquettes de classes. Les résultats obtenus sur plusieurs bases de tests sont très encourageants et prometteurs.
**** *annee_2013  *numText_195
Cet article propose un nouvel algorithme pour le problème de subspace clustering dénommé SNOW. Contrairement aux approches descendantes classiques, il ne repose pas sur l'hypothèse de localité et permet l'affectation d'une donnée à plusieurs clusters dans des sous-espaces différents. Les expérimentations préliminaires montrent que notre approche obtient de meilleurs résultats que l'algorithme COPAC sur une base de référence et a été appliquée sur une base de données réelles.
**** *annee_2013  *numText_196
La factorisation de matrices offre une grande qualité de prédiction pour les systèmes de recommandation. Mais sa nature statique empêche de tenir compte des nouvelles notes que les utilisateurs produisent en continu. Ainsi, la qualité des prédictions décroît entre deux factorisations lorsque de nombreuses notes ne sont pas prises en compte. La quantité de notes écartées est d'autant plus grande que la période entre deux factorisation est longue, ce qui accentue la baisse de qualité.Nos travaux visent à améliorer la qualité des recommandations. Nous proposons une factorisation de matrices utilisant des groupes de produits et intégrant en ligne les nouvelles notes des utilisateurs. Nous attribuons à chaque utilisateur un biais pour chaque groupe de produits similaires que nous mettons à jour. Ainsi, nous améliorons significativement les prédictions entre deux factorisations. Nos expérimentations sur des jeux de données réels montrent l'efficacité de notre approche.
**** *annee_2013  *numText_197
Dans cet article, nous nous intéressons aux méthodes d'extraction d'informations spatiales dans des documents textuels. Nous présentons la méthode hybride Text2Geo qui combine une approche d'extraction d'informations, fondée sur des patrons avec une approche de classification supervisée permettant d'explorer le contexte associé. Nous discutons des résultats expérimentaux obtenus sur le jeu de données de l'étang de Thau.
**** *annee_2013  *numText_198
Alors que les réseaux sociaux s'attachaient à représenter des entités et les relations qui existaient entre elles, les réseaux d'information intègrent également des attributs décrivant ces entités ; ce qui conduit à revisiter les méthodes d'analyse et de fouille de ces réseaux. Dans cet article, nous proposons une méthode de classification des sommets d'un graphe qui exploite d'une part leurs relations et d'autre part les attributs les caractérisant. Cette méthode reprend le principe de la méthode de Louvain en l'étendant de façon à permettre la manipulation d'attributs continus d'une manière symétrique à ce qui existe pour les relations.
**** *annee_2013  *numText_200
Dans le contexte de la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. Dans cet article, nous proposons un Framework basé sur des itemsets pour la construction de variables à partir des tables secondaires. L'informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen d'un critère régularisé qui vise à éviter le sur-apprentissage. Pour ce faire, nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes. Une distribution a priori est définie sur cet espace de modèles, pour obtenir ainsi un critère sans paramètres permettant d'évaluer la pertinence des variables construites. Des expérimentations préliminaires montrent la pertinence de l'approche.
**** *annee_2013  *numText_201
La croissance des informations disponibles sur le web nécessite des outils de recherche de plus en plus performants permettant de répondre efficacement aux besoins des utilisateurs. Dans ce contexte, l'utilisation des ontologies présente des atouts importants. Cependant, la construction manuelle d'ontologies est très coûteuse, ceci a poussé à proposer des approches permettant d'automatiser cette construction. Cet article présente un système de recherche d'information hybride basée sur le Raisonnement à Partir de Cas (RàPC) et la composition d'ontologies. Ce système vise à combiner la construction automatique d'ontologies modulaires et le RàPC, qui a pour but d'améliorer les résultats de recherche d'information (RI). Des expérimentations ont été menées et les résultats obtenus montrent une amélioration de la précision dans le cas d'une recherche d'information sur le Web.
**** *annee_2013  *numText_202
Dans cet article, nous abordons le problème de classification non supervisée sous contraintes fondé sur la programmation par contraintes (PPC). Nous considérons comme critère d'optimisation la minimisation du diamètre maximal des clusters. Nous proposons un modèle pour cette tâche en PPC et nous montrons aussi l'importance des stratégies de recherche pour améliorer son efficacité. Notre modèle basé sur la distance entre les objets permet de traiter des données qualitatives et quantitatives. Des contraintes supplémentaires sur les clusters et les instances peuvent directement être ajoutées. Des expériences sur des ensembles de données classiques montrent l'intérêt de notre approche.
**** *annee_2013  *numText_203
Les méthodes dédiées à l'extraction automatique de thématiques sont issues de domaines variés : linguistique computationnelle, TAL, algèbre linéaire, statistique, etc. A ces méthodes spécifiques, peuvent s'ajouter des méthodes adaptées d'autres domaines, notamment de l'apprentissage automatique non supervisé. Les résultats produits par l'ensemble de ces méthodes prennent des formes hétérogènes : partitions de documents, distributions de probabilités sur les mots, matrices. Cela pose clairement un problème pour les comparer de manière uniforme. Dans cet article, nous proposons une nouvelle mesure de qualité, intitulée Vraisemblance Généralisée, pour permettre une évaluation et ainsi la comparaison de différentes méthodes d'extraction de thématiques. Les résultats, obtenus sur un corpus de documents Web autour des élections présidentielles françaises de 2012, ainsi que sur le corpus Associated Press, montrent la pertinence de la mesure proposée.
**** *annee_2013  *numText_205
Les cartes cognitives sont un modèle graphique représentant des influences entre des concepts. Malgré le fait qu'une carte cognitive soit relativement simple à construire, certaines influences peuvent se contredire l'une l'autre. Cet article propose différents critères pour valider une carte cognitive, c'est-à dire indiquer si la carte contient ou non des contradictions. Nous distinguons deux types de critères : les critères de vérification qui valident une carte cognitive en déterminant sa cohérence interne et les critères de test qui valident une carte à partir d'un ensemble de contraintes choisies par le concepteur.
**** *annee_2013  *numText_206
La classification non supervisée (clustering) évolutive surpasse généralement par celle statique en produisant des groupes de données (clusters) qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Dans ce travail, nous présentons un cadre différent pour le clustering évolutif d'une manière incrémentale par un suivi précis des variables de proximité temporelles entre les objets suivis par un clustering statique ordinaire.
**** *annee_2013  *numText_207
Dans cet article, nous nous intéressons à la gestion d'expériences générées au sein des processus de résolution mixte (individuelle et/ou collective) de problèmes afin d'assister la capitalisation et le partage des connaissances dans les environnements collaboratifs. Dans ce contexte, nous proposons un cadre ontologique générique par rapport au domaine dédié à la modélisation formelle et consensuelle de ces expériences en adoptant une architecture multicouche basée sur quatre strates. La première strate est basée sur la spécialisation d'ontologies fondationnelles. La deuxième strate est basée sur la conception de trois patrons conceptuels ontologiques (PCO) noyaux (le PCO organisationnel, le PCO téléologique et le PCO argumentatif modélisant respectivement les acteurs, le problème et les solutions proposées). La troisième strate est basée sur la spécialisation des PCO noyaux dans un domaine particulier et la dernière strate est basée sur l'instanciation du modèle ontologique de domaine pour la représentation d'une situation du monde réel.
**** *annee_2013  *numText_208
Dans cet article, nous proposons un cadre visant à automatiser la construction de variables pour l'apprentissage supervisé, en particulier dans le cadre multi-tables. La connaissance du domaine est spécifiée d'une part en structurant les données en variables, tables et liens entre tables, d'autre part en choisissant des règles de construction de variables. L'espace de construction de variables ainsi défini est potentiellement infini, ce qui pose des problèmes d'exploration combinatoire et de sur-apprentissage. Nous introduisons une distribution de probabilité a priori sur l'espace des variables constructibles, ainsi qu'un algorithme performant de tirage d'échantillons dans cette distribution. Des expérimentations intensives montrent que l'approche est robuste et performante.
**** *annee_2013  *numText_209
Le calcul de similarité entre les séquences est d'une extrême importance dans de nombreuses approches d'explorations de données. Il existe une multitude de mesures de similarités de séquences dans la littérature. Or, la plupart de ces mesures sont conçues pour des séquences simples, dites séquences d'items. Dans ce travail, nous étudions d'un point de vue purement combinatoire le problème de similarité entre des séquences complexes (i.e., des séquences d'ensembles ou itemsets). Nous présentons de nouveaux résultats afin de compter efficacement toutes les sous-séquences communes à deux séquences. Ces résultats théoriques sont la base d'une mesure de similarité calculée efficacement grâce à une approche de programmation dynamique.
**** *annee_2013  *numText_210
Dans cet article, nous proposons une parallélisation sur CPU et GPU d'une méthode de visualisation radiale à base de points d'intérêt. Nous montrons que cette approche peut visualiser avec des temps très courts des millions de données sur des dizaines de dimensions, et nous étudions l'efficacité de la parallélisation dans différentes configurations.
**** *annee_2012  *numText_212
Dans ce papier, nous proposons une approche de détection de nouveauté fondée sur les opérateurs de projection orthogonale et l'idée de double bootstrap (bi- bootstrap). Notre approche appelée Random Subspace Novelty Detection Filter (RS-NDF), combine une technique de rééchantillonnage et l'idée d'apprentissage d'ensemble. RS-NDF est un ensemble de filtres NDF(Novelty Detection Filter), induits à partir d'échantillons bootstrap des données d'apprentissage, en utilisant une sélection aléatoire des variables pour l'apprentissage des filtres. RS-NDF utilise donc un double bootstrap, c'est à dire un rééchantillonnage avec remise sur les observations et un rééchantillonnage sans remise sur les variables. La prédiction est faite par l'agrégation des prédictions de l'ensemble des filtres. RS-NDF présente généralement une importante amélioration des performances par rapport au modèle de base NDF unique. Grâce à son algorithme d'apprentissage en ligne, l'approche RS-NDF est également en mesure de suivre les changements dans les données au fil du temps. Plusieurs métriques de performance montrent que l'approche proposée est plus efficace,robuste et offre de meilleures performances pour la détection de nouveauté comparée aux autres techniques existantes.
**** *annee_2012  *numText_213
Avec le développement du numérique, des quantités très importantes de documents composés de texte et d'images sont échangés, ce qui nécessite le développement de modèles permettant d'exploiter efficacement ces informations multimédias. Dans le contexte de la recherche d'information, un modèle possible consiste à représenter séparément les informations textuelles et visuelles et à combiner linéairement les scores issus de chaque représentation. Cette approche nécessite le paramétrage de poids afin d'équilibrer la contribution de chaque modalité. Le but de cet article est de présenter une nouvelle méthode permettant d'apprendre ces poids, basée sur l'analyse linéaire discriminante de Fisher(ALD). Des expérimentations réalisées sur la collection Image Clef montrent que l'apprentissage des poids grâce à l'ALD est pertinent et que la combinaison des scores correspondante améliore significativement les résultats par rapport à l'utilisation d'une seule modalité.
**** *annee_2012  *numText_215
Le biclustering de données numériques est devenu depuis le début des années 2000 une tâche importante d'analyse de données, particulièrement pour l’étude de données biologiques d'expression de gènes. Un bicluster représente une association forte entre un ensemble d'objets et un ensemble d'attributs dans une table de données numériques. Les biclusters de valeurs similaires peuvent être vus comme des sous-tables maximales de valeurs proches. Seules quelques méthodes se sont penchées sur une extraction complète (i.e. non heuristique),exacte et non redondante de tels motifs, qui reste toujours un problème difficile,tandis qu'aucun cadre théorique fort ne permet leur caractérisation. Dans le présent article, nous introduisons des liens importants avec l'analyse formelle de concepts. Plus particulièrement, nous montrons de manière originale que l’analyse de concepts triadiques (TCA) propose un cadre mathématique intéressant et puissant pour le biclustering de données numériques. De cette manière, les algorithmes existants de la TCA, qui s'appliquent habituellement à des données binaires,peuvent être utilisés (directement ou après quelques modifications) après un prétraitement des données pour l'extraction désirée.
**** *annee_2012  *numText_216
Nous nous intéressons aux méthodes de classification hiérarchique ou pyramidale, où chaque classe formée correspond à un concept, i.e. une paire (extension,intention), considérant des données décrites par des variables quantitatives à valeurs réelles ou intervalles, ordinales et/ou prenant la forme de distribution de probabilités/fréquences sur un ensemble de catégories. Les concepts sont obtenus par une correspondance de Galois avec généralisation par intervalles, ce qui permet de traiter les données de différents types dans un cadre commun. Une mesure de la généralité d'un concept est alors calculée sous une forme commune pour les différents types de variables. Un exemple illustre la méthode proposée.
**** *annee_2012  *numText_217
L'objectif de ce travail est de prédire l'état de vigilance d'un individu à partir de l'étude de son activité cérébrale (signaux d'électro-encéphalographie EEG). La variable à prédire est binaire (état de vigilance normal ou relaxé).Des EEG de 44 participants dans les deux états (88 enregistrements), ont été recueillis via un casque à 58 électrodes. Après une étape de prétraitement et de validation des données, un critère nommé critère des pentes a été choisi. Des méthodes de classification supervisée usuelles (k plus proches voisins, arbres binaires de décision (CART), forêts aléatoires, PLS et sparse PLS discriminante)ont été appliquées afin de fournir des prédictions de l'état des participants. Le critère utilisé a ensuite été raffiné grâce à un algorithme génétique, ce qui a permis de construire un modèle fiable (taux de bon classement moyen par CART égal à 86.68 ± 1.87%) et de sélectionner une électrode parmi les 58 initiales.
**** *annee_2012  *numText_218
Ce papier présente un algorithme spectrale pour maximiser le critère de la modularité étendu à la classification des données catégorielles. Il met évidence la connexion formelle entre la maximisation de la modularité et la classification spectrale, il présente en particulier le problème de maximisation de la modularité sous forme d'un problème algébrique de maximisation de la trace.Nous développons ensuite un algorithme efficace pour trouver la partition optimale maximisant le critère de modularité. Les résultats expérimentaux montrent l’efficacité de notre approche
**** *annee_2012  *numText_219
Nous proposons dans ce papier un nouvel algorithme de classification non supervisée à base de modèle de mélange topologique pour des données non i.i.d (non independently and identically distributed). Ce nouveau paradigme probabiliste, plonge les cartes topologiques probabilistes dans une formulation sous forme de chaînes de Markov cachées. Dans cette formulation, la génération d'une observation à un instant donné du temps est conditionnée par les états voisins au même instant du temps. Ainsi, une grande proximité impliquera une grande probabilité pour la contribution à la génération. L'approche proposée est évaluée en utilisant des données séquentielles réelles issues des bases de données de l'Institut Nationale de l'Audiovisuel (INA). Les résultats obtenus sont très encourageants et prometteurs.
**** *annee_2012  *numText_220
Cet article présente une carte auto-organisatrice probabiliste pour l'analyse et la classification topologique des données catégorielles. En considérant un modèle de mélanges parcimonieux nous introduisons une nouvelle carte auto-organisatrice (SOM) probabiliste. L'estimation des paramètres de notre modèle est réalisée à l'aide de l'algorithme EM classique. Contrairement à SOM, l'algorithme d'apprentissage proposé optimise une fonction objective. Ces performances ont été évaluées sur des données réelles et les résultats obtenus sont encourageants et prometteurs à la fois pour la classification et pour la modélisation.
**** *annee_2012  *numText_221
L'utilisation de modèles de procédure chirurgicale (Surgical ProcessModel, SPM) a récemment émergé dans le domaine de la conception d'outils d'intervention chirurgicale assistée par ordinateur. Ces modèles, qui sont utilisés pour analyser et évaluer les interventions, représentent des procédures chirurgicales(Surgical Process, SP) qui sont formalisées comme des structures symboliques décrivant une chirurgie à un niveau de granularité donné. Un enjeu important réside dans la définition de métriques permettant la comparaison et l'évaluation de ces procédures. Ainsi, les relations entre ces métriques et des données pré-opératoires permettent de classer les chirurgies pour mettre en lumière des informations sur la procédure elle-même, mais également sur le comportement du chirurgien. Dans ce papier, nous étudions la classification automatique d'un ensemble de procédures chirurgicales en utilisant l'algorithme Dynamic TimeWarping (DTW) pour calculer une mesure de similarité entre procédures chirurgicales.L'utilisation de DTW permet de se concentrer sur les différents types d'activité effectués pendant la procédure, ainsi que sur leur séquencement tout en réduisant les différences temporelles. Des expériences ont été menées sur 24 procédures chirurgicales d'hernie discale lombaire dans le but de discriminer le niveau d'expertise des chirurgiens à partir d'une classification connue. A l'aide d'un algorithme de clustering hiérarchique utilisant DTW nous avons retrouvé deux groupes de chirurgiens présentant des niveaux d'expertise différents (junior et senior).
**** *annee_2012  *numText_222
Dans cet article, il est question de clustering de courbes. Nous proposons une méthode non paramétrique qui segmente les courbes en clusters et discrétise en intervalles les variables continues décrivant les points de la courbe.Le produit cartésien de ces partitions forme une grille de données qui est inférée en utilisant une approche Bayésienne de sélection de modèle ne faisant aucune hypothèse concernant les courbes. Enfin, une technique de post-traitement, visant à réduire le nombre de clusters dans le but d'améliorer l'interprétabilité des clusters, est proposée. Elle consiste à fusionner successivement et de façon optimale les clusters, ce qui revient à réaliser une classification hiérarchique ascendante dont la mesure de dissimilarité correspond à la variation du critère.De manière intéressante, cette mesure est en fait une somme pondérée de divergences de Kullback-Leibler entre les distributions des clusters avant et après fusions. L'intérêt de l'approche dans le cadre de l'analyse exploratoire de données fonctionnelles est illustré par un jeu de données artificiel et réel.
**** *annee_2012  *numText_224
La sélection de caractéristiques est une technique permettant de choisir les caractéristiques les plus pertinentes, celles adaptées à la résolution d'un problème particulier. Les méthodes classiques présentent certains inconvénients.Par exemple, elles peuvent être trop complexes, elles peuvent faire dépendre les caractéristiques sélectionnées du classificateur utilisé, elles risquent de sélectionner des caractéristiques redondantes. Dans le but de limiter ces inconvénients,nous proposons dans cet article une nouvelle méthode rapide de sélection de caractéristiques basée sur la construction et la sélection de classificateurs simples associés à chacune des caractéristiques. Une optimisation par un algorithme génétique est proposée afin de trouver la meilleure combinaison des classificateurs. Différentes méthodes de combinaison sont considérées et adaptées à notre problème. Cette méthode a été appliquée sur différents ensembles de caractéristiques de tailles variées et construite à partir de la base de chiffres manuscrits MNIST. Les résultats obtenus montrent la robustesse de l'approche ainsi que l'efficacité de la méthode. En moyenne, le nombre de caractéristiques sélectionnées a diminué de 69,9% tout en conservant le taux de reconnaissance.
**** *annee_2012  *numText_225
Nous proposons dans cet article une nouvelle approche de classification fondée sur la théorie des fonctions de croyance. Cette méthode repose sur la fusion entre la classification supervisée et la classification non supervisée. En effet, nous sommes face à un problème de manque de données d’apprentissage pour des applications dont les résultats de classification supervisée et non supervisée sont très variables selon les classificateurs employés. Les résultats ainsi obtenus sont par conséquent considérés comme incertains.Notre approche se propose de combiner les résultats des deux types de classification en exploitant leur complémentarité via la théorie des fonctions de croyance.Celle-ci permet de tenir compte de l'aspect d'incertitude et d'imprécision. Après avoir dresser les différentes étapes de notre nouveau schéma de classification,nous détaillons la fusion de classificateurs. Cette nouvelle approche est appliquée sur des données génériques, issues d'une vingtaine de bases de données.Les résultats obtenus ont montré l'efficacité de l'approche proposée.
**** *annee_2012  *numText_227
Les systèmes de surveillance maritime permettent la récupération et la fusion des informations sur les navires (position, vitesse, etc.) à des fins de suivi du trafic maritime sur un dispositif d'affichage. Aujourd'hui, l'identification des risques à partir de ces systèmes est difficilement automatisable compte-tenu de l'expertise à formaliser, du nombre important de navires et de la multiplicité des risques (collision, échouement, etc). De plus, le remplacement périodique des opérateurs de surveillance complique la reconnaissance d'événements anormaux qui sont éparses et parcellaires dans le temps et l'espace. Dans l'objectif de faire évoluer ces systèmes de surveillance maritime, nous proposons dans cet article,une approche originale fondée sur le data mining pour l'extraction de motifsfréquents. Cette approche se focalise sur des règles de prévision et de ciblage pour l'identification automatique des situations induisant ou constituant le cadre des accidents maritimes.
**** *annee_2012  *numText_228
Nous proposons dans ce papier une nouvelle méthode de détection de groupes outliers. Notre mesure nommée GOF (Group Outlier Factor) est estimée par l'apprentissage non-supervisé. Nous l'avons intégré dans l'apprentissage des cartes topologiques. Notre approche est basée sur la densité relative de chaque groupe de données, et fournit simultanément un partitionnement des données et un indicateur quantitatif (GOF) sur la particularité de chaque cluster ou groupe. Les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique.
**** *annee_2012  *numText_229
L'apprentissage non supervisé a classiquement pour objectif la détection de sous-populations homogènes (classes) considérées de manière équivalente sans information a priori sur celles-ci. Le problème étudié dans cet article est quelque peu distinct. On se focalise ici uniquement sur une sous-population d'intérêt que l'on cherche à identifier avec un rappel et une précision optimales.Nous proposons, pour cela, une méthode s'appuyant sur les principes suivants :(1) travailler dans l'espace de représentation fourni par des experts faibles pour cette tâche, (2) confronter ces experts pour détecter des seuils de sélection plus pertinents, et (3) les combiner itérativement afin de converger vers l'expert idéal.Cette méthode est éprouvée et comparée sur des données synthétiques.
**** *annee_2012  *numText_232
Lors de l'analyse de graphes, il est important de connaître leurs propriétés afin de pouvoir par exemple identifier leur structure et les comparer.Une des caractérisations importante de ces graphes repose sur le fait de déterminer s'il s'agit ou non d'un petit monde. Pour ce faire, la valeur du diamètre du graphe est essentielle. Or la mesure du diamètre est pour un très grand graphe, une opération extrêmement longue. Nous proposons un algorithme en deux phases qui permet d'obtenir rapidement une estimation du diamètre d'un graphe avec une proportion d'erreur faible. En réduisant cet algorithme à une seule phase et en acceptant une marge d'erreur plus élevée, nous obtenons une estimation très rapide du diamètre. Nous testons cet algorithme sur deux grands graphes de terrain (plus d'un million de nœuds) et comparons ses performances avec celles d'un algorithme de référence BFS (Breadth-First Search). Les résultats obtenus sont décrits et commentés.
**** *annee_2012  *numText_233
Nous présentons dans cet article une nouvelle approche pour la génération automatique de structures lexicales (ou taxonomies) à partir de textes.Cette tâche est fondée sur l'hypothèse forte selon laquelle l'accumulation de faits statistiques simples sur les usages en corpus permet d'approximer des informations de niveau sémantique sur le lexique. Nous utilisons la prétopologie comme cadre de travail afin de formaliser et de combiner plusieurs hypothèses sur les usages terminologiques et enfin de structurer le lexique sous la forme d’une taxonomie. Nous considérons également le problème de l'évaluation des taxonomies résultantes et proposons un nouvel indice afin de les comparer et de positionner notre approche par rapport à la littérature.
**** *annee_2012  *numText_234
L'analyse de grands réseaux est très étudiée en fouille de données.Toutefois, les approches existantes proposent une analyse soit à un niveau macroscopique(étude des propriétés globales comme la distribution des degrés),soit à un niveau microscopique (extraction de sous-graphes fréquents ou denses).Nous proposons une nouvelle méthode qui effectue une analyse intermédiaire permettant de découvrir des motifs regroupant des propriétés microscopiques et macroscopiques du réseau. Ces motifs capturent des co-variations entre des propriétés numériques relatives aux sommets. Par exemple, un motif mésoscopique dans un réseau de co-auteurs peut être plus le nombre de publications à EGC est important, plus la centralité des sommets correspondants dans le réseau l'est également. Notre contribution est multiple. D'abord, ce travail est le premier à exploiter conjointement des propriétés locales et des propriétés topologiques.De plus, nous produisons de nouvelles avancées dans le domaine de l'extraction de co-variations en revisitant les motifs émergents dans ce contexte. Enfin, nous rapportons une analyse d'un réseau bibliographique réel issu de DBLP.
**** *annee_2012  *numText_235
La découverte de dépendances fonctionnelles (DF) à partir d'une relation existante est une technique importante pour l'analyse de Bases de Données.L'ensemble des DF exactes ou approximatives extraites par les algorithmes existants est valide tant que la relation n'est pas modifiée. Ceci est insuffisant pour des situations réelles où les relations sont constamment mises à jour.Nous proposons une approche incrémentale qui maintient à jour l'ensemble des DF valides, exactes ou approximatives selon une erreur donnée, quand des tuples sont insérés et supprimés. Les résultats expérimentaux indiquent que lors de l'extraction de DF à partir d'une relation continuellement modifiée, les algorithmes existants sont sensiblement dépassés par notre stratégie incrémentale.
**** *annee_2012  *numText_236
Cet article présente FLMin, une nouvelle méthode d'extraction de motifs fréquents dans les réseaux sociaux. Contrairement aux méthodes traditionnelles qui s'intéressent uniquement aux régularités structurelles, l'originalité de notre approche réside dans sa capacité à exploiter la structure et les attributs des noeuds pour extraire des régularités, que nous appelons liens fréquents, dans les liens entre des nœuds partageant des caractéristiques communes.
**** *annee_2012  *numText_237
Lors de l'extraction des séquences, la granularité temporelle est plus ou moins importante selon les besoins des utilisateurs et les contraintes du domaine d'application. Nous proposons un algorithme d'extraction de séquences fréquentes par intervalles à partir de séquences à estampilles temporelles discrètes.Nous intégrons une relaxation des contraintes temporelles en introduisant la définition de séquences temporelles par intervalles (STI). Ces intervalles reflètent une incertitude sur les occurrences précises des événements. Nous formalisons ce nouveau concept en exhibant certaines de ses propriétés et nous menons quelques expériences afin de comparer (qualitativement) nos résultats avec une autre proposition assez proche de la nôtre.
**** *annee_2012  *numText_238
Différentes ressources ontologiques généralistes de très grande taille ont été développées de façon collective et sont aujourd'hui disponibles sur le web. Ainsi l'ontologie YAGO est une énorme base de connaissances décrivant plus de 2 millions d'entités. Afin de tirer parti de ce gigantesque travail collectif,nous montrons comment en extraire des sous-parties thématiquement focalisées pour enrichir une autre ontologie, dite cible, de taille plus limitée mais de domaine centré sur une application particulière 1.
**** *annee_2012  *numText_239
Les technologies de l'information et le succès des services associés(e.g., blogs, forums,...) ont ouvert la voie à un mode d'expression massive d'opinions sur les sujets les plus variés. Récemment, de nouvelles techniques de détection automatique d'opinions (opinion mining) ont fait leur apparition et via des analyses statistiques des avis exprimés, tendent à dégager une tendance globale des opinions exprimées par les internautes. Néanmoins une analyse plus fine de celle-ci montre que les arguments avancés par les internautes relèvent de critères de jugement distincts. Ici, un film sera décrié pour un scénario décousu,là il sera encensé pour une bande son époustouflante. Dans cet article, nous proposons,après avoir caractérisé automatiquement des critères dans un document,d'en extraire l'opinion relative. A partir d'un ensemble restreint de mots clés d'opinions, notre approche construit automatiquement une base d'apprentissage de documents issus du web et en déduit un lexique de mots ou d'expressions d'opinions spécifiques au domaine d'application. Des expériences menées sur des jeux de données réelles illustrent l'efficacité de l'approche.
**** *annee_2012  *numText_240
Indexer une vidéo consiste à rattacher un ou plusieurs concepts à des segments de cette vidéo, un concept étant défini comme une représentation intellectuelle d’une idée abstraite. L'indexation automatique se base sur l’extraction automatique de caractéristiques fournies par un système de traitement d'images.Cependant, il est nécessaire de définir les index ou concepts. Pour cela il faut définir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépare les caractéristiques extraites sur lesquelles se base l'indexation automatique et les concepts est appelé fossé sémantique qui est le manque de concordance entre les informations que les machines peuvent extraire depuis les documents numériques et les interprétations que les humaines en font. La définition d'un concept peut être faite automatiquement si l'on dispose d'une base d'apprentissage liée au concept. Dans ce cas, il est possible d'apprendre le concept de manière statistique. Mais la construction de cette base d'apprentissage nécessite de faire intervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer sur ses connaissances pour extraire des segments vidéo représentatifs du concept que l'on souhaite définir. On peut lui demander d'indexer manuellement la base d'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article,nous proposons une méthode qui permet d'extraire l'expertise pour que l'implication de l'expert soit la plus simple et la plus limitée possible.
**** *annee_2012  *numText_242
Nous proposons une méthode utilisant les histogrammes de gradient orienté (HOG) et les séparateurs à vaste marge (SVM) pour la détection de personnes à partir d'images prises depuis un petit robot mobile autonome. Les travaux antérieurs réalisés dans le domaine de la détection d'êtres humains à partir d'images ne peuvent pas être employés pour ce type d'application car ils supposent que les images sont prises à partir d'une position élevée (au moins la hauteur d'un petit enfant) alors que la taille de notre robot n'est que de 15cm. Nous employons à la fois les HOG et les SVM car cette combinaison de méthodes est reconnue comme étant celle ayant le plus de succès pour la détection de personnes. Pour traiter une grande variété de formes humaines, principalement en raison de la distance existant entre les personnes et le robot, nous avons développé une nouvelle méthode de prédiction à deux étapes utilisant deux types de classificateurs SVM qui reposent sur une estimation de la distance. L'estimation est basée sur une proportion de pixels de couleur de peau dans l'image, ce qui nous permet de clairement séparer notre problème de la détection de corps entier et de celle de corps partiel. Les essais réalisés dans un bureau ont montré des résultats prometteurs de notre méthode avec une valeur de F de 0,93.
**** *annee_2012  *numText_244
Lorsqu'on désire contacter un client pour lui proposer un produit on calcule au préalable la probabilité qu'il achètera ce produit. Cette probabilité est calculée à l'aide d'un modèle prédictif pour un ensemble de clients. Le service marketing contacte ensuite ceux ayant la plus forte probabilité d'acheter le produit. En parallèle, et avant le contact commercial, il peut être intéressant de réaliser une typologie des clients qui seront contactés. L'idée étant de proposer des campagnes différenciées par groupe de clients. Cet article montre comment il est possible de contraindre la typologie, réalisée à l'aide des k-moyennes, à respecter la proximité des clients vis-à-vis de leur score d'appétence.
**** *annee_2012  *numText_245
Cet article étudie la faisabilité et l'intérêt de l'extraction de règles de dépendance entre ensembles de variables multivaluées en comparaison du problème bien connu de l'extraction des règles d'association fréquentes. Une règle de dépendance correspond à une dépendance fonctionnelle approximative caractérisée principalement par l'entropie conditionnelle associée. L'article montre comment établir une analogie formelle entre les deux familles de règles et comment adapter à l'aide de cette analogie l'algorithme « Eclat » afin d'extraire d'un jeu de données les règles de dépendance dites bien définies. Une étude expérimentale conclut sur les forces et inconvénients des règles de dépendance bien définies vis-à-vis des règles d'association fréquentes
**** *annee_2012  *numText_246
La littérature s'est beaucoup intéressée à l'extraction de règles classiques(ou positives) et peu à l'extraction des règles négatives en raison essentiellement d'une part, du coût de calculs et d'autre part, du nombre prohibitif de règles redondantes et inintéressantes extraites. La démarche que nous avons retenue est de dégager les règles négatives lors de l'extraction des règles positives,et pour cela, nous recherchons les règles négatives que l'on peut inférer ou pas à partir de la pertinence d'une règle positive. Ces différentes inférences vont être formalisées par un ensemble de méta-règles.
**** *annee_2012  *numText_248
L'automatisation et la supervision des systèmes pervasifs est à l'heure actuelle principalement basée sur l'utilisation massive de capteurs distribués dans l'environnement. Dans cet article, nous proposons un modèle de supervision d'interactions basé sur l'analyse sémantique des logs domotiques (commandes émises par l'utilisateur), visant à limiter l'utilisation de ces capteurs :le principe est d'utiliser des outils d'inférences avancés, afin de déduire les informations habituellement captées. Pour cela, une ontologie, automatiquement dérivée d'un processus dirigé par les modèles, définit les interactions utilisateur système.L'utilisation d'un système de règles permet ensuite d'inférer des informations sur la localisation et l'intention de l'utilisateur, dans le but de réaliser du monitoring et de proposer des services domotiques adaptés.
**** *annee_2012  *numText_250
Le prétraitement des variables numériques dans le contexte de la fouille de données multi-tables diffère de celui des données classiques individu variable.La difficulté vient principalement des relations un-à-plusieurs où les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires. Dans cet article, nous décrivons une méthode de discrétisation des variables numériques situées dans des tables secondaires. Nous proposons un critère qui évalue les discrétisations candidates pour ce type de variables. Nous décrivons un algorithme d'optimisation simple qui permet d'obtenir la meilleure discrétisation en intervalles de fréquence égale pour le critère proposé. L'idée est de projeter dans la table cible l'information contenue dans chaque variable secondaire à l'aide d'un vecteur d'attributs (un attribut par intervalle de discrétisation). Chaque attribut représente le nombre de valeurs de la variable secondaire appartenant à l'intervalle correspondant. Ces attributs d'effectifs sont conjointement partitionnés à l'aide de modèles en grille de données afin d'obtenir une meilleure séparation des valeurs de la classe. Des expérimentations sur des jeux de données réelles et artificielles révèlent que l'approche de discrétisation permet de découvrir des variables secondaires pertinentes.
**** *annee_2012  *numText_251
Concevoir une carte géographique, plus particulièrement sa légende,exige des compétences spécifiques. L'objectif de ce papier est de présenter une base de connaissances destinée à aider tout utilisateur à concevoir une ou plusieurs légendes adaptées à son besoin et conformes aux règles de cartographie.La base de connaissances est formée d'une ontologie de la cartographie nommée OntoCarto, d'un corpus de règles : OntoCartoRules et d'un moteur de raisonnement: Corese. Dans ce papier, chaque demande de conception de légende est vue comme une instanciation particulière de l'ontologie, associée à une sélection de règles pertinentes dans le corpus de règles, sur laquelle Corese va raisonner pour construire des légendes adaptées à la configuration spécifique traitée. La conception de la légende s'appuie sur la définition de deux hiérarchies d'objets géographiques et cartographiques. Les principes de fonctionnement de Corese sont présentés. Un prototype a été implémenté et des extraits des résultats sont montrés.
**** *annee_2012  *numText_252
Dans cet article, nous nous intéressons à la recherche agrégée dans des documents XML. Pour cela, nous proposons un modèle basé sur les réseaux bayésiens. Les relations de dépendances entre requête-termes d'indexation et termes d'indexation-éléments sont quantifiées par des mesures de probabilité.Dans ce modèle, la requête de l'utilisateur déclenche un processus de propagation pour trouver des éléments. Ainsi, au lieu de récupérer une liste des éléments qui sont susceptibles de répondre à la requête, notre objectif est d'agréger dans un agrégat des éléments pertinents, non-redondants et complémentaires. Nous avons évalué notre approche dans le cadre de la compagne d'évaluation INEX2009 et avons présenté quelques résultats expérimentaux mettant en évidence l'impact de l'agrégation de tels éléments.
**** *annee_2012  *numText_254
Dans cet article nous proposons un nouvel algorithme pour la réorganisation hiérarchique des cubes OLAP (On-Line Analytical Processing) ayant pour objectif d'améliorer leur visualisation. Cet algorithme se caractérise par le fait qu'il peut traiter des dimensions organisées hiérarchiquement et optimiser conjointement les dimensions du cube, contrairement aux autres approches. Il utilise un algorithme génétique qui réorganise des arbres n-aires quelconques. Il a été intégré dans une interface OLAP puis testé en comparaison avec d'autres approches de réorganisation, et fournit des résultats très positifs. A ce titre,nous avons également généralisé l'algorithme heuristique classique BEA (bondenergy algorithm) au cas de hiérarchies OLAP. Enfin, notre approche a été évaluée par des utilisateurs et les résultats soulignent l'intérêt de la réorganisation dans des exemples de tâches à résoudre pour OLAP.
**** *annee_2012  *numText_256
Le but principal des systèmes de recherche d'informations (SRI) classiques est de retrouver dans un corpus de documents l'information considérée comme pertinente pour une requête utilisateur. Cette pertinence est souvent liée à la fréquence d'apparition des termes dans le texte par rapport au corpus sans tenir compte du contexte de la recherche. Partant de ce constat, nous proposons dans cet article une approche pour la recherche d'information contextuelle par segmentation thématique de documents (RICSH). Cette approche s'appuie sur la méthode de pondération tf-idf que nous avons adaptée dans notre cas pour indexer le corpus. Cette adaptation se situe au niveau de l'importance du terme et de son pouvoir de discrimination par rapport aux fragments de textes et non au corpus. Ces fragments sont obtenus grâce à un processus d'identification des unités thématiques les plus pertinentes pour chaque document.
**** *annee_2012  *numText_257
Cet article analyse la consistance asymptotique des modèles en grille appliqués à l'estimation de densité jointe de deux variables catégorielles. Les modèles en grille considèrent un partitionnement des valeurs de chacune des variables,le produit Cartésien des partitions formant une grille dont les cellules permettent de résumer la table de contingence des deux variables. Le meilleur modèle de co-partitionnement est recherché au moyen d'une approche MAP(maximum a posteriori), présentant la particularité peu orthodoxe d'exploiter une famille de modèles et une distribution a priori de ces modèles qui dépendent des données. Ces modèles sont par nature des modèles de l'échantillon d'apprentissage,et non de la distribution sous-jacente. Nous démontrons la consistance de l'approche, qui se comporte comme un estimateur universel de densité jointe convergeant asymptotiquement vers la vraie distribution jointe.
**** *annee_2012  *numText_259
L'informatique juridique, est un domaine en évolution constante. Le contexte général de notre travail est l'élaboration d'un système de recherche de jurisprudence tunisienne en langue arabe. L'objectif opérationnel de ce système est de fournir une aide aux juristes pour résoudre une situation juridique donnée en mettant à leur disposition une collection de situations similaires ce qui améliorera leur raisonnement futur. Une ontologie du domaine juridique construite à partir des documents des décisions juridiques est nécessaire dans notre contexte.Cette ontologie a pour but : (i) la structuration des décisions, (ii)la formulation des requêtes d'interrogation de la base des décisions, et (iii) la recherche des décisions. Dans cet article, nous présentons l'architecture de notre système de recherche de jurisprudence. Nous nous focalisons sur l'ontologie du domaine de jurisprudence que nous avons élaborée, ainsi que sur le module de structuration des décisions.
**** *annee_2012  *numText_260
Le projet ANR ISICIL 1 mixe les nouvelles applications virales du web avec des représentations formelles et des processus d'entreprise pour les intégrer dans les pratiques de veille en entreprise. Les outils développés s'appuient sur les interfaces avancées des applications du web 2.0 (blog, wiki, social bookmarking,extensions de navigateurs) pour les interactions et sur les technologies du web sémantique pour l'interopérabilité et le traitement de l'information. Le présent article décrit plus précisément le wiki sémantique développé dans le cadre de ce projet et son intégration au cœur du framework ISICIL.
**** *annee_2012  *numText_261
Plusieurs méthodes ont été développées ces dernières années pour détecter,dans un réseau social, les membres qualifiés, selon les auteurs, d'influenceurs,de médiateurs, d'ambassadeurs ou encore d'experts. Dans cet article, nous proposons un nouveau cadre méthodologique permettant d'identifier des diffuseurs dans le contexte où seule l'information sur l'appartenance des membres du réseau à des communautés est disponible. Ce cadre, basé sur une représentation du réseau sous forme d'hypergraphe, nous a permis de formaliser la notion de diffuseur et d'introduire l'algorithme TMD-MINER, dédié à la détection des diffuseurs et basé sur les itemsets essentiels.
**** *annee_2012  *numText_263
Dans ce papier, nous proposons une étude sur l'utilisation de l'apprentissage topologique pondéré et les méthodes de factorisation matricielle pour transformer l'espace de représentation d'un jeu de données sparse afin d'augmenter la qualité de l'apprentissage, et de l'adapter au cas de l'apprentissage par transfert. La factorisation matricielle nous permet de trouver des variables latentes et l'apprentissage topologique pondéré est utilisé pour détecter les plus pertinentes parmi celles-ci. La représentation de nouvelles données est basée sur leurs projections sur le modèle topologique pondéré.Pour l'apprentissage par transfert, nous proposons une nouvelle méthode où la représentation des données est faite de la même manière que dans la première phase, mais en utilisant un modèle topologique élagué.Les expérimentations sont présentées dans le cadre d'un Challenge International où nous avons obtenu des résultats prometteurs (5ieme rang de la compétition internationale).
**** *annee_2012  *numText_264
classification automatique (De Carvalho et al., 2012) capable de partitionner des objets en prenant en compte de manière simultanée plusieurs matrices de dissimilarité qui les décrivent. Ces matrices peuvent avoir été générées en utilisant différents ensembles de variables et de fonctions de dissimilarité.Cette méthode, basée sur l'algorithme de nuées dynamiques est conçu pour fournir une partition et un prototype pour chaque classe tout en découvrant une pondération pertinente pour chaque matrice de dissimilarité en optimisant un critère d'adéquation entre les classes et leurs représentants. Ces pondérations changent à chaque itération de l'algorithme et sont différentes pour chacune des classes.Nous présentons aussi plusieurs outils d'aide à l'interprétation des groupes et de la partition fournie par cette nouvelle méthode. Deux exemples illustrent l’intérêt de la méthode. Le premier utilise des données concernant des chiffres manuscrits(0 à 9) numérisés en images binaires provenant de l'UCI. Le second utilise un ensemble de rapports dont nous connaissons une classification experte donnée à priori.
**** *annee_2012  *numText_265
Nous nous intéressons dans cet article au problème de l'automatisation du processus de choix et de paramétrage des visualisations en fouille visuelle de données. Pour résoudre ce problème, nous avons développé un assistant utilisateur qui effectue deux étapes : à partir des objectifs annoncés par l'utilisateur et des caractéristiques de ses données, le système commence par proposer à l'utilisateur différents appariements entre la base de données à visualiser et les visualisations qu'il gère. Ces appariements sont générés par une heuristique utilisant une base de connaissances sur les visualisations et la perception visuelle. Ensuite, afin d'affiner les différents paramétrages suggérés par le système, nous utilisons un algorithme génétique interactif qui permet aux utilisateurs d'évaluer et d'ajuster visuellement ces paramétrages. Nous présentons une évaluation utilisateur qui montre l'intérêt de notre système pour deux tâches.
**** *annee_2012  *numText_266
La plupart des processus de classification d'images comportent trois principales étapes : l'extraction de descripteurs de bas niveaux, la création d'un vocabulaire visuel par quantification et l'apprentissage à l'aide d'un algorithme de classification (eg.SVM). De nombreux problèmes se posent pour le passage à l'échelle comme avec l'ensemble de données ImageNet contenant 14 millions d'images et 21,841 classes. La complexité concerne le temps d'exécution de chaque tâche et les besoins en mémoire et disque (eg. le stockage des SIFTs nécessite 11 To). Nous présentons une version parallèle de LibSVM pour traiter de grands ensembles de données dans un temps raisonnable. De plus, il y a beaucoup de perte d'information lors de la phase de quantification et les mots visuels obtenus ne sont pas assez discriminants pour de grands ensembles d'images. Nous proposons d'utiliser plusieurs descripteurs simultanément pour améliorer la précision de la classification sur de grands ensembles d'images. Nous présentons nos premiers résultats sur les 10 plus grandes classes (24,817 images) d'ImageNet.
**** *annee_2012  *numText_267
Aujourd'hui, les réseaux sociaux en ligne sont devenus des outils très puissants de propagation de l'information. Ils favorisent la diffusion rapide à grande échelle de contenu et les conséquences d'une information inexacte voire fausse peuvent alors prendre une ampleur considérable. Par conséquent il devient indispensable de proposer des moyens d'analyser le phénomène de diffusion de l'information dans ces réseaux. De nombreuses études récentes ont traité de la modélisation du processus de diffusion de l'information, essentiellement d'un point de vue topologique et dans une perspective théorique, mais les facteurs impliqués sont encore méconnus. Nous proposons ici une solution pratique dont l'objectif est de prédire la dynamique temporelle de la diffusion au sein de Twitter, basée sur des techniques d'apprentissage automatique. Notre approche repose sur l'inférence de probabilités de diffusion tirées d'une analyse multidimensionnelle des comportements individuels. Les expérimentations menées montrent l'intérêt de la modélisation proposée.
**** *annee_2012  *numText_268
La plupart des distances entre histogrammes sont définies pour comparer des histogrammes ordonnés (dont les entités représentées sont totalement ordonnées) ou des histogrammes nominaux (dont les entités représentées ne peuvent pas être comparées). Cependant, il n'existe aucune distance qui permette de comparer des histogrammes nominaux dans lesquels il est possible de quantifier des valeurs de proximité sémantique entre les entités considérées. Cet article propose une nouvelle distance permettant de pallier ce problème. Dans un premier temps, une hiérarchie d'histogrammes, obtenue par le biais d'une fusion progressive des entités considérées (prenant en compte leurs proximités sémantiques),est construite. Pour chaque étage de cette hiérarchie, une distance standard de comparaison d'histogrammes nominaux est calculée. Finalement, pour obtenir la distance proposée, ces différentes distances sont fusionnées en prenant en compte la cohérence sémantique associée aux niveaux de chaque étage de la hiérarchie. Cette distance a été validée dans le cadre de la classification de données géographiques. Les résultats obtenus sont encourageants et montrent ainsi l'intérêt et l'utilité de cette dernière pour des processus de fouille de données.
**** *annee_2012  *numText_270
Les services de personnalisation du Web 2.0 reposent sur l'exploitation de modèles utilisateurs. Schématiquement, plus la quantité d'informations sur les utilisateurs est grande, meilleures sont la modélisation et la qualité du service.En pratique, nombre de services rencontrent un problème de manque d'informations sur les utilisateurs. Dans cet article, nous y répondons par médiation inter-domaines de modèles utilisateurs, c'est-à-dire la complétion de modèles en exploitant des données d'un autre domaine. La médiation que nous proposons repose sur un transfert d'informations inter-domaines. Ce transfert consiste en l'utilisation de couples invariants ou très corrélés pouvant être des couples de ressources ou de descripteurs sémantiques, identifiés après enrichissement sémantique des modèles. Nous montrons que le transfert sous forme de couple de ressources permet une complétion de qualité et que l'exploitation de descripteurs sémantiques augmente la couverture à qualité égale. Enrichir sémantiquement est donc bénéfique pour le transfert inter-domaines.
**** *annee_2012  *numText_271
De nombreux algorithmes de fragmentation de graphes fonctionnent par agrégations ou divisions successives de sous-graphes menant à une décomposition hiérarchique du réseau étudié. Une question importante dans ce domaine est de savoir si cette hiérarchie reflète la structure du réseau ou si elle n'est qu'un artifice lié au déroulement de la procédure. Nous proposons un moyen de valider et, au besoin, d'optimiser la décomposition multi-échelle produite parce type de méthode. On applique notre approche sur l'algorithme proposé par Blondel et al. (2008) basé sur la maximisation de la modularité. Dans ce cadre,une généralisation de cette mesure de qualité au cas multi-niveaux est introduite.Nous testons notre méthode sur des graphes aléatoires ainsi que sur des exemples réels issus de divers domaines.
**** *annee_2012  *numText_272
L'un des objectifs d'Observox est de traiter et gérer l'imprécision des données agronomiques tant spatialement (parcelles agricoles) et quantitativement(quantités de produits disséminées) et de toujours associer une évaluation de la qualité aux données. Aussi, nous avons choisi le cadre théorique des ensembles flous. A partir d'un modèle conceptuel gérant l'imperfection, nous construisons une base de données gérant des entités spatio-temporelles imprécises appelées « entités agronomiques floues ». Cependant, ce choix de représentation rend possible le chevauchement des composantes spatiales entre entités.Dans ce cas, nous propageons l'imprécision du spatial vers le quantitatif à l'aide d'un opérateur de caractère additif qui prend en compte à la fois l'information spatiale et quantitative, et qui fournit une information quantitative locale et floue. Le système ainsi construit nous permet d'obtenir une représentation floue des quantités de produits phytosanitaires disséminés à chaque endroit du territoire étudié.
**** *annee_2012  *numText_273
Ces dernières années, l'augmentation de la quantité d'informations spatio-temporelles stockées dans les bases de données a fait naître de nouveaux besoins, notamment en matière de gestion des risques naturels, sanitaires ou anthropiques(p. ex. compréhension de la dynamique d'une épidémie de Dengue).Dans cet article, nous définissons un cadre théorique pour l'extraction de motifs spatio-séquentiels, séquences de motifs spatiaux représentant l'évolution dans le temps d'une localisation et de son voisinage. Nous proposons un algorithme d'extraction efficace qui effectue un parcours en profondeur en s'appuyant sur des projections successives de la base de données. Nous introduisons également une mesure d'intérêt adaptée aux aspects spatio-temporels de ces motifs. Les expérimentations réalisées sur des jeux de données réels soulignent la pertinence de l'approche proposée par rapport aux méthodes de la littérature.
**** *annee_2012  *numText_274
Dans de nombreux domaines (e.g., fouille de données, entrepôts de données), l'existence de hiérarchies sur certains attributs peut être extrêmement utile dans le processus analytique. Toutefois, cette connaissance n'est pas toujours disponible ou adaptée. Il est alors nécessaire de disposer d'un processus de découverte automatique pour palier ce problème. Dans cet article, nous combinons et adaptons des techniques issues de la théorie de l'information et du clustering pour proposer une technique orientée données de construction automatique de taxonomies. Les deux principaux avantages d'une telle approche sont son caractère totalement non-supervisé et l'absence de paramètre utilisateur à spécifier. Afin de valider notre approche, nous l'avons appliquée sur des données réelles et avons conduit plusieurs types d'expérimentation. D'abord,les hiérarchies obtenues ont été expertisées pour en examiner le pouvoir informatif.Ensuite, nous avons évalué l'apport de ces taxonomies comme support à des tâches de fouille de données nécessitant une définition hiérarchique des valeurs d'attributs : l'extraction de séquences fréquentes multidimensionnelles et multi-niveaux ainsi que la construction de résumés de tables relationnelles. Les résultats obtenus permettent de conclure quant à l'intérêt de notre approche
**** *annee_2012  *numText_275
Depuis son apparition au sein du W3C, la définition de la ressource Web n'a cessé d'évoluer au delà du simple document. Lieu, service, concept d'ontologie, représentation d'un objet réel ou non, la ressource web est complexe et il nous a semblé que les outils à disposition des internautes pour sa manipulation,comme les bookmarks par exemple, n'exploitaient pas pleinement ces nouvelles dimensions. Dans cet article, nous présenterons le modèle Webmarks qui permet de préciser l'objet du marquage, la ressource, mais également l'intérêt de l'auteur de la marque. L'implémentation de ce modèle au sein du projet ISICIL sera également présentée et nous discuterons de son apport en comparaison des technologies existantes
**** *annee_2011  *numText_276
Dans les industries à risque, comme le nucléaire, les connaissances liées au savoir et à l'expérience participent à la maîtrise des activités. Elles sont explicites, formalisables dans des documents, ou tacites, expression du savoir faire moins souvent prise en compte. AREVA développe la méthode @KRex pour valoriser le retour d'expérience existant, créer une dynamique d'extraction et de capitalisation des connaissances, faciliter leur partage et leur enrichissement. Cette communication décrit le protocole expérimental de construction des connaissances explicites et tacites du métier sécurité nucléaire.
**** *annee_2011  *numText_278
Les structures lexico-sémantiques jouent un rôle essentiel dans les processus de fouille de textes. En codant les relations sémantiques entre concepts du discours elles apportent une connaissance stratégiques pour enrichir les capacités de raisonnement. Le développement de telles structures étant fortement limité du fait des efforts nécessaires à leur construction, nous proposons un nouveau formalisme d'acquisition automatique d'ontologies terminologiques à partir de textes. Nous utilisons pour cela une formalisation prétopologique de l'espace des termes sur laquelle s'appuie un modèle générique de structuration. Nous présentons une étude empirique préliminaire rendant compte du potentiel de ce modèle en terme d'extraction de connaissances.
**** *annee_2011  *numText_279
Les développements récents en tarification de l'assurance non-vie se concentrent majoritairement sur la maîtrise et l'amélioration des Modèles Linéaires Généralisés. Performants, ces modèles imposent cependant à la fois des contraintes sur la structure du risque modélisé et sur les interactions entre variables explicatives du risque. Ces restrictions peuvent conduire, dans certaines sous-populations d'assurés, à une estimation biaisée de la prime d'assurance. Les arbres de régression permettent de s'affranchir de ces contraintes et, de plus, augmentent la lisibilité des résultats de la tarification. Nous présentons une modification de l'algorithme CART pour prendre en compte les spécificités des données d'assurance non-vie. Nous comparons alors notre proposition aux modèles linéaires généralisés sur un portefeuille réel de véhicules. Notre proposition réduit les mesures d'erreur entre le risque mesuré et le risque modélisé, et permet ainsi une meilleure tarification.
**** *annee_2011  *numText_280
Dans les années à venir, plusieurs millions de compteurs électriques communicants seront déployés sur l'ensemble du territoire français. Afin d'assurer la fiabilité d'un réseau de cette envergure nous proposons une topologie de communication multi-chemins qui repose sur la duplication des données transmises. Toute exploitation des données collectées doit alors tenir compte de la présence d'éléments dupliqués. Dans cet article, nous proposons une nouvelle méthode permettant de calculer en ligne des consommations électriques agrégées (agrégation spatiale). L'idée est d'adapter l'algorithme probabiliste Summation sketch de Considine et al. au contexte des compteurs communicants. Cette approche a l'avantage d'être insensible à la duplication et permet de profiter de la structure massivement distribuée du réseau de communication des futurs compteurs électriques. L'expérimentation de cette méthode sur des données réelles montre qu'elle donne une bonne précision sur l'estimation des consommations agrégées. Cette approche est aussi complétée par une méthode basée sur la théorie des sondages : On obtient une meilleure réactivité de l'estimateur avec rapidement et donc sur des données significativement partielles une erreur inférieure à 2.5%
**** *annee_2011  *numText_281
Cet article traite de l'analyse visuelle de réseaux sociaux pour la détection de comportements suspects à partir de données de communications fournies à des enquêteurs suivant deux procédures : l'interception légale et la rétention de données. Nous proposons les contributions suivantes : (i) un modèle de données et un ensemble d'opérateurs pour interroger ces données dans le but d'extraire des comportements suspects et (ii) une représentation visuelle conviviale pour une navigation simplifiée dans les données de communication accompagnée avec une implémentation.
**** *annee_2011  *numText_283
Plusieurs méthodologies et outils de construction automatique des ontologies à partir de ressources textuelles ont été proposés ces dernières années. Dans cet article nous analysons quatre approches en les comparant à une approche de référence - Methontology. Dans leur sélection nous avons privilégié celles qui couvrent l'ensemble des étapes du processus de construction d'ontologies. Puis nous analysons et comparons la portée, les limites et les performances des implémentations logicielles associées aux approches analysées. Ces outils ont été testés sur un corpus de ressources textuelles, et nous avons comparé leurs résultats à ceux obtenus manuellement.
**** *annee_2011  *numText_284
Nous étudions ici le comportement de deux types d'indices probabilistes discriminants en présence de données dont le volume va en croissant. A cet égard, un modèle spécifique de croissance de la taille des données et de liaison entre variables est mis en oeuvre et celui-ci va permettre de déterminer le comportement limite des différents indices quel que soit le niveau de liaison entre la prémisse et la conclusion de la règle donnée. La clarté des résultats obtenus nous conduit à en chercher l'explication formelle. L'expérimentation a été effectuée avec la base de données UCI Wages.
**** *annee_2011  *numText_285
Nous proposons un outil graphique interactif qui permet de visualiser et d'extraire des connaissances à partir des résultats de l'Analyse Factorielle des Correspondances (AFC) sur les images. L'AFC est une technique descriptive développée pour analyser des tableaux de contingence. L'AFC est originellement utilisée dans l'Analyse des Données Textuelles (ADT) où le corpus est représenté par un tableau de contingence croisant des documents et des mots. Dans la fouille d'images, nous définissons d'abord les « mots visuels » dans les images (analogues aux mots textuels). Ces mots visuels sont construits à partir des descripteurs locaux SIFT (Scale Invariant Feature Transform) dans l'image. Ensuite, nous appliquons l'AFC sur le tableau de contingence obtenu. Notre outil (appelé HCAViz) analyse ce tableau de contingence de façon récursive et aide l'utilisateur à interpréter et interagir avec les résultats de l'AFC. D'abord, les résultats de la première AFC sur les images sont visualisés. L'utilisateur sélectionne ensuite un groupe d'images et fait une deuxième AFC sur le nouveau tableau de contingence. Ce processus peut continuer jusqu'à ce qu'un thème « pur » se dévoile. Ceci permet de découvrir une arborescence des thèmes dans une collection d'images. Une application sur la base Caltech-4 illustre l'intérêt de HCAViz dans la fouille d'images.
**** *annee_2011  *numText_286
Cet article présente une nouvelle approche qui permet de compter le nombre d'individus franchissant une ligne de comptage. L'approche proposée accumule dans le temps les vecteurs de mouvement pour chaque point de la ligne de comptage formant une carte spatio-temporelle. Une procédure de détection en ligne des blobs est ensuite utilisée afin de déterminer les régions de la carte spatio-temporelle qui correspondent à des personnes franchissant cette ligne. Le nombre d'individus associé à chaque blob est estimé grâce à un modèle de régression linéaire appliqué aux caractéristiques du blob. L'approche proposée est validée sur la base de plusieurs ensembles de données enregistrées à l'aide d'une caméra verticale ou d'une caméra oblique.
**** *annee_2011  *numText_287
La reconnaissance d'entités nommées est une problématique majoritairement traitée par des modèles spécifiés à l'aide de règles ou par apprentissage numérique. Les premiers ont le désavantage d'être coûteux à développer pour obtenir une couverture satisfaisante, les seconds sont souvent difficiles à interpréter par des experts (linguistes). Dans cet article, nous présentons une approche, dont l'objectif est d'extraire des règles symboliques discriminantes qu'un humain puisse consulter. A partir d'un corpus de référence, nous extrayons des règles de transduction, dont seules les plus informatives sont retenues. Elles sont ensuite appliquées pour effectuer une annotation : à cet effet, un algorithme recherche parmi les annotations possibles celles de meilleure qualité en termes de couverture et de probabilité. Nous présentons les résultats expérimentaux et discutons de l'intérêt et des perspectives de notre approche.
**** *annee_2011  *numText_289
Des travaux récents (Pilaszy et al., 2009) suggèrent que les métadonnées sont quasiment inutiles pour les systèmes de recommandation, y compris en situation de cold-start : les données de logs de notation sont beaucoup plus informatives. Nous étudions, sur une base de référence de logs d'usages pour la recommandation automatique de DVD (Netflix), les performances de systèmes de recommandation basés sur des sources de données collaboratives, thématiques et hybrides en situation de démarrage à froid (cold-start). Nous exhibons des cas expérimentaux où les métadonnées apportent plus que les données de logs d'usage (collaboratives) pour la performance prédictive. Pour gérer le cold-start d'un système de recommandation, nous montrons que des approches en cascade, thématiques puis hybrides, puis collaboratives, seraient plus appropriées.
**** *annee_2011  *numText_290
La Carte Auto-Organisatrice (SOM : Self-Organizing Map) est une méthode populaire pour l'analyse de la structure d'un ensemble de données. Cependant, certaines contraintes topologiques de la SOM sont fixées avant l'apprentissage et peuvent ne pas être pertinentes pour la représentation de la structure des données. Dans cet article nous nous proposons d'améliorer les performances des SOM avec un nouvel algorithme qui apprend les contraintes topologiques de la carte à partir des données. Des expériences sur des bases de données artificielles et réelles montrent que l'algorithme proposé produit de meilleurs résultats que SOM classique. Ce n'est pas le cas avec une relaxation triviale des contraintes topologiques, qui résulte en une forte augmentation de l'erreur topologique de la carte.
**** *annee_2011  *numText_291
Les Réseaux Logiques de Markov (MLNs) combinent l'apport statistique des Réseaux de Markov à la logique du premier ordre. Dans cette approche, chaque clause logique se voit affectée d'un poids, l'instanciation des clauses permettant alors de produire un Réseau de Markov. L'apprentissage d'un MLN consiste à apprendre d'une part sa structure (la liste de clauses logiques) et d'autre part les poids de celles-ci. Nous proposons ici une méthode d'apprentissage génératif de Réseau Logique de Markov. Cette méthode repose sur l'utilisation d'un graphe des prédicats, produit à partir d'un ensemble de prédicats et d'une base d'apprentissage. Une méthode heuristique de variabilisation est mise en œuvre afin de produire le jeu de clauses candidates. Les résultats présentés montrent l'intérêt de notre approche au regard de l'état de l'art.
**** *annee_2011  *numText_292
Une carte cognitive est un réseau d'influences entre différents concepts. Le modèle des cartes cognitives permet à un utilisateur de calculer l'influence entre deux concepts. Les cartes cognitives contenant un grand nombre de concepts et d'influences sont difficiles à comprendre. Cet article introduit la notion de carte cognitive ontologique qui associe une ontologie à une carte cognitive classique pour en organiser les concepts. Afin de faciliter la compréhension d'une carte, l'utilisateur peut obtenir une vue de cette carte la simplifiant selon une échelle qu'il aura choisie. Un profil peut être créé pour construire des vues correspondant aux objectifs d'un type d'utilisateur. Si une carte est manipulée par différents utilisateurs, leurs profils combinés permettent de construire une vue partagée.
**** *annee_2011  *numText_293
La recherche de règles d'association intéressantes est un domaine de recherche important et actif en fouille de données. Les algorithmes de la famille Apriori reposent sur deux mesures pour extraire les règles, le support et la confiance. Bien que ces deux mesures possèdent des vertus algorithmiques accélératrices, elles génèrent un nombre prohibitif de règles dont la plupart sont redondantes et sans intérêt. Il est donc nécessaire de disposer d'autres mesures filtrant les règles inintéressantes. Des travaux ont été réalisés pour dégager les bonnes propriétés des mesures d'extraction des règles et ces propriétés ont été évaluées sur 61 mesures. L'objectif de cet article est de dégager des catégories de mesures afin de répondre à une préoccupation des utilisateurs : le choix d'une ou plusieurs mesures lors d'un processus d'extraction des connaissances dans le but d'éliminer les règles valides non pertinentes extraites par le couple (support, confiance). L'évaluation des propriétés sur les 61 mesures a permis de dégager 9 classes de mesures, classes obtenues grâce à deux techniques : une méthode de la classification ascendante hiérarchique et une version de la méthode de classification non-hiérarchique des k-moyennes.
**** *annee_2011  *numText_294
En apprentissage supervisé, les Méthodes Ensemble (ME) ont montré leurs qualités. L'une des méthodes de référence dans ce domaine est les Forêts Aléatoires (FA). Cette dernière repose sur des partitionnements de l'espace de représentation selon des frontières parallèles aux axes ou obliques. Les conséquences de cette façon de partitionner l'espace de représentation peuvent affecter la qualité de chaque prédicteur. Il nous a semblé que cette approche pouvait être améliorée si on se libérait de cette contrainte de manière à mieux coller à la structure topologique de l'ensemble d'apprentissage. Dans cet article, nous proposons une nouvelle ME basée sur des graphes de voisinage dont les performances, sur nos premières expérimentations, sont aussi bonnes que celles des FA.
**** *annee_2011  *numText_295
Dans le présent travail, nous proposons un outil d'aide à la reconnaissance de cibles radar basé sur la signature de forme et de la pose de la cible. La tâche principale dans le cadre de cet article consiste à établir la fonction de recherche d'images ISAR par l'exemple en exploitant l'information de pose estimée depuis les images ISAR. L'objectif est d'introduire l'information de pose dans l'indexation des images, notamment dans la phase de sélection des images candidates. Nous proposons une nouvelle méthode d'estimation de la pose basée sur l'axe le plus symétrique de la cible. La méthode proposée est ensuite comparée avec d'autres techniques connues telles que la transformée de Hough et la transformée en ondelette. Enfin, la tâche de classification est réalisée en utilisant les k-plus proches voisins incluant l'information de la pose.
**** *annee_2011  *numText_297
L'élaboration d'une échelle de probabilité discriminante pour la comparaison mutuelle entre plusieurs attributs observés sur un échantillon d'objets de grosse taille, nécessite une normalisation préalable. L'objet de cet article est l'analyse comparée entre deux approches. La première dérive de l' Analyse de la Vraisemblance des Liens Relationnels Normalisée. La seconde est fondée sur la notion de Valeur Test sur un échantillon virtuel de taille 100, synthétisant l'échantillon initial.
**** *annee_2011  *numText_303
Les séries temporelles d'images satellites (ou Satellite Image Time Series - SITS) sont d'importantes sources d'informations sur l'évolution du territoire. Étudier ces images permet de comprendre les changements sur des zones précises mais aussi de découvrir des schémas d'évolution à grande échelle. Toutefois, découvrir ces phénomènes impose de répondre à plusieurs défis qui sont liés aux caractéristiques des SITS et à leurs contraintes. Premièrement, chaque pixel d'une image satellite est décrit par plusieurs valeurs (les niveaux radiométriques sur différentes longueurs d'ondes). Deuxièmement, ces motifs d'évolution portent sur des périodes très longues et ne sont pas forcément synchrones selon les régions. Troisièmement, les régions qui ne sont pas concernées par des évolutions significatives sont majoritaires et leur domination rend difficile l'extraction des motifs d'évolution. Dans cet article, nous proposons une méthode qui répond à ces difficultés et nous la validons sur une série d'images satellites acquises sur une période de 20 ans.
**** *annee_2011  *numText_305
L'analyse de flux de données traite des données massives grâce à des algorithmes en ligne qui évitent le stockage exhaustif des données. La détection de changements dans la distribution d'un flux est une question importante dont les applications potentielles sont nombreuses. Dans cet article, la détection de changement est transposée en un problème d'apprentissage supervisé. Nous avons choisi d'utiliser la méthode de discrétisation supervisée MODL car celle-ci présente des propriétés intéressantes. Notre approche est comparée favorablement à une méthode de l'état-de-l'art sur des flux de données artificiels.
**** *annee_2011  *numText_306
Nous nous intéressons dans cet article à la réconciliation d'annotations floues associées à des tableaux de données par une méthode d'annotation sémantique, qui est guidée par une ontologie de domaine. Etant donnés deux tableaux, la méthode consiste à détecter leurs instances de relation redondantes. Elle s'appuie sur les connaissances déclarées dans l'ontologie, ainsi que sur des scores de similarité entre les annotations floues représentées par des sous-ensembles flous numériques ou par des sous-ensembles flous symboliques
**** *annee_2011  *numText_307
La conception des profils et contextes utilisateurs se situe au cœur de l'étude et de la mise en oeuvre des mécanismes de personnalisation ou d'adaptation de contenus (recherche d'information, systèmes de recommandation, etc.). Plusieurs modèles et dimensions de profils et contextes sont décrits dans la littérature. Dans la vie réelle tout comme dans les systèmes d'information, le comportement de l'utilisateur est très souvent influencé par son environnement social. Cependant, la dimension sociale des profils et contextes utilisateurs reste très peu étudiée et évaluée. Dans cet article, nous présentons une méthode de visualisation des profils utilisateurs permettant d'évaluer la pertinence du réseau social de l'utilisateur dans l'évolution de son profil. L'expérimentation de la méthode à partir de Facebook permet d'identifier d'une part, les centres d'intérêts à court-terme et à long-terme des profils utilisateurs, et d'autre part, l'influence réelle à court-terme et à long-terme du réseau social de chaque utilisateur. Ces résultats démontrent l'intérêt de modéliser et d'intégrer une dimension sociale dans les profils et contextes utilisateurs, afin de tenter d'améliorer les mécanismes de personnalisation ou d'adaptation de contenus.
**** *annee_2011  *numText_310
Cet article propose une méthode originale d'évaluation de la qualité des motifs en anticipant la manière qui sera utilisée pour les analyser. Nous commençons par introduire le modèle de l'analyse aléatoire d'un ensemble de motifs selon une mesure d'intérêt. Avec ce modèle, nous constatons que l'étude des motifs fréquents avec le support conduit à une analyse déséquilibrée du jeu de données. Afin que chaque transaction reçoive la même attention, nous définissons le support équilibré qui corrige le support classique en pondérant les transactions. Nous proposons alors un algorithme qui calcule ces poids et nous validons expérimentalement son efficacité.
**** *annee_2011  *numText_311
Le choix d'une mesure de proximité entre objets a un impact direct sur les résultats de toute opération de classification, de comparaison, d'évaluation ou de structuration d'un ensemble d'objets. Pour un problème donné, l'utilisateur est amené à choisir une parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence choisie, comme celle basée sur les préordonnances, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche pour comparer les mesures de proximité. Celle-ci est basée sur l'équivalence topologique. A cet effet, nous introduisons un nouveau concept baptisé équivalence topologique. Ce dernier fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure. Nous établissons ensuite des liens formels avec l'équivalence en préordonnance. Les deux approches sont comparées sur le plan théorique et sur le plan empirique. Nous illustrons le principe de cette comparaison sur un exemple simple pour une quinzaine de mesures de proximités de la littérature.
**** *annee_2011  *numText_312
La recherche de structures dans les graphes est un sujet étudié depuis longtemps, qui a bénéficié d'un regain d'intérêt avec la mise à disposition de graphes de grande taille sur le web, tels les réseaux sociaux. De nombreuses méthodes de recherche de clusters naturels dans les graphes ont été proposées, fondées notamment sur la modularité de Newman. On introduit dans cet article une nouvelle façon de résumer la structure des graphes de grande taille, en utilisant des estimateurs de densité des arcs exploitant des modèles en grille, basés sur un co-partitionnent des nœuds source et cible des arcs. Les structures identifiées par cette méthode vont au delà de la classique détection de clusters dans les graphes, et permettent d'estimer asymptotiquement la densité des arcs. Les expérimentations confirment le potentiel de l'approche, qui permet d'identifier des structures fortement informatives dans les graphes, sans faire l'hypothèse d'une décomposition en clusters denses.
**** *annee_2011  *numText_314
L'article décrit l'évaluation de deux outils d'extraction terminologique Acabit et Quezao. Si acabit est plus connu car librement disponible, Quezao est issu des travaux d'Orange Labs sur la recherche d'informations. Après une comparaison sur les approches théoriques des deux systèmes, une évaluation concrète va porter sur un corpus d'actualité (2424Actu) pour l'aspect qualitatif et sur un corpus de presse pour l'aspect quantitatif
**** *annee_2011  *numText_315
Les motifs séquentiels traditionnels ne tiennent généralement pas compte des informations contextuelles fréquemment associées aux données séquentielles. Dans le cas des séquences d'achats de clients dans un magasin, l'extraction classique de motifs se focalise sur les achats des clients sans considérer leur catégorie socio-professionnelle, leur sexe, leur âge. Or, en considérant le fait qu'un motif séquentiel est spécifique à un contexte donné, un expert pourra adapter sa stratégie au type du client et prendre les décisions adéquates. Dans cet article, nous proposons d'extraire des motifs de la forme «l'achat des produits A et B suivi de l'achat du produit C est spécifique aux jeunes clients». En mettant en valeur les propriétés formelles de tels contextes, nous développons un algorithme efficace d'extraction de motifs séquentiels contextuels. Les expérimentations effectuées sur un jeu de données réelles montrent les apports et l'efficacité de l'approche proposée.
**** *annee_2011  *numText_316
La fouille de base de données séquentielles a pour objet l'extraction de motifs séquentiels représentatifs. La plupart des méthodes concernent des motifs composés d'événements liés par des relations temporelles basées sur la précédence des instants. Pourtant, dans de nombreuses situations réelles une information quantitative sur la durée des événements ou le délai inter-événements est nécessaire pour discriminer les phénomènes. Nous proposons deux algorithmes, QTIAPriori et QTIPrefixSpan, pour extraire des motifs temporels composés d'événements associés à des intervalles décrivant leur position dans le temps et leur durée. Chacun d'eux ajoute aux algorithmes GSP et PrefixSpan une étape de catégorisation d'intervalles multi-dimensionnels pour extraire les intervalles temporelles représentatifs. Les expérimentations sur des données simulées montrent la capacité des algorithmes à extraire des motifs précis en présence de bruit et montrent l'amélioration des performances en temps de calcul.
**** *annee_2011  *numText_317
Dans un contexte d'entreprise, beaucoup d'informations importantes restent stockées dans des bases de données relationnelles, constituant une source riche pour construire des réseaux sociaux. Le réseau, ainsi extrait, a souvent une taille importante ce qui rend son analyse et sa visualisation difficiles. Dans ce travail, nous proposons une étape d'extraction suivie d'une étape d'agrégation des réseaux sociaux à partir des bases de données relationnelles. L'étape d'extraction ou de construction transforme une base de données relationnelle en base de données graphe, puis le réseau social est extrait. L'étape d'agrégation, qui est basée sur l'algorithme k-SNAP, produit un graphe résumé.
**** *annee_2011  *numText_318
Nous proposons une méthode de fouille de données sur des graphes ayant un ensemble d'étiquettes associé à chaque sommet. Une application est, par exemple, d'analyser un réseau social de chercheurs co-auteurs lorsque des étiquettes précisent les conférences dans lesquelles ils publient.Nous définissons l'extraction sous contraintes d'ensembles de cliques tel que chaque sommet des cliques impliquées partage suffisamment d'étiquettes. Nous proposons une méthode pour calculer tous les Ensembles Maximaux de Cliques dits Homogènes qui satisfont une conjonction de contraintes fixée par l'analyste et concernant le nombre de cliques séparées, la taille des cliques ainsi que le nombre d'étiquettes partagées. Les expérimentations montrent que l'approche fonctionne sur de grands graphes construits à partir de données réelles et permet la mise en évidence de structures intéressantes
**** *annee_2011  *numText_319
La recherche de motifs ensemblistes dans des matrices de données booléennes est une problématique importante dans un processus d'extraction de connaissances. Elle consiste à rechercher tous les rectangles de 1 dans une matrice de données à valeurs dans {0,1} dans lesquelles l'ordre des lignes et colonnes n'est pas important. Plusieurs algorithmes ont été développés pour répondre à ce problème, mais s'adaptent difficilement à des données réelles susceptibles de contenir du bruit. Un des effets du bruit est de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinents, entraînant une explosion du nombre de motifs résultats. Dans le cadre de ce travail, nous proposons une nouvelle approche heuristique basée sur les algorithmes de graphes pour la recherche de motifs ensemblistes dans des contextes binaires bruités. Pour évaluer notre approche, différents tests ont été réalisés sur des données synthétiques et des données réelles issues d'applications bioinformatiques.
**** *annee_2011  *numText_320
La première étape du processus de visualisation d'information consiste à transformer les données d'un format brut vers une structure de données utilisable par les différents composants de visualisation. Dans les applications réelles, cette première étape représente une barrière empêchant l'accès des utilisateurs novices à une riche variété de techniques de visualisation. Par exemple, il peut être techniquement impossible pour un utilisateur lambda de transformer des données arborescentes en un modèle de graphe pouvant utiliser une représentation à base de TreeMap. Une autre barrière est aussi la multitude de transformations possible des données brutes. Il faut pouvoir explorer cet ensemble de combinaisons. Basé sur nos retours d'expériences avec des utilisateurs finaux, dans cet article, nous considérons que le format brut est sous forme tabulaire. Ce format est le plus couramment utilisé et est facilement accessible par nos utilisateurs. Nous proposons une méthode novatrice permettant de générer automatiquement des graphes valués à partir de n'importe quelle table. En analysant le contenu de chaque dimension nous identifions les interconnexions entre celles-ci. Puis nous caractérisons les entités, les attributs et les relations possibles au sein des tables. Finalement, nous intégrons l'utilisateur dans le processus de transformation en lui proposant un ensemble de transformations valides.
**** *annee_2011  *numText_321
Cet article a pour cadre un environnement informatique pour l'apprentissage humain (EIAH) dédié à la chirurgie orthopédique, et plus précisément sur le diagnostic des connaissances des apprenants. Pour ce faire, un réseau bayésien infère à partir d'exercices que les étudiants réalisent sur un simulateur avec bras articulé. Ce réseau résulte d'une approche centrée expert du domaine, comme très souvent dans les EIAH. Pourtant, dans un domaine comme la chirurgie où les connaissances sont tacites, le geste de l'apprenant semble intéressant à considérer. Le but de nos travaux est donc d'adopter une démarche plus centrée sur les données en incorporant au réseau bayésien les données haptiques continues issues du simulateur. Divers problèmes se posent néanmoins, d'une part sur le besoin d'étudier la nature des données pour conserver la généricité du système, et d'autre part pour trouver des méthodes de validation pertinentes concernant leur traitement
**** *annee_2011  *numText_323
Ce papier présente une vue spectrale sur l'approche de l'analyse relationnelle pour la classification des données catégorielles. Il établit d'abord le lien théorique entre l'approche de l'analyse relationnelle et le problème de classification spectrale. En particulier, le problème de classification relationnelle est présenté comme un problème de maximisation de trace, ce problème est donc transformé par la relaxation spectrale en un problème d'optimisation sous contraintes qui peut être résolu par des multiplicateurs de Lagrange, la solution est donnée par un problème de valeurs propres.
**** *annee_2011  *numText_326
Cet article est un état de l'art sur les moteurs de wiki sémantique, en particulier sur leur utilisation des technologies du Web sémantique. Les principales notions liées aux wikis sémantiques sont d'abord présentées. Ensuite, plusieurs projets actifs de moteurs de wiki sont comparés selon différents points de vue. Finalement, des recommandations sont données pour le choix d'un moteur de wiki. En conclusion, les auteurs s'interrogent sur les perspectives des wikis sémantiques telles que la faible interopérabilité de certains moteurs.
**** *annee_2011  *numText_328
Dans cet article, nous proposons une mesure de concordance d'une source avec les autres sources. Cette mesure pourra servir à réduire l'importance de ses fonctions de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit. Cette mesure sera illustrée par des données réelles.
**** *annee_2011  *numText_329
L'autonomie des participants dans les systèmes P2P pour le partage de données peut conduire à une situation d'hétérogénéité sémantique dans le cas où les participants utilisent leurs propres ontologies pour représenter leurs données. Dans cet article nous commençons par définir des mesures de disparité entre participants en considérant leurs contextes sémantiques. En considérant la topologie du système et les disparités entre participants, nous proposons des mesures d'hétérogénéité sémantique d'un système P2P non-structuré.
**** *annee_2011  *numText_330
Nous proposons d'extraire des connaissances lexicales en exploitant les « gloses » de mot, ces descriptions spontanées de sens, repérables par des marqueurs lexicaux et des configurations morpho-syntaxiques spécifiques. Ainsi dans l'extrait suivant, le mot testing est suivi d'une glose en c'est-à dire : « 10 % de ces embauches vont porter sur un métier qui monte : le «testing», c'est-à-dire la maîtrise des méthodologies rigoureuses de test des logiciels». Cette approche ouvre des perspectives pour l'acquisition lexicale et terminologique, fondamentale pour de nombreuses tâches. Dans cet article, nous comparons deux façons d'extraire les unités en relation de glose : patrons et statistiques d'associations d'unités sur le web, en les évaluant sur des données réelles.
**** *annee_2011  *numText_333
Dans ce papier, nous proposons un nouveau cadre théorique permettant de modéliser la dynamique de phénomènes spatio-temporels. Nous définissons le concept de séquences spatio-temporelles de motifs afin de capturer les interactions entre des ensembles de propriétés et un phénomène à observer. Un algorithme incrémental est proposé pour extraire des séquences spatio-temporelles de motifs sous contraintes, et une nouvelle structure de données est mise en place afin d'améliorer ses performances. Un prototype a été développé et testé sur des données réelles.
**** *annee_2011  *numText_334
Nous proposons un modèle de la propagation de l'information dans un réseau, en détaillant toutes les étapes de sa réalisation et de son utilisation dans un cadre de simulation. A partir de données réelles extraites du Web, nous identifions parmi les sources des catégories de comportements de publication distincts. Nous proposons ensuite une extension d'un modèle de diffusion de l'information existant, afin d'augmenter son pouvoir d'expression, en particulier pour reproduire ces comportements de publication, puis nous le validons sur un exemple de simulation.
**** *annee_2011  *numText_335
Nous proposons dans cet article une modélisation d'une ressource termino-ontologique (RTO) de domaine, guidée par la tâche d'annotation sémantique de tableaux. L'annotation d'un tableau consiste à annoter ses cellules, pour pouvoir ensuite identifier les concepts représentés par ses colonnes et enfin identifier la ou les relations n-aires qu'il représente. La RTO proposée permet d'une part de modéliser dans sa composante lexicale les termes utilisés pour l'annotation des cellules en intégrant la gestion des synonymes et du multilingue, et, d'autre part, de modéliser dans sa composante conceptuelle les concepts symboliques, les concepts numériques et les relations n-aires, qui sont propres au domaine étudié.
**** *annee_2011  *numText_337
Cet article présente comment la gestion et l'exploitation de connaissances issues du site web Wikipedia ont permis de développer une telle fonction qui a été intégrée depuis février 2010 dans un moteur de recherche internet français pour le grand public. Aujourd'hui cette fonction est capable de répondre à des questions formulées en langage naturelle sur environs 170 000 lieux ou personnes. La formalisation des données extraites de wikipedia en connaissances au format OWL ou RDFS a permis de déduire de nouvelles informations manquantes, de typer les entités nommées trouvées et de traiter de nouvelles formes de questions qui étaient non traitées.
**** *annee_2011  *numText_338
Bien que largement étudiée, l'extraction de motifs séquentiels reste une tâche très difficile et pose aussi le défi du grand nombre de motifs produits. Dans cet article, nous proposons une nouvelle approche extrayant les motifs séquentiels les plus généraux à fréquence similaire. Nous montrons en quoi l'extension de cette notion, déjà connue pour les motifs ensemblistes, est un problème particulièrement difficile pour les séquences. Les motifs delta-libres ainsi produits sont en nombre réduit et facilitent les usages d'un processus de fouille et nous montrons leur apport comme descripteurs dans un contexte de classification de séquences.
**** *annee_2011  *numText_339
Avec l'explosion du multimedia, l'utilisation des métadonnées est devenue cruciale pour assurer une bonne gestion des contenus. Cependant, il est nécessaire d assurer un accès uniforme aux métadonnées. Plusieurs techniques ont ainsi été développées afin de réaliser cette interopérabilité. La plupart d'entre elles sont spécifiques à un seul langage de description. Les systèmes de matching existants présentent certaines limites, en particulier dans le traitement des informations structurelles. Nous présentons dans cet article un nouveau système d'intégration qui supporte des schémas provenant de langages descriptifs différents. De plus, la méthode de matching proposée a recours à plusieurs types d'information de façon à augmenter la précision de matching
**** *annee_2011  *numText_341
La fouille de graphes est devenue une piste de recherche intéressante et un défi réel en matière de fouille de données. Parmi les différentes familles de motifs de graphes, les graphes fréquents permettent une caractérisation intéressante des groupes de graphes, ainsi qu'une discrimination des différents graphes lors de la classification ou de la segmentation. A cause de la NP-complétude du test d'isomorphisme de sous-graphes et de l'immensité de l'espace de recherche, les algorithmes de fouille de graphes sont exponentiels en temps d'exécution et/ou occupation mémoire. Dans cet article, nous étudions un nouvel opérateur de projection polynomial nommé AC-projection basé sur une propriété clé du domaine de la programmation par contraintes, à savoir l'arc consistance. Cet opérateur est censé remplacer l'utilisation de l'isomorphisme de sous-graphes en établissant un biais sur la projection. Cette étude est suivie d'une évaluation expérimentale du pouvoir discriminant des patterns AC-réduits découverts.
**** *annee_2011  *numText_342
Dans ce papier, nous proposons une approche basée sur la programmation par contraintes pour aborder efficacement le problème de l'alignement des ontologies, et plus particulièrement l'extraction des correspondances à partir des mesures de similarités. La complexité de ce problème est accentuée dans les applications à caractère dynamique où l'aspect performance est capital. Plus précisément, nous exploitons la contrainte globale de différence développée dans le domaine de la programmation par contraintes pour extraire un alignement total et injectif. Nous montrons que cette approche est efficace et se prête à une mise en œuvre à la fois interactive et automatique.
**** *annee_2011  *numText_343
Le classifieur Bayésien naïf est un outil de classification efficace en pratique pour de nombreux problèmes réels, en dépit de l'hypothèse restrictive d'indépendance des variables conditionnellement à la classe. Récemment, de nouvelles méthodes permettant d'améliorer la performance de ce classifieur ont vu le jour, sur la base à la fois de sélection de variables et de moyennage de modèles. Dans cet article, nous proposons une extension de la sélection de variables pour le classifieur Bayésien naïf, en considérant un modèle de pondération des variables utilisées et des algorithmes d'optimisation directe de ces poids. Les expérimentations confirment la pertinence de notre approche, en permettant une diminution significative du nombre de variables utilisées, sans perte de performance prédictive.
**** *annee_2011  *numText_346
Dans cet article, nous proposons une nouvelle approche de classification topologique et de pondération des variables mixtes (qualitatives et quantitatives codées en binaire) durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage est combiné à un mécanisme de pondération des différentes variables sous forme de poids d'influence sur la pertinence des variables. L'apprentissage des pondérations et des prototypes est réalisé d'une manière simultanée en favorisant une classification optimisée des données. L'approche proposée a été validée sur des données qualitatives codées en binaire et plusieurs bases de données mixtes.
**** *annee_2011  *numText_348
Nous présentons une étude pour la prédiction des trajectoires de cyclones dans l'océan Atlantique Nord à partir de données issues d'images satellites. On y extrait des mesures de vitesses de vent, de vorticité, d'humidité (base JRA-25) et des mesures de latitude, de longitude et de vitesse de vent instantanée des cyclones toutes les 6 heures (base IBTrACS). Les modèles de référence à ce jour ne tiennent pas compte des corrélations entre les données et les prévisions ce qui limite leur intérêt pour certains utilisateurs. Nous proposons ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 h toutes les 6 h à l'aide de forêts aléatoires avec arbres de régression. Sur le long terme, à partir de 18 h, la méthode proposée donne de meilleurs résultats que les méthodes existantes.
**** *annee_2011  *numText_350
La fouille de données relationnelles considère des données contenues dans au moins deux tables reliées par une association un-à-plusieurs, par exemple des clients et leurs achats, ou des molécules et leurs atomes. Une façon de fouiller ces données consiste à transformer les données en une seule table attribut-valeur. Cette transformation est appelée propositionalisation. Les approches existantes gèrent principalement les attributs catégoriels. Une première solution est donc de discrétiser les attributs numériques pour les transformer en attributs catégoriels. Les approches alternatives, qui gèrent les attributs numériques, consistent à les agréger. Nous proposons une approche duale de la discrétisation, qui inverse l'ordre de traitement du nombre d'objets et du seuil, et dont la discrétisation généralise les quartiles. Nous pouvons ainsi construire des attributs que les approches existantes de propositionalisation ne peuvent pas construire, et qui ne peuvent pas non plus être obtenus par les systèmes complets de fouille de données.
**** *annee_2011  *numText_352
Cet article propose une approche utilisant les modèles de direction et de magnitude de mouvement pour détecter les actions qui sont effectuées par des êtres humains dans des séquences vidéo. Des mélanges Gaussiens et de lois de von Mises sont estimés à partir des orientations et des magnitudes des vecteurs du flux optique calculés pour chaque bloc de la scène. Les paramètres de ces modèles sont estimés grâce à un algorithme d'apprentissage en ligne. Les actions sont reconnues grâce à une mesure qui se base sur la distance de Bhattacharyya et qui permet de comparer le modèle d'une séquence donnée avec les modèles créés à partir de séquences d'apprentissage. L'approche proposée est évaluée sur deux ensembles de vidéos contenant des actions variées exécutées aussi bien dans des environnements intérieur qu'extérieur.
**** *annee_2011  *numText_353
Une façon d'assister l'analyse d'entrepôt de données repose sur l'exploitation et la fouille de fichiers logs de requêtes OLAP. Mais, à notre connaissance, il n'existe pas de méthode permettant d'obtenir une représentation d'un tel log qui soit à la fois concise et exploitable. Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L'idée de base est qu'une requête résume une autre requête et qu'un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d'une algèbre simple destinée à résumer des requêtes OLAP, et d'une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l'intérêt de notre approche.
**** *annee_2011  *numText_354
Dans la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. La plupart des approches existantes opèrent en transformant la représentation multi-tables, notamment par mise à plat. Par conséquent, on perd la représentation initiale naturellement compacte mais également on risque d'introduire des biais statistiques. Notre approche a pour objectif d'évaluer l'informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs. Elle consiste à résumer l'information contenue dans chaque variable par un tuple d'attributs représentant les effectifs des modalités de celle-ci. Des modèles en grilles multivariées sont alors employés pour qualifier l'information apportée conjointement par les nouveaux attributs, ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs. Les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement pertinentes sur tout le domaine relationnel.
**** *annee_2011  *numText_357
Dans le cadre de nos travaux sur le portage linguistique des systèmes de gestion de contenu traitant des énoncés spontanés en langue naturelle, nous présentons ici une évaluation du portage d'IMRS (système de recherche de morceau de musique en langue naturelle) Kumamoto (2007) du japonais vers le français. Cette évaluation peut se faire au niveau des représentations internes en les comparant, ou au niveau de la tâche. Ici, nous nous intéressons à une évaluation liée à la tâche en proposant un service Web qui permet de mesurer la performance globale de la nouvelle version obtenue. Nous avons par la suite cherché à améliorer et ajouter de nouvelles fonctionnalités en proposant un service de recherche de musique adaptable à la perception de chaque utilisateur. En effet, un même morceau de musique peut être jugé calme pour un premier auditeur, très calme pour un deuxième, et assez calme pour un troisième, etc. On se demande l'impression finale que porte ce dernier morceau de musique. C'est naturel que les utilisateurs évaluent différemment un même morceau de musique car ils ont des perceptions différentes. Devant cette situation, nous proposons un service de recherche de musique basé des méthodes simples et automatisées et qui sont adaptables à la perception de chaque utilisateur.
**** *annee_2011  *numText_358
Depuis les deux dernières décennies, l'augmentation du nombre de sites d'emploi sur Internet a accentué la nécessité de proposer des outils d'aide à la décision adaptés aux besoins des recruteurs. Cet article présente un système pour la catégorisation des textes d'offres d'emploi destinées à être diffusées sur Internet. Après un pré-traitement adapté des offres, les termes descripteurs sont choisis en fonction de leur pouvoir discriminant vis-à-vis des différentes classes ce qui permet de réduire leur nombre de manière significative. Les offres sont ensuite représentées par leurs coordonnées dans l'espace factoriel obtenu par analyse des correspondances et la classification réalisée dans un cadre supervisé à l'aide de SVM.
**** *annee_2011  *numText_360
Le concept de SKYLINE a été introduit pour mettre en évidence les objets « les meilleurs » selon différents critères. Une généralisation multidimensionnelle du SKYLINE a été proposée à travers le SKYCUBE qui réunit tous les SKYLINES possibles selon toutes les combinaisons de critères et permet d'analyser les liens entre objets SKYLINES. Comme le data cube, le SKYCUBE s'avère extrêmement volumineux si bien que des approches de réduction sont incontournables. Dans cet article, nous définissons une approche de matérialisation partielle du SKYCUBE. L'idée sous-jacente est d'éliminer de la représentation les Skycuboïdes facilement re-calculables. Pour atteindre cet objectif de réduction, nous caractérisons un cadre formel : le treillis des concepts ACCORDS. Cette structure combine la notion d'ensemble en accord et le treillis des concepts. À partir de cette structure, nous dérivons le treillis des concepts SKYLINES qui en est une instance contrainte. Le point fort de notre approche est d'être orientée attribut ce qui permet de borner le nombre de nœuds du treillis et d'obtenir une navigation efficace à travers les Skycuboïdes.
**** *annee_2011  *numText_361
L'utilisation de règles de classification dans les modèles prédictifs a été très étudiée ces dernières années. La forme simple et interprétable des règles en font des motifs très populaires. Les classifieurs combinant des règles de classification intéressantes (selon une mesure d'intérêt) offrent de bonnes performances de prédictions. Cependant, les performances de ces classifieurs dépendent de la mesure d'intérêt (e.g., confiance, taux d'accroissement, ... ) et du seuillage (non-trivial) de cette mesure pour déterminer les règles pertinentes. De plus, il est facile de montrer que les règles extraites ne sont pas individuellement robustes. Dans cet article, nous proposons un nouveau critère pour évaluer la robustesse des règles de classification dans les données Booléennes. Notre critère est issu d'une approche Bayésienne : nous proposons une expression analytique de la probabilité d'une règle connaissant les données. Ainsi, les règles les plus probables sont robustes. Le critère Bayésien nous permet alors d'identifier (sans paramètre) les règles robustes parmi un ensemble de règles données.
**** *annee_2011  *numText_362
Les tags fournis par les utilisateurs des plate-formes de tagging social ne sont pas explicitement liés sémantiquement, et ceci limite considérablement les possibilités d'exploitation de ces données. Nous présentons dans cet article notre approche pour l'enrichissement sémantiques des folksonomies qui intègre une combinaison de traitements automatiques ainsi que la capture des contributions de structuration des utilisateurs via une interface ergonomique. De plus, notre modèle supporte les points de vue qui divergent tout en permettant de les combiner en respectant leur cohérence locale. Cette approche s'adresse aux communautés de connaissances collaborant en ligne, et en intégrant leurs usages, nous sommes en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies. La navigation dans les données de tagging est ainsi améliorée, et les folksonomies peuvent alors être directement intégrées dans la construction de thesauri.
**** *annee_2011  *numText_365
Le CNSS - Cellular Neuro-Symbolic System - est un système hybride ralliant conjointement le neuro-symbolique et le cellulaire. CNSS permet, à partir d'une base de cas pratique, de faire coopérer un réseau de neurones, un graphe d'induction et un automate cellulaire pour la construction d'un modèle de prédiction. En détectant et en éliminant les individus non applicables et les variables non pertinentes, le réseau de neurones optimise la base d'apprentissage. Le résultat ainsi obtenu est affiné par un processus d'apprentissage symbolique à base de graphe d'induction. Ce raffinement se fait par une modélisation booléenne qui va assister l'apprentissage symbolique à optimiser le graphe d'induction et va assurer, par la suite, la représentation et la génération des règles de classification sous forme conjonctives avant d'entamer la phase de déduction par un moteur d'inférence cellulaire. CNSS a été testé sur plusieurs applications en utilisant des problèmes académiques et réels. Les résultats montrent que le système CNSS a des performances supérieures et de nombreux avantages.
**** *annee_2011  *numText_367
Nous proposons dans cet article une méthode qui calcule la distance entre ontologies dans un but d'aide à la décision sur la pertinence ou non de leur fusion. Cette méthode calcule la distance entre parties homologues de deux ontologies par rapport à leurs niveaux de détail et leurs structures taxonomiques, et ce en exploitant les correspondances produites par un alignement préalablement effectué entre ces ontologies, et en adaptant la méthode de la distance d'édition entre arbres ordonnés. Nous limitons notre étude ici aux ontologies légères, c'est à dire des taxonomies représentées en langages OWL, le langage d'ontologies pour le Web. Notre méthode a été implémentée et testée sur des ontologies réelles, et les résultats obtenus semblent prometteurs.
**** *annee_2011  *numText_368
Dans un contexte économique difficile, la fidélisation des clients figure au premier rang des préoccupations des entreprises. En effet, selon le Gartner, fidéliser des clients existants coûterait beaucoup moins cher que prospecter de nouveaux clients. Pour y parvenir, les entreprises optimisent la marge et le cycle de vie des clients en développant une relation personnalisée aboutissant à de meilleures recommandations. Dans cet article, nous proposons une méthodologie pour les systèmes de recommandations fondée sur l'analyse des chiffres d'affaires des clients sur des familles de produits. Plus précisément, la méthodologie consiste à extraire des comportements de référence sous la forme de règles d'association et à en évaluer l'intérêt économique et l'actionnabilité. Les recommandations sont réalisées en ciblant les contre-exemples les plus actionnables sur les règles les plus rentables. Notre méthodologie est appliquée sur 12 000 clients et 100 000 produits de VMMatériaux afin d'orienter les commerciaux sur les possibilités d'accroissement de la valeur client.
**** *annee_2011  *numText_369
Récemment de nouvelles techniques regroupées sous le vocable de détection automatique d'opinions (opinion mining) ont fait leur apparition et proposent une évaluation globale d'un document. Ainsi, elles ne permettent pas de mettre en avant le fait que les personnes expriment une opinion très positive du scénario d'un film alors qu'elles trouvent que les acteurs sont médiocres. Dans cet article, nous proposons de caractériser automatiquement les segments de textes relevant d'un critère donné sur un corpus de critiques.
**** *annee_2011  *numText_370
Nous proposons dans cet article une nouvelle méthode de classification hiérarchique et topologique. Notre approche consiste à construire de manière auto-organisée une partition de données représentées par un ensemble forêt d'arbres répartis sur une grille 2D. Chaque cellule de la grille est modélisée par un arbre dont les noeuds représentent les données. La partition globale obtenue est visualisée à l'aide d'une carte de TreeMap dans laquelle chaque TreeMap représente un arbre de données. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Des résultats numériques et visuels seront présentés et discutés.
**** *annee_2011  *numText_371
L'essor récent des technologies associées à la géomatique a permis la production rapide de nombreuses données géographiques. Or, pour tirer profit de ces données, il convient de pouvoir évaluer leur pertinence et leur complexité vis à vis de l'application à laquelle on les destine. Dans cet article, nous présentons une application permettant à un utilisateur de découvrir le contenu de bases de données géographiques, à savoir, quels types d'entités géographiques sont représentés au sein de chaque base et comment. Pour accéder à ces informations l'utilisateur interroge le système via une ontologie globale du domaine qui décrit les types d'entités topographiques du monde réel. Des ontologies locales ou d'application sont utilisées pour formaliser les spécifications de chaque base de données décrite. Elles sont annotées à l'aide de concepts issus de l'ontologie globale. Ce système est implémenté sous la forme d'une interface Web et inclut un affichage cartographique d'échantillons de données
**** *annee_2011  *numText_373
En établissant des relations entre des concepts issus de deux ontologies distinctes, les outils d'alignement peuvent être utilisés pour enrichir une des deux ontologies avec les concepts de l'autre. A partir d'une expérience menée dans le cadre du projet ANR GeOnto  dans le domaine de la topographie, cet article identifie des traitements complémentaires à l'alignement pour l'enrichissement et montre leur mise en œuvre dans TaxoMap Framework.
**** *annee_2011  *numText_375
La croissance exponentielle des données engendre des volumétries de bases de données très importantes. Une solution couramment envisagée est l'utilisation d'une description condensée des propriétés et de la structure des données. De ce fait, il devient crucial de disposer d'outils de visualisation capables de représenter la structure des données, non pas à partir des données elles mêmes, mais à partir de ces descriptions condensées. Nous proposons une méthode de description des données à partir de prototypes enrichis puis segmentés à l'aide d'un algorithme adapté de classification non supervisée. Nous introduisons ensuite un procédé de visualisation capable de mettre en valeur la structure intra et inter-groupes des données.
**** *annee_2010  *numText_376
De nombreux systèmes de recommandation se focalisent sur les articles(que nous appellerons items) les plus populaires et ignorent souvent la longue traîne des produits qui le sont moins. Nous proposons l'algorithme AbsTop-kα qui améliore les recommandations en se basant sur la combinaison(pondérée par α) de paires hautement corrélées entre des abstractions d'items etentre des paires d'items concrets classiquement recherchées.
**** *annee_2010  *numText_378
Nous nous intéressons au problème de l'affichage de publicités sur le web. De plus en plus d'annonceurs souhaitent maintenant payer uniquement lorsque quelqu'un clique sur leurs publicités. Dans ce modèle, l'opérateur du portail a intérêt à identifier les publicités les plus cliquées, selon ses catégories de visiteurs. Comme les probabilités de clic sont inconnues a priori, il s'agit d'un dilemme exploration/exploitation. Ce problème a souvent été traité en ne tenant pas compte de contraintes provenant du monde réel : les campagnes de publicités ont une durée de vie et possèdent un nombre de clics à assurer et ne pas dépasser.Pour cela, nous introduisons une approche hybride (MAB+LP) entre la programmation linéaire et les bandits. Nos algorithmes sont testés sur des modèles créés avec un important acteur du web commercial. Ces expériences montrent que ces approches atteignent une performance très proche de l'optimum et mettent en évidence des aspects clés du problème.
**** *annee_2010  *numText_380
La maintenance de trains est un problème particulièrement délicat lié à de nombreux enjeux à la fois financiers, sécuritaires et énergétiques. Nous nous intéressons à la mise en place d'une maintenance préventive basée sur la détection et la correction de tout comportement anormal susceptible de provoquer un problème majeur dans un futur proche. Nous proposons ainsi un outil d'aide à la décision afin de (i) dégager des connaissances utiles sur l'historique des trains,et (ii) détecter et étudier les anomalies comportementales, dans le but de prendre des décisions optimales en termes de maintenance ferroviaire
**** *annee_2010  *numText_381
Dans cet article, nous étudions la relation entre la découverte de motifs sous contraintes et les CSPs (Constraint Satisfaction Problems) afin de définir des contraintes de plus haut niveau qui sont précieuses pour mener à bien des tâches de fouille de données. Pour cela, nous proposons une approche de modélisation et d'extraction de motifs sous contraintes n-aires exploitant les motifs locaux. L'utilisateur définit un ensemble de contraintes n-aires et un solveur de CSP génère l'ensemble des solutions. Notre approche profite des progrès récents sur l'extraction de motifs locaux et permet de modéliser de manière concise et élégante tout ensemble de contraintes combinant plusieurs motifs locaux, permettant ainsi la découverte de motifs répondant mieux aux buts finaux de l'utilisateur.Les expériences menées montrent la faisabilité de notre approche.
**** *annee_2010  *numText_382
L'utilisation des documents pédagogiques, disponibles sur le web,devient de plus en plus large tant pour l'enseignant qui a besoin de préparer son support de cours que pour l'étudiant qui désire, par exemple, s'autoformer. La description d'un document pédagogique, en l'alimentant par des métadonnées, s'avère une solution qui confère une valeur ajoutée au document afin d'expliciter des informations placées dans ce document. Dans cette optique, nous proposons une méthode d'annotation de documents pédagogiques selon différents points de vue, qui est basée sur l'analyse sémantique des éléments discursifs du texte
**** *annee_2010  *numText_384
Les entrepôts de données et l'analyse en ligne OLAP (On-line AnalysisProcessing) présentent des solutions reconnues et efficaces pour le processus d'aide à la décision. Notamment l'analyse en ligne, grâce aux opérateurs OLAP,permet de naviguer et de visualiser des données représentées dans un cube multidimensionnel.Mais lorsque les données ou les objets à analyser sont complexes,il est nécessaire de redéfinir et d'enrichir ces opérateurs OLAP. Dans cet article,nous proposons de combiner l'analyse OLAP et la fouille de données (data mining)afin de créer un nouvel opérateur de visualisation d'objets complexes. Cet opérateur utilise l'analyse factorielle des correspondances.
**** *annee_2010  *numText_385
Les systèmes de vidéo-surveillance sont de plus en plus autonomes dans la détection des événements anormaux. Cet article présente une méthode de détection des flux majeurs et des événements qui surviennent dans une scène de foule. Ces détections sont effectuées en utilisant un modèle directionnel construit à partir d'un mélange de lois de von Mises appliqué à l'orientation des vecteurs de mouvement. Les flux majeurs sont alors calculés en récupérant les orientations les plus importantes des mélanges. Divers événements se produisant dans une foule sont aussi détectés en utilisant en plus du modèle d'orientation, un modèle probabiliste de magnitude des vecteurs de mouvement. Les résultats de l'expérimentation sur un échantillon de vidéos d'événements sont présentés.
**** *annee_2010  *numText_389
Ce papier présente une approche d'apprentissage de patrons lexico-syntaxiques à partir de textes annotés. Les patrons lexico-syntaxiques sont utilisés pour identifier des relations lexicales dans les corpus textuels. Leur construction manuelle est une tâche fastidieuse et des solutions permettant l'apprentissage sont souhaitables. Nous proposons une approche d'apprentissage qui repose sur l'utilisation des chemins de dépendance pour représenter les patrons et l'implémentation d'un algorithme de classification. L'approche a été appliquée dans le domaine biomédical pour identifier des patrons lexico-syntaxiques exprimant des relations fonctionnelles.
**** *annee_2010  *numText_392
Face à la quantité sans cesse grandissante de données stockées, les algorithmes de fouille et de visualisation de données doivent pouvoir être capable de traiter de grandes quantités de données.Une des solutions est d'effectuer un prétraitement des données permettant la réduction de la dimension des données sans perte significative d'informations. L'idée est donc de réduire l'ensemble de descripteurs avant de faire appel à la méthode de visualisation sous forme d'un graphe.
**** *annee_2010  *numText_393
Cet article présente une méthode complexe pour la caractérisation et l'indexation d'images graphiques de documents anciens. A partir d'un bref état de l'art, une méthode pour décrire ces images en tenant compte de leur complexité est proposée. Trois étapes principales de ce traitement sont détaillées dont une méthode novatrice d'analyse, de segmentation et de description des traits. Les résultats sont issus de travaux en cours et sont encourageants
**** *annee_2010  *numText_395
La masse des données aujourd'hui disponibles engendre des besoins croissants de méthodes décisionnelles adaptées aux données traitées. Ainsi, récemment de nouvelles approches fondées sur des cubes de textes sont apparues pour pouvoir analyser et extraire de la connaissance à partir de documents. L'originalité de ces cubes est d'étendre les approches traditionnelles des entrepôts et des technologies OLAP à des contenus textuels. Dans cet article, nous nous intéressons à deux nouvelles fonctions d'agrégation. La première propose une nouvelle mesure de TF-IDF adaptative permettant de tenir compte des hiérarchies associées aux dimensions. La seconde est une agrégation dynamique permettant de faire émerger des groupements correspondant à une situation réelle. Les expériences menées sur des données issues du serveur HAL d'une université confirment l'intérêt de nos propositions.
**** *annee_2010  *numText_396
Internet est devenu une source importante d'informations médicales pour les patients et leurs proches : recherche d'informations sur leurs maladies et les dernières recherches cliniques, ainsi que pour y constituer des communautés numériques de dialogue et de partage. Cependant, accès à Internet ne signifie pas nécessairement accès à l'information. Le manque de familiarité avec le langage médical constitue un problème majeur pour les usagers de santé dans l'accès à l'information et son interprétation. Ce papier s'inscrit dans la problématique d'étude et de caractérisation de la terminologie des usagers de santé pour pouvoir proposer des services adaptés à leur langage et à leur niveau de connaissances. Le travail réalisé est une ontologie dans le domaine du cancer du sein orientée vers les usagers de santé. Cette ontologie est construite à partir d'un ensemble de corpus de textes représentant deux catégories : les médiateurs et les usagers de santé. Les éléments de cette ontologie ont été analysés en utilisant des méthodes quantitatives et qualitatives sur plusieurs niveaux : termes,concepts et relations.
**** *annee_2010  *numText_397
Nous présentons, dans ce papier, l'outil CARTOCEL (CARTOgraphiesCELlulaires) permettant une visualisation automatique et dynamique des domaines de connaissances. Le fonctionnement de CARTOCEL est basé sur une approche originale de modélisation booléenne de la cartographie des domaines de connaissances métiers/stratégiques inspirée du principe de la machine cellulaire CASI (Cellular Automata for Symbolic Induction). Le but,après une modélisation booléenne de la cartographie des domaines de connaissances,est double : d'une part affiner la cartographie par une fouille de donnée orchestrée par CASI, et d'autre part réduire la complexité de stockage, ainsi que le temps de calcul
**** *annee_2010  *numText_399
Cet article présente un ensemble d'outils destiné à analyser des séquences d'événements en sciences sociales et à visualiser les résultats obtenus.Nous commençons par formaliser la notion de séquence d'événements avant de définir une mesure de dissimilarité entre ces séquences afin de construire des typologies et de tester les liens entre ces séquences et d'autres variables d'intérêts.Initialement définie par Moen (2000), cette mesure se base sur la notion de distance d'édition entre séquences et permet d'identifier les différences d'ordonnancement et de temporalité des événements. Nous proposons une extension de celle-ci afin de pouvoir prendre en compte la simultanéité des événements ainsi qu'une méthode de normalisation qui garantit le respect de l'inégalité triangulaire.Dans un deuxième temps, nous présentons un ensemble d'outils destinés à interpréter les résultats. Nous proposons ainsi deux méthodes de visualisation d'un ensemble de séquences et nous introduisons la notion de sous-séquence discriminante qui permet d'identifier les différences d'ordonnancement des événements les plus significatives entre groupes. L'ensemble des outils présentés est disponible au sein de la librairie R TraMineR.
**** *annee_2010  *numText_400
La classification des documents numériques garantit un accès rapide et ciblé à l'information. Si nous considérons qu'un document est représenté par sa ou ses structures, définir des classes de documents revient à définir des classes de structures. Une classe structurelle représente donc des structures« proches ». Ainsi, associer la structure d'un document à sa classe structurelle revient à calculer une distance dite « structurelle ». Elle tiendra compte à la fois de l'organisation des éléments (position des nœuds, chemin), du coût d'adaptation des représentants des classes ainsi que de la représentativité dessous-graphes. Sur un corpus de documents représentant des notices de livres issus de la bibliothèque de l'université, nous discuterons de la construction de cette distance, de l'intérêt de chacun des trois paramètres utilisés
**** *annee_2010  *numText_401
Le besoin récent de nombreuses applications multimédia basées sur le contenu a engendré une demande croissante de technologies dans le domaine de la recherche d'information multimédia. Basée sur l'état de l'art des techniques existantes, nous proposons dans cet article une approche de recherche d'information multimédia qui prend en compte les informations de scène et exploite un modèle de sélection de caractéristiques. Les principaux avantages de notre modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de sélection de caractéristiques; (iii) un index multidimensionnel. Notre framework propose un bon compromis entre précision et rapidité de la recherche
**** *annee_2010  *numText_402
Dans la phase de préparation des données du data mining, les méthodes de discrétisation et de groupement de valeurs supervisé possèdent de nombreuses applications : interprétation, estimation de densité conditionnelle,sélection de type filtre des variables, recodage des variables en amont des classifieurs. Ces méthodes supposent habituellement un faible nombre de valeur à expliquer (classes), typiquement moins d'une dizaine, et trouvent leur limite quand leur nombre augmente. Dans cet article, nous introduisons une extension des méthodes de discrétisation et groupement de valeurs, consistant à partitionner d'une part la variable explicative, d'autre part la variable à expliquer.Le meilleur co-partitionnement est recherché au moyen d'une approche Bayesienne de la sélection de modèle. Nous présentons ensuite comment utiliser cette méthode de prétraitement en préparation pour le classifieur Bayesien naïf. Des expérimentations intensives démontrent l'apport de la méthode dans le cas de centaines de classes.
**** *annee_2010  *numText_403
Le calcul des cubes de données est excessivement coûteux aussi bien en temps d'exécution qu'en mémoire et son stockage sur disque peut s'avérer prohibitif. Plusieurs efforts ont été consacrés à ce problème à travers les cubes fermés, où les cellules préservant la sémantique d'agrégation sont réduites à une cellule, sans perte d'information. Dans cet article, nous introduisons le concept du cube de données non-dérivable fermé, nommé CND-Cube, qui généralise la notion des modèles non-dérivables fermés fréquents bidimensionnels à un contexte multidimensionnel. Nous proposons un nouvel algorithme pour extraire le CND-Cube à partir des bases de données multidimensionnelles en se basant sur trois contraintes anti-monotones, à savoir être fréquent, être non dérivable et être un générateur minimal. Les expériences montrent que notre proposition fournit la représentation la plus concise d'un cube de données et elle est ainsi la plus efficace pour réduire l'espace de stockage.
**** *annee_2010  *numText_404
L'écriture logosyllabique des anciens Mayas comprend plus de 500signes et est en bonne partie déchiffrée, avec des degrés de certitude divers.Nous avons appliqué au codex de Dresde, l'un des trois seuls manuscrits qui nous soient parvenus, codé sous LATEX avec le système mayaTEX, notre méthodede représentation graduée, par apprentissage non supervisé hybride entre clusteringet analyse factorielle oblique, sous la métrique de Hellinger, afin d'obtenir une image nuancée des thèmes traités : les individus statistiques sont les 212segments de folio du codex, et leurs attributs sont les 1687 bigrammes de signes extraits. Pour comparaison, nous avons introduit dans cette approche endogène un élément exogène, la décomposition en éléments des signes composites, pour préciser plus finement les contenus. La rétro-visualisation dans le texte original des résultats et expressions dégagées éclaire la signification de certains glyphes peu compris, en les situant dans des contextes clairement interprétables.
**** *annee_2010  *numText_406
Avec l'avènement d'applications sociales en entreprise (blogs, wikis,etc.), il est fréquent que des individus aux niveaux d'expertise relativement distants se réunissent au sein de communautés en ligne. Ces disparités d'expertise se traduisent entre autres par des comportements différents dans la manière de tagguer les contenus créés, notamment en ce qui concerne les termes utilisés,rendant ainsi complexe la découverte d'informations pourtant publiées. Dans cet article, nous mettons en avant la possibilité offerte par les technologies du Web Sémantique, combinées avec les paradigmes du Web Social, de résoudre cette problématique. Nous proposons ainsi une chaîne de traitement combinant ontologies,wikis sémantiques et indexation de contenus permettant la production de graphes sémantiques interconnectés et facilitant de cette manière la découverte de contenus créés au sein de tels systèmes
**** *annee_2010  *numText_407
L'utilisation de connaissances pour améliorer les processus de fouille de données a mobilisé un important effort de recherche ces dernières années. Ilest cependant souvent difficile de formaliser ce type de connaissances, comme celles-ci sont souvent dépendantes du domaine. Dans cet article, nous nous intéressons à l'intégration de connaissances sous la forme d'objets étiquetés dans les algorithmes de clustering. Plusieurs critères permettant d'évaluer la pureté des clusters sont présentés et leur comportement est comparé sur des jeux de données artificiels. Les avantages et les inconvénients de chaque critère sont analysés pour aider l'utilisateur à faire un choix.
**** *annee_2010  *numText_408
Dans le domaine de la fouille de données, mesurer les similitudes entre différents sous-ensembles est une question importante qui a été peu étudiée jusqu'à présent. Dans cet article, nous proposons une nouvelle méthode basée sur l'apprentissage non-supervisé. Les différents sous-ensembles à comparer sont caractérisés au moyen d'un modèle à base de prototypes. Ensuite, les différences entre les modèles sont détectées en utilisant une mesure de similarité
**** *annee_2010  *numText_409
Nous proposons dans cet article une première approche qui consiste à exploiter les réseaux sociaux afin de faciliter la composition de services parles utilisateurs finaux. Nous introduisons un Framework, nommé Social Composer(SoCo), qui implémente cette approche. SoCo fournit à l'utilisateur des recommandations dynamiques de services basées entre autre sur le réseau social de l'utilisateur qui est construit implicitement à partir des interactions entre les utilisateurs, les services, les différentes compositions opérées par les membres du réseau social, ainsi que le réseau social global.
**** *annee_2010  *numText_410
Nous montrons qu'un ensemble d'arbres de décision avec une composante aléatoire permet de construire un noyau efficace destiné à l'apprentissage supervisé. Nous étudions théoriquement les propriétés d'un tel noyau et montrons que sous des conditions très souvent rencontrées en pratique, il existe une séparabilité linéaire entre exemples de classes distinctes dans l'espace induit par celui-ci. Parallèlement, nous observons également que le classique vote à la majorité d'un ensemble d'arbres est un hyperplan (sans garantie d'optimalité) dans l'espace induit par le noyau. Enfin, comme le montrent nos expérimentations,l'utilisation conjointe d'un ensemble d'arbres et d'un séparateur à vaste marge(SVM) aboutit à des résultats extrêmement encourageants.
**** *annee_2010  *numText_411
Le concept de Cube Émergent a été introduit afin de comparer deux data cubes. Dans cet article, nous introduisons deux nouvelles représentations réduites du Cube Émergent sans perte des mesures : le Cube Fermé Émergent et le Cube Quotient Émergent. La première représentation est basée sur le concept de fermeture cubique. C'est la plus petite représentation possible du cube de données émergent. À partir du Cube Fermé Émergent et donc en stockant le minimum d'informations, il est possible de répondre efficacement aux requêtes qui peuvent être exécutées sur le Cube Émergent lui-même. La seconde représentation s'appuie sur la structure du Cube Quotient qui a été proposé pour résumer un cube de données. Le Cube Quotient est revisité afin de le doter d'une sémantique basée sur la fermeture cubique et donc adapté au contexte du Cube Émergent. Le Cube Quotient Émergent résultant est moins réduit que le Cube Fermé Émergent mais il préserve la propriété de spécialisation/généralisation du data cube qui permet la navigation au sein du Cube Émergent. Nous établissons également le lien entre les deux représentations introduites et celle basée sur les bordures classiques en fouille de données. Des expérimentations effectuées sur divers jeux de données visent à comparer la taille des différentes représentations.
**** *annee_2010  *numText_413
Les Dépendances Fonctionnelles Conditionnelles (DFC) ont été introduites en 2007 pour le nettoyage des données. Elles peuvent être considérées comme une unification de Dépendances Fonctionnelles (DF) classiques et de Règles d'Association (RA) puisqu'elles permettent de spécifier des dépendances mixant des attributs et des couples de la forme attribut/valeur.Dans cet article, nous traitons le problème de la découverte des DFC, i.e. Déterminer une couverture de l'ensemble des DFC satisfaites par une relation r. Nous montrons comment une technique connue pour la découverte des DF (exactes et approximatives) peut être étendue aux DFC. Cette technique a été implémentée et des expériences ont été menées pour montrer la faisabilité et le passage à l'échelle de notre proposition.
**** *annee_2010  *numText_414
Dans ce papier nous proposons PLCM, un algorithme parallèle de découverte d'itemsets fréquents fermés basé sur l'algorithme LCM, reconnu comme l'algorithme séquentiel le plus efficace pour cette tâche. Nous présentons aussi une interface de parallélisme à la fois simple et puissante basée sur la notion de Tuple Space, qui permet d'avoir une bonne répartition dynamique du travail.Grâce à une étude expérimentale détaillée, nous montrons que PLCM est le seul algorithme qui soit suffisamment générique pour calculer efficacement des itemsets fréquents fermés à la fois sur des bases creuses et sur des bases denses,améliorant ainsi l'état de l'art.
**** *annee_2010  *numText_419
Dans cet article, nous présentons différentes variantes GMM-SMOs pour l'identification du locuteur en mode indépendant du texte. Pour mettre en œuvre les différents systèmes, nous avons opté une représentation multi-gaussienne de l'espace des caractéristiques basées sur l'algorithme Expectation Maximisation (EM). Ces nouvelles représentations constituent les vecteurs d'entrés pour entraîner les supports vecteurs machines (SVMs) par l'algorithme de type Optimisation par Minimisation Séquentielle (SMO).
**** *annee_2010  *numText_422
Cet article traite un problème dans le domaine de la gestion des bases de données classiques. Il s'agit d'exploiter une ontologie de domaine pour aider l'utilisateur d'une base de données relationnelle dans sa recherche et de lui permettre une interrogation transparente de la base de données. Pour cela, nous proposons une approche d'expansion automatique de requêtes SQL lorsque celles-ci n'ont pas de réponses. Notre approche est décrite par un algorithme défini de manière générique afin d'être utilisé pour une base de données quelconque.
**** *annee_2010  *numText_423
L'objectif des systèmes d'intégration de données est de faciliter l'exploitation et l'interprétation d'informations hétérogènes provenant de différentes sources. Lorsque l'on doit intégrer de grands volumes de données, le recours à un expert n'est pas envisageable mais l'exploitation de processus d'intégration automatiques peut introduire des approximations ou des erreurs. Nous nous focalisons sur les résultats fournis par les méthodes de réconciliation de données. Ces dernières comparent les données entre elles et détectent celles qui réfèrent à la même entité du monde réel. Pour renforcer la confiance des utilisateurs dans les résultats retournés par ces méthodes, nous proposons dans cet article une approche d'explication graphique fondée sur les réseaux de Petri colorés qui est particulièrement adaptée aux approches de réconciliation globales, numériques et guidées par une ontologie.
**** *annee_2010  *numText_426
La découverte automatique de règles et motifs graduels (plus l'âge d'une personne est élevé, plus son salaire est élevé) trouve de très nombreuses applications sur des bases de données réelles (e.g. biologie, flots de données de capteurs). Si des algorithmes de plus en plus efficaces sont proposés dans des articles récents, il n'en reste pas moins que ces méthodes génèrent un nombre de motifs tellement important que les experts peinent à les exploiter. Dans cet article, nous proposons donc une représentation condensée des motifs graduels en introduisant les concepts théoriques associés aux opérateurs de fermeture sur de tels motifs.
**** *annee_2010  *numText_427
La recherche de liens entre objets fréquents a été popularisée par les méthodes d'extraction de règles d'association. Dans le cas de séquences d'événements,les méthodes de fouille permettent d'extraire des sous-séquences qui peuvent ensuite être exprimées sous la forme de règles d'association séquentielle entre événements. Cette utilisation de la fouille de séquences pour la recherche de liens entre des événements pose deux problèmes. Premièrement, le critère principal utilisé pour sélectionner les sous-séquences d'événements est la fréquence, or les occurrences de certains événements peuvent être fortement liées entre elles même lorsqu'elles sont peu fréquentes. Deuxièmement, les mesures actuelles utilisées pour caractériser les règles d'association ne tiennent pas compte du caractère temporel des données, comme l'importance du timing des événements ou le problème des données censurées. Dans cet article, nous proposons une méthode pour rechercher des liens significatifs entre des événements à l'aide de modèles de durée. Les règles d'association sont construites à partir des motifs séquentiels observés dans un ensemble de séquences. L'influence sur le risque que l'événement « conclusion » se produise après le ou les événements « prémisse » est estimée à l'aide d'un modèle semi-paramétrique à risques proportionnels.Outre la présentation de la méthode, l'article propose une comparaison avec d'autres mesures d'association
**** *annee_2010  *numText_428
Dans cet article, nous abordons la problématique d'extraction de séquences fréquentes à partir de corpus de textes parallèles en prenant en compte l'ordre d'apparition des mots dans une phrase. Notre finalité est d'exploiter ces séquences dans la traduction automatique (TA). Nous introduisons ainsi la notion de règles associatives inter-langues (RAIL) et nous définissons notre modèle de traduction à base de ces associations. Nous décrivons également les différentes expérimentations conduites sur le corpus EUROPARL afin de construire à partir des RAIL une table de traduction bilingue qui est intégrée par la suite dans un processus complet de TA.
**** *annee_2010  *numText_429
L'extraction d'itemsets distinctifs est un sujet de recherche récent qui connait plusieurs algorithmes pour les données statiques (Knobbe et Ho, 2006;Heikinheimo et al., 2007). Ces solutions ne sont toutefois pas conçues pour le cas des flux de données, pour lesquels les temps de réponse doivent être aussi faibles que possible. Nous considérons le problème de l'extraction d'itemsets distinctifs dans les flux, qui peut avoir de nombreuses applications dans la sélection de variables, la classification ou encore la recherche d'information. Nous proposons l'heuristique IDkF (Itemsets Distinctifs dans les Flux) et des résultats  d'expérimentations en comparaison d'une technique de la littérature.
**** *annee_2010  *numText_430
La fouille visuelle de données (ou Visual Data Mining, VDM) a pour objectif de faciliter l'interprétation des résultats issus d'une fouille de données,grâce à l'usage de représentations graphiques. Au cours de la dernière décennie,un grand nombre de techniques de visualisation d'information ont été mises au point, permettant la visualisation de données multidimensionnelles dans des environnements virtuels. Lors des travaux antérieurs, les chercheurs ont proposé des taxonomies pour classer les techniques de VDM (Chi (2000), Herman et al.(2000)). Toutefois, ces taxonomies ne prennent en compte que partiellement les techniques récentes relatives à l'utilisation de la 3D et de la réalité virtuelle. Le but de cet article est de faire un état de l'art récent et spécifique à ces techniques.Celles-ci sont détaillées, classées et comparées selon différents critères : les applications,l'encodage graphique, les techniques d'interaction, les avantages et les inconvénients de chaque approche. Ces techniques sont présentées dans des tableaux accompagnées d'illustrations graphiques
**** *annee_2010  *numText_431
Dans cet article nous proposons une approche de la gestion des droits d'accès pour les systèmes de gestion de contenu qui reposent sur les modèles et techniques du web sémantique. Nous présentons l'ontologie AMO qui consiste (1) en un ensemble de classes et propriétés permettant d'annoter les ressources dont il s'agit de contrôler l'accès et (2) en une base de règles d'inférence modélisant la stratégie de gestion des droits à mettre en œuvre. Appliquées sur la base d'annotations des ressources, ces règles permettent de gérer les ressources selon une stratégie donnée. Cette modélisation garantit ainsi l'adaptabilité de l'ontologie à différentes stratégies de gestion des droits d'accès. Nous illustrons l'utilisation de l'ontologie AMO sur les documents du projet ANR ISICIL produits par le wiki sémantique SweetWiki. Nous montrons comment les documents sont annotés avec AMO, quelles règles sont mises en oeuvre et quelles requêtes permettent le contrôle de l'accès aux documents.
**** *annee_2010  *numText_433
L'inférence des dépendances fonctionnelles est l'une des problématiques les plus étudiées en bases de données. Elle a fait l'objet de plusieurs travaux qui ont proposé des algorithmes afin d'inférer, efficacement, les dépendances fonctionnelles pour les utiliser dans différents domaines : administration de bases de données, ré-ingénierie, optimisation des requêtes,etc. Toutefois,pour les application réelles, les bases de données sont évolutives et les relations sont fréquemment augmentées ou diminuées de tuples. Par conséquent, afin de s'adapter à ce cadre dynamique, une solution consiste à appliquer l'un des algorithmes,disponibles dans la littérature, pour inférer les dépendances fonctionnelles,après chaque mise à jour. Cette solution étant coûteuse, nous proposons,dans cet article, d'inférer les dépendances fonctionnelles d'une manière incrémentale.À cet effet, nous introduisons un nouvel algorithme, appelé INCFDS, et nous évaluons ses performances par rapport à l'approche classique d'inférence des dépendances fonctionnelles à partir d'une relation dynamique.
**** *annee_2010  *numText_434
Nous présentons un nouvel algorithme incrémental et parallèle d'analyse factorielle des correspondances (AFC) pour la recherche d'images à grande échelle en utilisant le processeur de la carte graphique (GPU). L'AFC est adaptée à la recherche d'images par le contenu en utilisant des descripteurs locaux des images (SIFT). L'AFC permet de réduire le nombre de dimensions et de découvrir des thèmes qui permettent de diminuer le nombre d'images à parcourir et donc le temps de réponse d'une requête. Pour traiter de très grandes bases d'images, nous présentons une version incrémentale et parallèle d'AFC, puis nous utilisons ses indicateurs pour construire des fichiers inversés pour retrouver les images contenant les mêmes thèmes que l'image requête.Cette étape est elle aussi parallélisée sur GPU pour obtenir des réponses rapides. Les résultats numériques sur la base de données d'images Nistér-Stewénius plongée dans 1 million d'images de FlickR montrent que notre algorithme incrémental et parallèle est très significativement plus rapide que sa version standard
**** *annee_2010  *numText_435
Cet article1 propose un nouvel indice de la complexité de séquences catégorielles. Bien que conçu pour des séquences représentant des trajectoires biographiques telles que celles rencontrées dans les sciences sociales, il s'applique à tous types de listes ordonnées d'états. L'indice prend en compte deux aspects distincts, soit la complexité induite par l'ordonnancement des états successifs qui est mesurée par le nombre de transitions (changements d'état) et la complexité liée à la distribution des états dont rend compte l'entropie.
**** *annee_2010  *numText_437
Cet article montre que si l'on dispose d'une connaissance a priori sur le problème en main, l'intégration de cette dernière dans le processus d'apprentissage d'une machine intelligente pour des tâches de classification peut améliorer la performance de cette machine. Nous étudions l'effet de l'intégration de la connaissance a priori de convexité sur le processus d'apprentissage du principe du Maximum d'Entropie (MaxEnt) en utilisant des exemples virtuels. Nous testons les idées proposées sur un problème benchmark bien connu dans la littérature des machines d'apprentissage, le problème de formes d'ondes de Breiman. Nous avons abouti à un taux d'erreur de généralisation de 15.57% qui est très proche du taux d'erreur théorique estimé par Breiman (14%).
**** *annee_2010  *numText_438
Il existe aujourd'hui de nombreuses méthodes de réduction de dimensions,que ce soit dans un cadre supervisé ou non supervisé. L'un des intérêts de ces méthodes est de pouvoir visualiser les données, avec pour objectif que les objets qui apparaissent visuellement proches soient similaires, dans un sens qui correspond aux connaissances d'un expert du domaine ou qui soit conforme aux informations de supervision. Nous nous plaçons ici dans un contexte semi supervisé où des connaissances sont ajoutées de façon interactive : ces informations seront apportées sous forme de contraintes exprimant les écarts entre la représentation observée et les connaissances d'un expert. Nous pourrons par exemple spécifier que deux objets proches dans l'espace d'observation sont en fait peu similaires, ou inversement. La méthode utilisée ici dérive de l'analyse en composantes principales (ACP), à laquelle nous proposons d'intégrer deux types de contraintes. Nous présentons une méthode de résolution qui a été implémentée dans un logiciel offrant une représentation 3D des données et grâce auquel l'utilisateur peut ajouter des contraintes de manière interactive, puis visualiser les modifications induites par ces contraintes. Deux types d'expérimentation sont présentés, reposant respectivement sur un jeu de données synthétique et sur des jeux standards : ces tests montrent qu'une représentation de bonne qualité peut être obtenue avec un nombre limité de contraintes ajoutées.
**** *annee_2010  *numText_439
Les systèmes de gestion de flux de données (SGFD) ont été conçus afin de traiter une masse importante de données produites en ligne de façon continue. Étant donné que les ressources matérielles ne permettent pas de conserver toute cette volumétrie, seule la partie récente du flux est mémorisée dans la mémoire du SGFD. Ainsi, les requêtes évaluées par ces systèmes ne peuvent porter que sur les données les plus récentes du flux. Par conséquent, les SGFD actuels ne peuvent pas traiter des requêtes qui portent sur des périodes très longues.Nous proposons dans cet article, une approche permettant d'évaluer des requêtes qui portent sur une période plus longue que la mémoire du SGFD. Ces fenêtres font appels à des données récentes et des données historisées. Nous présentons le niveau logique de cette approche ainsi que son implantation sous le SGFD Esper. Une technique d'échantillonnage associée à une technique de fenêtre point de repère est appliquée pour conserver une représentation compacte des données du flux.
**** *annee_2010  *numText_441
Cet article présente la machine abstraite de graphes de connaissance KGRAM qui unifie les notions d'homomorphisme de graphe et de calcul de requêtes telles que celles du langage SPARQL sur des données RDF. KGRAM implémente un ensemble extensible d'expressions qui définissent une famille de langages abstraits d'interrogation de graphes, GRAAL. Nous décrivons la sémantique dynamique de GRAAL en Sémantique Naturelle et nous présentons la machine abstraite KGRAM conçue comme l'interprète de GRAAL, qui implémente les règles de sémantique naturelle du langage.
**** *annee_2010  *numText_442
Le conflit apparaît naturellement lorsque plusieurs sources d'informations imparfaites sont en jeu. La théorie des fonctions de croyance offre un formalisme adapté à la fusion d'informations dans lequel la considération du conflit est centrale. Ce travail propose de revenir sur les différentes définitions du conflit dans cette théorie, tentant de les synthétiser et de montrer comment supprimer ce conflit, ou bien comment en tenir compte lors de la combinaison des informations.
**** *annee_2010  *numText_443
La majorité des modèles de langue appliqués à la recherche d'information repose sur l'hypothèse d'indépendance des mots.Plus précisément, ces modèles sont estimés à partir des mots simples apparaissant dans les documents sans considérer les éventuelles relations sémantiques et conceptuelles. Pour pallier ce problème, deux grandes approches ont été explorées : la première intègre des dépendances d'ordre surfacique entre les mots, et la seconde repose sur l'utilisation des ressources sémantiques pour capturer les dépendances entre les mots. Le modèle de langue que nous présentons dans cet article s'inscrit dans la seconde approche.Nous proposons d'intégrer les dépendances entre les mots en représentant les documents et les requêtes par les concepts.
**** *annee_2010  *numText_444
XML étant devenu omniprésent et ses techniques de stockage et d'interrogation de plus en plus efficaces, le nombre de cas d'utilisations de ces technologies augmente tous les jours. Un sujet prometteur est l'intégration d'XML et des entrepôts de données, dans laquelle une base de données XML native stocke les données multidimensionnelles et exécute des requêtes OLAP écrites à l'aide du langage d'interrogation XML XQuery. Ce papier explore les questions qui peuvent survenir lors de l'implémentation d'un tel entrepôt de données XML.
**** *annee_2010  *numText_446
On sait bien que la confiance des règles d'association n'est pas vraiment satisfaisant comme mesure d'interêt. Nous proposons, au lieu de la substituer par des autres mesures (soit, en l'employant de façon conjointe a des autres mesures), évaluer la nouveauté de chaque règle par comparaison de sa confiance par rapport á des règles plus fortes qu'on trouve au même ensemble de données. C'est á dire, on considère un seuil relative de confiance au lieu du seuil absolute habituel. Cette idée se précise avec la magnitude du confidence boost, mesurant l’incrément relative de confiance prés des règles plus fortes.Nous prouvons que nôtre proposte peut remplacer la confidence width et le blockage de règles employés a des publications précédentes.
**** *annee_2010  *numText_447
Les modèles de classification recouvrante ont montré leur capacité à générer une organisation plus fidèle aux données tout en conservant la simplification attendue par une structuration en classes strictes. Par ailleurs les modèles neuronaux non-supervisés sont plébiscités lorsqu'il s'agit de visualiser la structure de classes.Nous proposons dans cette étude d'étendre les cartes auto-organisatrices traditionnelles aux cartes auto-organisatrices recouvrantes. Nous montrons que cette nouvelle structure apporte des solutions à certaines problématiques spécifiques en classification recouvrante (nombre de classes, complexité, cohérence des recouvrements).L'algorithme OSOM s'inspire de la version recouvrante des nuées dynamiques et de l'approche de Kohonen pour générer de telles cartes recouvrantes. Nous discutons du modèle proposé d'un point de vue théorique (fonction d'énergie associée, complexité, ...). Enfin nous présentons un cadre d'évaluation générale que nous utilisons pour valider les résultats obtenus sur des données réelles.
**** *annee_2010  *numText_449
Les règles d'association cycliques vise la découverte de nouvelles relations entre des produits qui varient d'une façon régulièrement cyclique dans le temps. Dans ce cadre, nous introduisons, un nouvel algorithme nommé PCAR caractérisé par sa performance et son aspect incrémental. L'étude empirique quenous avons menée montre la robustesse et l'efficacité de notre algorithme proposé vs. ceux de la littérature
**** *annee_2010  *numText_450
Initialement utilisés pour les systèmes de commande, les règles et motifs graduels (de la forme plus une personne est âgée, plus son salaire est élevé)trouvent de très nombreuses applications, par exemple dans les domaines de la biologie, des données en flots (e.g. issues de réseaux de capteurs), etc. Très récemment, des algorithmes ont été proposés pour extraire automatiquement de tels motifs. Cependant, même si certains d'entre eux ont permis des gains de performance importants, les algorithmes restent coûteux et ne permettent pas de traiter efficacement les bases de données réelles souvent très volumineuses(en nombre de lignes et/ou nombre d'attributs). Nous proposons donc dans cet article une méthode originale de recherche de ces motifs utilisant le multi-threading pour exploiter au mieux les multiples cœurs présents dans la plupart des ordinateurs et serveurs actuels. L'efficacité de cette approche est validée par une étude expérimentale.
**** *annee_2010  *numText_452
PretopoLib est une librairie JAVA implémentant les concepts de la prétopologie. Son intérêt réside dans la représentation de structures de données permettant la manipulation des données par des opérations ensemblistes.Celle-ci offre un cadre de développement d'algorithmes efficaces pour la fouillede données, l'apprentissage topologique et la modélisation des systèmes complexes.
**** *annee_2010  *numText_454
La classification associative est une méthode de prédiction à base de règles issue de la fouille de règles d'association. Cette méthode est particulièrement intéressante car elle recherche de façon exhaustive les règles d'association pertinentes qu'elle filtre pour ne garder que les règles d'association de classe(celles admettant pour conséquent une modalité de classe), qui sont utilisées comme classifieur. Les connaissances produites sont ainsi directement interprétables.Des études antérieures montrent les inconvénients de cette approche,qu'il s'agisse de la génération massive de règles non utilisées ou de la mauvaise prédiction de la classe minoritaire lorsque les classes sont déséquilibrées.Nous proposons une approche originale du type boosting de règles d'association de classes qui utilise comme classifieur faible une base de règles significatives construites par un algorithme de génération d'itemsets fréquents qui se limite à l'extraction des seules règles de classe significatives et qui prend en compte le déséquilibre des données. Des comparaisons avec d'autres méthodes de classification associative montrent que notre approche améliore la précision et le rappel.
**** *annee_2010  *numText_455
Protein Graph Repository (PGR) est i, outil bioinformatique sur le web permettant d'obtenir une nouvelle représentation de protéines sous la forme de graphes d'acides aminés, une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes. La génération des graphes est faite à partir d'un parseur appliqué sur des fichiers des protéines PDB extraits de la base Protein Data Bank et en précisant les paramètres et la méthode a utiliser. Les graphes générés sont ensuite enregistres dans un entrepôt doté de moyens de recherche, de filtrage et de téléchargement. PGR peut etre provisoirement consulte à l'adresse http://www.enode-edition.com/pgr/, il est spécialement dédié aux recherches intéressées à l'étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux.
**** *annee_2010  *numText_457
Dans ce papier, nous présentons une approche de recherche sémantique basée sur les ontologies modulaires et le raisonnement à base de cas (RaPC). Un cas représente l'ensemble des requêtes similaires associées à leurs résultats pertinents. Les ontologies modulaires sont utilisées pour représenter et indexer les cas qui sont construits sur la base des requêtes antérieures et les résultats pertinents sélectionnés par les utilisateurs. La similarité à base d'ontologies est utilisée pour retrouver les cas similaires à la requête utilisateur et pour fournir à celui-ci des propositions de reformulation de requêtes correspondants à son besoin. La principale contribution de ce travail réside dans l'utilisation d'un mécanisme de RaPC et une représentation ontologique à deux fins: l'amélioration de la recherche sémantique et l'enrichissement d'ontologies à partir de cas. L'expérimentation de l'approche proposée montre que la précision et le rappel des résultats se sont nettement améliorés.
**** *annee_2010  *numText_459
Inspiré des performances du cerveau humain à identifier les éléments par la vue, le problème de la réduction de la dimension dans le domaine de la perception visuelle consiste à extraire une quantité réduite des caractéristiques d'un ensemble d'images afin de les identifier.Ce papier présente une approche innovante bi-directionnelle d'extraction de caractéristiques d'images fondée sur l'utilisation partielle d'une méthode spatio-temporelle.Les expériences numériques appliquées sur 70000 images représentant des chiffres écrits à la main ainsi que sur 698 images illustrant un visage sous différentes postures démontrent l'efficacité de notre approche à fortement réduire la dimension tout en conservant les relations intelligibles entre les objets des données, permettant même d'obtenir une meilleure classification à partir des versions réduites des images qu'à partir des versions originales
**** *annee_2010  *numText_460
Les flux de séries temporelles sont aujourd'hui produits dans de nombreux domaines comme la finance (Zhu et Shasha (2002)), la surveillance deréseaux (Borgne et al. (2007); Airoldi et Faloutsos (2004)), la gestion de l'historique des usages fréquents (Giannella et al. (2003); Teng et al. (2003)), etc.Résumer de tels flux est devenu un domaine important qui permet de surveilleret d'enregistrer des informations fiables sur les séries observées. À ce jour, la majorité des algorithmes de ce domaine s'est concentrée sur des résumés séparés et indépendants (Giannella et al. (2003); Zhu et Shasha (2002); Chen et al.(2002)), en accordant à chaque série le même espace en mémoire. Toutefois, la gestion de cet espace mémoire est un sujet important pour les flux de données et une stratégie accordant la même quantité de mémoire à chaque série n'est pas forcément appropriée. Dans cet article, nous considérons que les séries doivent être en compétition vis à vis de l'espace mémoire, selon leur besoin de précision.Ainsi, nous proposons : (1) une stratégie de gestion de l'espace mémoire optimisée et (2) une nouvelle méthode de résumé des séries temporelles par approximation.Dans ce but, nous observons à la fois l'erreur globale et les erreurs locales. La répartition de la mémoire suit les étapes suivantes : (1) recherche de la séquence la mieux représentée et (2) recherche de la partie à compresser en minimisant l'erreur. Nos expérimentations sur des données réelles montrent l'efficacité et la pertinence de notre approche.
**** *annee_2010  *numText_461
Organiser les données textuelles et en tirer du sens est un défi majeur aujourd'hui. Ainsi, lorsque l'on souhaite analyser un débat en ligne ou un forum de discussion, on voudrait pouvoir rapidement voir quels sont les principaux thèmes abordés et la manière dont la discussion se structure autour d'eux.Pour cela, et parce que un même texte peut être associé à plusieurs thèmes, nous proposons une méthode originale pour regrouper les données textuelles en autorisant les chevauchements et pour nommer chaque groupe de manière lisible.La contribution principale de cet article est une méthode globale qui permet de réaliser toute la chaîne, partant des données textuelles brutes jusqu'à la caractérisation des groupes à un niveau sémantique qui dépasse le simple ensemble de mots.
**** *annee_2010  *numText_462
Appréhender, parcourir des données ou des connaissances reste une tâche difficile en particulier lorsque les utilisateurs sont confrontés à de gros volumes de données. De nombreux travaux se sont intéressés à extraire des points skylines comme outil de restitution. La prise en compte des préférences a retenu l'attention des travaux les plus récents mais les solutions existantes restent très consommatrices en terme de stockage d'informations additionnelles afin d'obtenir des délais raisonnables de réponse aux requêtes. Notre proposition,EC2Sky (Efficient computation of compromises), se focalise sur deux points :(1) comment répondre efficacement à des requêtes de type skyline en présence de préférences utilisateurs malgré de gros volumes de données (aussi bien en terme de dimensions que de préférences) ; (2) comment restituer les connaissances les plus pertinentes en soulignant les compromis associés aux préférences spécifiées.
**** *annee_2010  *numText_463
Lorsque le volume des données est trop important pour qu'elles soient stockées dans une base de données, ou lorsque leur fréquence de production est élevée, les Systèmes de Gestion de Flux de Données (SGFD) permettent de capturer des flux d'enregistrements structurés et de les interroger à la volée par des requêtes permanentes (exécutées de façon continue). Mais les SGFD ne conservent pas l'historique des flux qui est perdu à jamais.Cette communication propose une définition formelle de ce que devrait être un résumé généraliste de flux de données. La notion de résumé généraliste est liée à la capacité de répondre à des requêtes variées et de réaliser des tâches variées de fouille de données, en utilisant le résumé à la place du flux d'origine. Une revue de plusieurs approches de résumés est ensuite réalisée dans le cadre de cette définition.
**** *annee_2010  *numText_465
Les entrepôts de données occupent aujourd'hui une place centrale dans le processus décisionnel.Outre leur consultation, une des finalités des entrepôts est de servir de socle aux techniques de fouilles de données. Malheureusement, les approches existantes exploitent peu les particularités des entrepôts (multidimensionnalité, hiérarchies et données historiques). Parmi ces méthodes, l'extraction de motifs séquentiels multidimensionnels a récemment été étudiée. Nous montrons dans cet article que ces dernières ne tirent pas pleinement profit des hiérarchies et ne découvrent par conséquent qu'une partie seulement des motifs qualitativement intéressants. Nous proposons alors une méthode d'extraction de motifs séquentiels multidimensionnels basée sur un automate et extrayant de nouveaux motifs. Les différentes expérimentations menées sur des jeux de données synthétiques attestent des bonnes performances de notre proposition.
**** *annee_2010  *numText_467
La segmentation d'une base client peut avoir différents objectifs et plusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapter avec les stratégies commerciales d'une entreprise. Dans ce papier, nous présentons un schéma expérimental visant à proposer un ensemble de segmentations alternatives. Ces segmentations sont produites sur des données réelles par la transformation des données initiales, la génération et la sélection de différentes segmentations.
**** *annee_2010  *numText_468
Les techniques d'extraction de connaissances appliquées aux gros volumes de données, issus de l'analyse de puces ADN, permettent de découvrir des connaissances jusqu'alors inconnues. Or, ces techniques produisent de très nombreux résultats, difficilement exploitables par les experts. Nous proposons un outil dédié à l'accompagnement de ces experts dans l'appropriation et l'exploitation de ces résultats. Cet outil est basé sur trois techniques de visualisation(nuages, systèmes solaire et treemap) qui permettent aux biologistes d'appréhender de grandes quantités de motifs séquentiels (séquences ordonnées de gènes).
**** *annee_2010  *numText_470
Le projet RECORDS (collaboration entre industriels et université) a pour objectif de développer une infrastructure de service sécurisée pour assurer le suivi et l'analyse des conditions d'utilisation d'aéronefs. Chaque aéronef est muni de capteurs. Au cours de chaque mission (vol) les données mesurées sont enregistrées localement. Ces dernières sont par la suite transférées dans une base de données centralisée à des fins d'analyse. Le problème rencontré est la grande quantité de données ainsi enregistrées, ce qui en rend l'exploitation difficile. Dans cet article, nous proposons des techniques de compression et de simplification de données avec un taux de perte contrôlé. Nos expérimentations montrent des gains drastiques en volumétrie avec de très faibles pertes d'informations.Ceci représente une première étape avant d'appliquer des techniques d'extraction de connaissances.
**** *annee_2010  *numText_471
La plate-forme SimTOLE est dédiée a l’évaluation d'algorithmes d'alignement d'ontologies hétérogènes et reparties a travers un reseau pair a pair (P2P). Cette plate-forme permet de simuler un réseau P2P dans lequel chaque pair dispose de sa propre ontologie ainsi que des outils permettant l'alignement entre l'ontologie locale et une ontologie stockée sur un pair distant. Le développement de cette plate-forme s'inscrit dans le cadre de travaux de recherche étudiant l'impact de la topologie du réseau P2P dans le processus d'inférence de correspondances sémantiques. Durant cette démonstration, la plate-forme simTole est présentée puis testée pour illustrer des scénarii montrant comment affiner le processus d'alignement d'ontologies dans un réseau P2P.
**** *annee_2010  *numText_472
Nous proposons dans cet article d'introduire une nouvelle approche pour la classification non supervisée hiérarchique. Notre méthode nommée So-Tree consiste à construire, d'une manière autonome et simultanée, une partition topologique et hiérarchique des données. Chaque cluster de la partition est associé à une cellule d'une grille 2D et est modélisé par un arbre, dont chaque noeud représente une donnée. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Les résultats préliminaires obtenus sont encourageants et prometteurs pour continuer dans cette direction.
**** *annee_2010  *numText_473
Plusieurs aspects pourraient influencer les systèmes d'apprentissage existants.Un de ces aspects est lié au déséquilibre des classes dans lequel le nombre d'observations appartenant à une classe, dépasse fortement celui des observations dans les autres classes. Dans ce type de cas assez fréquent, le système d'apprentissage a des difficultés au cours de la phase d'entraînement liées au déséquilibre inter-classe. Nous proposons une méthode de sous-échantillonnage adaptatif pour traiter ce type de bases déséquilibrées.Le processus procède par le sous-échantillonnage des données majoritaires, guidé par les données minoritaires tout au long de la phase d'un apprentissage semi-supervisée.Nous utilisons comme modèle d'apprentissage les cartes auto-organisatrices. L'approche proposée a été validée sur plusieurs bases de données en utilisant les arbres de décision comme classificateur avec une validation croisée. Les résultats expérimentaux ont montré des performances très prometteuses.
**** *annee_2010  *numText_475
Un grand nombre d'informations qui ont une structure complexe proviennent de diverses sources. Ces informations contiennent des connaissances très utiles pour l'aide à la décision. L'Extraction des Connaissances à partir des Données (ECD), permet d'acquérir des informations pertinentes pour les systèmes interactifs d'aide à la décision (SIAD). Mais, dans plusieurs domaines,les données évoluent d'une manière dynamique et finissent par dépendre de plusieurs dimensions. Les Réseaux Bayésiens dynamiques (RBD)sont des modèles représentant des connaissances incertaines sur des phénomènes complexes de processus dynamiques. Notre objectif revient à fixer les meilleures modèles de connaissances extraites par les RBD et à les utiliser pour la prise de décision dynamique. Ainsi, Nous proposons dans cet article une démarche pour la mise en place d'un processus d'extraction des connaissances à partir des données multidimensionnelles et temporelles.
**** *annee_2010  *numText_477
L'objectif de cet article est de montrer que l'utilisation de la règle de décision du maximum de masse de croyance en lieu et place de celle du maximum de probabilité a posteriori peut permettre de réduire le taux d'erreur en classification supervisée. Nous proposons une technique efficace pour extraire, à partir d'un vecteur de probabilités a posteriori, un vecteur de masses de croyance sur lequel baser la décision par le maximum de masse de croyance. L'application de notre méthode dans le domaine de la classification automatique en stades de sommeil montre une amélioration des performances pouvant atteindre 80% de réduction du taux d'erreur de classification.
**** *annee_2010  *numText_478
Cet article présente une méthode d'extraction de relations sémantiques pour la construction d'ontologies à partir de corpus de textes. Notre objectif est de proposer une méthode générique, qui soit indépendante du domaine et de la langue. Elle repose sur une analyse distributionnelle des unités sémantiques du corpus pour faire émerger des relations sémantiques candidates. Cette méthode ne fait aucune hypothèse sur les types de relations recherchées ni sur leur forme linguistique. Il s'agit de regrouper les associations de termes dans des classes qui représentent des relations sémantiques candidates. L'hypothèse sous-jacente est que les occurrences de ces associations réunies sur la base des éléments de contexte qu'elles partagent ont des chances de relever d'une même relation sémantique et que les relations candidates ainsi proposées peuvent aider le travail de conceptualisation de l'ontologue.
**** *annee_2010  *numText_479
L'interrogation de bases de données, dont les dimensions ne cessent de croître, se heurte fréquemment au problème de la gestion des réponses pléthoriques.Une des approches envisageables pour réduire l'ensemble des résultats retournés et le rendre exploitable est de contraindre la requête initiale parl'ajout de nouvelles conditions. L'approche présentée dans cet article s'appuie sur l'identification de liens de corrélation entre prédicats associés aux attributs de la relation concernée. La requête initiale peut ainsi être intensifiée automatiquement ou par validation de l'utilisateur à travers l'ajout de prédicats proches sémantiquement de ceux spécifiés.
**** *annee_2010  *numText_480
Dans cet article, nous valorisons et défendons l'idée que les modèles génératifs sont une approche prometteuse pour l'identification de structure de communautés (ISC). Nous proposons un nouveau modèle probabiliste pour l'identification de structures de communautés qui utilise le lissage afin de pallier le petit nombre de liens entre les nœuds. Notre modèle étant très sensible aux paramètres de lissage, nous proposons également une méthode basée sur la modularité pour leur estimation. Les résultats expérimentaux obtenus sur 3 jeux de données montrent que notre modèle SPCE est largement meilleur que le modèle PHITS
**** *annee_2010  *numText_482
Les correspondances complexes ont été étudiées à plusieurs reprises dans le domaine d'alignement de schémas de bases de données. Par contre,dans le domaine d'alignement des ontologies, elles ont été peu étudiées. Nous proposons, dans ce papier, une nouvelle approche de découverte de correspondances complexes entre deux ontologies. L'approche proposée est extensionnelle,terminologique et implicative. Dans cette approche, nous utilisons le modèle des règles d'association afin de découvrir des correspondances de typex ⇒ y1 ∧ ... ∧ yn entre deux ontologies.
**** *annee_2010  *numText_483
Dans cet article, une nouvelle stratégie d'apprentissage actif est proposée. Cette stratégie est fondée sur une méthode de discrétisation Bayésienne semi-supervisée. Des expériences comparatives sont menées sur des données unidimensionnelles, l'objectif étant d'estimer la position d'un échelon à partir de données bruitées.
**** *annee_2010  *numText_484
L'acquisition des connaissances en vue de résoudre des problèmes concernant l'évolution des artefacts, comme elle se doit d'être pratiquée en conception inventive, a des caractéristiques spécifiques. Elle nécessite la sélection de certaines des connaissances qui peuvent induire des évolutions,elle amène à reformuler le problème initial afin de construire un modèle abstrait de l'artefact concerné. La méthode de conception inventive induite parla théorie de la Résolution des Problèmes Inventifs (aussi connue sous l'acronyme TRIZ) n'a pas encore fait l'objet d'une véritable formalisation.Nous proposons ici une ontologie des notions principales des concepts liés à l'acquisition des connaissances dans ce cadre. Cette ontologie, outre la clarification des notions en jeu, est utilisée comme support d'un environnement informatique d'aide à la mise en oeuvre d'une méthode pour acquérir les connaissances et formuler les problèmes.
**** *annee_2010  *numText_485
Le paradigme des flots de données rend impossible la conservation de l'intégralité de l'historique d'un flot qu'il faut alors résumer. L'extraction d'itemsets fréquents sur des fenêtres temporelles semble tout à fait adaptée mais l'amoncellement des résultats indépendants rend impossible l'exploitation de ces résultats. Nous proposons une structure basée sur les hiérarchies des données afin d'unifier ces résultats. De plus, puisque la plupart des données d'un flot présentent un caractère multidimensionnel, nous intégrons la prise en compte d'itemsets multidimensionnels. Enfin, nous pallions une faiblesse majeure des Tilted TimeWindows (TTW) en prenant en compte la distribution des données.
**** *annee_2010  *numText_487
Une des tâches classiques en fouille de données spatiales est l'extraction de co-localisations intéressantes dans des données géo-référencées. L'objectif est de trouver des sous-ensembles de caractéristiques booléennes apparaissant fréquemment dans des objets spatiaux voisins. Toutefois, les relations découvertes peuvent ne pas être pertinentes pour les experts, et leur interprétation sous forme textuelle peut être difficile. Nous proposons, dans ce contexte, une nouvelle approche pour intégrer la connaissance des experts dans la découverte des co-localisations, ainsi qu'une nouvelle représentation visuelle de ces motifs. Un prototype a été développé et intégré dans un SIG. Des expérimentations on été menées sur des données géologiques réelles, et les résultats validés par un expert du domaine.
**** *annee_2010  *numText_489
Wikipedia, devenue l'une des bases de connaissances les plus populaires,pose le problème de la fiabilité de l'information qu'elle dissémine. Nous proposons WikipediaViz, un ensemble de visualisations basé sur un mecanisme de collecte et d'agrégation de données d'édition Wikipedia pour aider le lecteurà appréhender la maturité d'un article. Nous listons cinq métriques importantes,déterminées lors de sessions de conception participative avec des experts Wikipedia pour juger de la qualité, que nous présentons au lecteur sous forme de visualisations compactes et expressives, dépeignant le profil d'évolution d'un article.Nos études utilisateur ont montré queWikipediaViz réduisait significativement le temps requis pour évaluer la qualité en maintenant une bonne précision
**** *annee_2010  *numText_490
Dans ce papier, nous proposons une approche WCUM (Web Contentand Usage based Approach) permettant de relier l'analyse du contenu d'un site Web à l'analyse de l'usage afin de mieux comprendre les comportements de navigation sur le site. L'apport de ce travail réside d'une part dans la proposition d'une approche reliant l'analyse du contenu à l'analyse de l'usage et d'autre part dans l'extension de l'application des méthodes de block clustering, appliquées généralement en bioinformatique, au contexte Web mining afin de profiter de leur pouvoir classificatoire dans la découverte de biclasses homogènes à partir d'une partition des instances et une partition des attributs recherchées simultanément.
**** *annee_2009  *numText_492
Cet article présente succinctement le retour d'expérience d'Ardans dans l'implantation de systèmes de gestion de connaissances dans des organisations très variées au début de ce 21ème siècle.
**** *annee_2009  *numText_493
La conception de systèmes d'Extraction d'Information (EI) destinés à extraire les réseaux d'interactions géniques décrits dans la littérature scientifique est un enjeu important. De tels systèmes nécessitent des représentations sophistiquées, s'appuyant sur des ontologies, afin de définir différentes relations biologiques, ainsi que les dépendances récursives qu'elles présentent entre elles.Cependant, l'acquisition de ces dépendances n'est pas possible avec les techniques d'apprentissage automatique actuellement employées en EI, car ces dernières ne gèrent pas la récursivité. Afin de palier ces limitations, nous présentons une application à l'EI de la Programmation Logique Inductive, en mode multipredicats. Nos expérimentations, effectuées sur un corpus bactérien, conduisent à un rappel global de 67.7% pour une précision de 75.5%.
**** *annee_2009  *numText_494
Nous présentons dans cet article le système Skin3D qui implémentet ous les composants matériels et logiciels nécessaires pour extraire des informations dans des images 3D de peau. Il s'agit à la fois du matériel d'éclairage et d'acquisition à base d'appareils photographiques stéréoscopiques, d'une méthode de calibration de caméras utilisant les algorithmes génétiques, de matériel de réalité virtuelle pour restituer les images en stéréoscopie et interagir avec elles, et enfin d'un ensemble de fonctionnalités interactives pour annoter les images, partager ces annotations et construire un hypermédia 3D. Nous présentons une étude comparative concernant la calibration et une application réelle de Skin3D sur des images de visages.
**** *annee_2009  *numText_497
Dans cet article, nous considérons des objets pour lesquels nous disposons d'une matrice des dissimilarités et nous nous intéressons à leurs liens avec des attributs. Nous nous centrons sur l'analyse de séquences d'états pour lesquelles les dissimilarités sont données par la distance d'édition. Toutefois, les méthodes développées peuvent être étendues à tout type d'objets et de mesure de dissimilarités. Nous présentons dans un premier temps une généralisation de l'analyse de variance (ANOVA) pour évaluer le lien entre des objets non mesurables (p. ex. des séquences) avec une variable catégorielle. La clef de l'approche est d'exprimer la variabilité en termes des seules dissimilarités ce qui nous permet d'identifier les facteurs qui réduisent le plus la variabilité. Nous présentons un test statistique général qui peut en être déduit et introduisons une méthode originale de visualisation des résultats pour les séquences d'états. Nous présentons ensuite une généralisation de cette analyse au cas de facteurs multiples et en discutons les apports et les limites, notamment en terme d'interprétation. Finalement, nous introduisons une nouvelle méthode de type arbre d'induction qui utilise le test précédent comme critère d'éclatement. La portée des méthodes présentées est illustrée à l'aide d'une analyse des facteurs discriminant le plus les trajectoires occupationnelles .
**** *annee_2009  *numText_498
Dans cet article, nous appliquons une méthode d'analyse sur des descriptions de procédures de neurochirurgie dans le but d'en améliorer la compréhension. La base de données XML utilisée dans cette étude est constituée de la description de 157 chirurgies de tumeurs. Trois cent vingt deux variables ont été identifiées et décomposées en variables prédictives(connues avant l'opération) et variables à prédire (décrivant des gestes chirurgicaux). Une analyse factorielle des correspondances (AFC) a été réalisée sur les variables prédictives, ainsi qu'un arbre de décision basé sur un dendrogramme préalablement établi. Six classes principales de variables prédictives ont ainsi été identifiées. Puis, pour chacune de ces classes, une analyse AFC a été réalisée sur les variables à prédire, ainsi qu'un arbre de décision. Bien que le nombre de cas et le choix des variables constituent une limite à cette étude, nous avons réussi à prédire certaines caractéristiques liées aux procédures en partant de données prédictives.
**** *annee_2009  *numText_499
Les activités de négoce de matériaux sont un marché extrêmement compétitif. Pour les acteurs de ce marché, les méthodes de fouille de données peuvent s'avérer intéressantes en permettant de dégager des gains de rentabilité importants. Dans cet article, nous présenterons le retour d'expérience du projet de fouille de données mené chez VM Matériaux pour améliorer le retour surinvestissement d'opérations commerciales. La synergie des informaticiens, du marketing et des experts métier a permis d'améliorer l'extraction des connaissances à partir des données de manière à aboutir à la connaissance actionnable la plus pertinente possible et ainsi aider les experts métier à prendre des décisions.
**** *annee_2009  *numText_500
Les systèmes décisionnels reposent sur des bases de données multidimensionnelles qui offrent un cadre adéquat aux analyses OLAP. L'article présente un nouvel opérateur OLAP nommé « BLEND » rendant possible des analyses multigraduelles. Il s'agit de transformer la structuration multidimensionnelle lors des interrogations pour analyser les mesures selon des niveaux de granularité différents recombinées comme un même paramètre. Nous menons une étude des combinaisons valides de l'opération dans le contexte des hiérarchies strictes. Enfin, une première série d'expérimentations implante l'opération dans le contexte R-OLAP en montrant le faible coût de l'opération.
**** *annee_2009  *numText_501
L'analyse sémantique est un nouveau paradigme d'interrogation du Web Sémantique qui a pour objectif d'identifier les associations sémantiques reliant des individus décrits dans des ontologies OWL-DL. Pour déduire davantage d'associations sémantiques et augmenter la précision de l'analyse, l'information spatio-temporelle attachée aux ressources doit être prise en compte. A ces fins - et pour combler l'absence actuelle de raisonneurs spatio-temporel défini pour les ontologies RDF(S) et OWL-, nous proposons le système de représentation et d'interrogation d'ontologies spatio-temporelles ONTOAST, compatible avec le langage OWL-DL. Nous présentons les principes de base de l'algorithme de découverte d'associations sémantiques entre individus intégré dans ONTOAST. Cet algorithme utilise deux contextes, l'un spatial et l'autre temporel qui permettent d'affiner la recherche. Nous décrivons enfin l'approche mise en œuvre pour la déduction de connexions spatiales entre individus.
**** *annee_2009  *numText_504
Dans cet article, nous proposons une nouvelle approche de classification et de pondération des variables durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage de ces cartes topologiques est combiné à un mécanisme d'estimation de pertinences des différentes variables sous forme de poids d'influence sur la qualité de la classification. Nous proposons deux types de pondérations adaptatives : une pondération des observations et une pondération des distances entre observations. L'apprentissage simultané des pondérations et des prototypes utilisés pour la partition des observations permet d'obtenir une classification optimisée des données. Un test statistique est ensuite utilisé sur ces pondérations pour élaguer les variables non pertinentes. Ce processus de sélection de variables permet enfin, grâce à la localité des pondérations, d'exhiber un sous ensemble de variables propre à chaque groupe (cluster) offrant ainsi sa caractérisation. L'approche proposée a été validé sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses.
**** *annee_2009  *numText_505
L'usage du modèle des règles d'association en fouille de données est limité par la quantité prohibitive de règles qu'il fournit et nécessite la mise en place d'une phase de post-traitement efficace afin de cibler les règles les plus utiles. Cet article propose une nouvelle approche intégrant explicitement les connaissances du décideur afin de filtrer et cibler les règles intéressantes.
**** *annee_2009  *numText_507
D'importants volumes d'images satellites et aériennes de tout type(panchromatiques, multispectrales, hyperspectrales) sont générées quotidiennement, et leur classification par des méthodes semi-automatiques devient nécessaire. Le logiciel ENVI Feature eXtractionTM (ENVI FXTM) sebase sur une approche « objet » -par opposition à une approche pixels classique- et sur des algorithmes innovants, pour la segmentation et la classification des images de télédétection avec un haut niveau de précision.
**** *annee_2009  *numText_509
Nous présentons dans cet article des approches visant à valider des relations syntaxiques induites de type Verbe-Objet. Ainsi, nous proposons d'utiliser dans un premier temps une approche s'appuyant sur des vecteurs sémantiques déterminés à l'aide d'un thésaurus. La seconde approche emploie une validation Web. Nous effectuons des requêtes sur un moteur de recherche associées à des mesures statistiques afin de déterminer la pertinence d'une relation syntaxique. Nous proposons enfin de combiner ces deux méthodes. La qualité de nos approches de validation de relations syntaxiques a été évaluée en utilisantdes courbes ROC.
**** *annee_2009  *numText_510
Le choix d'une mesure pour comparer les données est au cœur des tâches de recherche d'information et d'apprentissage automatique. Nous considérons ici ce problème dans le cas où seul l'ordre induit par la mesure importe,et non les valeurs numériques qu'elle fournit : cette situation est caractéristique des moteurs de recherche de documents par exemple. Nous étudions dans ce cadre les mesures de comparaison classiques pour données numériques, telles que les distances et les noyaux les plus courants. Nous identifions les mesures équivalentes, qui induisent toujours le même ordre ; pour les mesures non équivalentes,nous quantifions leur désaccord par des degrés d'équivalence basés sur le coefficient de Kendall généralisé. Nous étudions les équivalences et quasi équivalences à la fois sur les plans théorique et expérimental.
**** *annee_2009  *numText_512
En classification supervisée, la présence de bruit sur les valeurs des descripteurs peut avoir des effets désastreux sur la performance des classifieurs et donc sur la pertinence des décisions prises au moyen de ces modèles. Traiter ce problème lorsque le bruit affecte un attribut classe a été très étudié. Il est plus rare de s'intéresser au bruit sur les autres attributs. C'est notre contexte de travail et nous proposons la construction de nouveaux descripteurs robustes lorsque ceux des exemples originaux sont bruités. Les résultats expérimentaux montrent la valeur ajoutée de cette construction par la comparaison des qualités obtenues (e.g., précision) lorsque l'on utilise les méthodes de classification à partir de différentes collections de descripteurs.
**** *annee_2009  *numText_513
Les systèmes d'analyse de flux de données prennent de plus en plus d'importance dans un contexte où les données circulant sur les réseaux sont de plus en plus volumineuses et où la volonté de réagir au plus vite, en temps réel,devient un besoin nécessaire. Afin de permettre des analyses aussi rapides et efficaces que possible, il convient de pouvoir contrôler les flots de données et de focaliser les traitements sur les données pertinentes. Le protocole présenté dans ce papier donne au module de traitement des capacités d'action et de contrôle sur les observations remontantes en fonction de l'état de l'analyse. La diminution des flux résultant de telles focalisations permet des traitements beaucoup plus efficaces, plus pertinents et moins consommateurs de ressources. Les premiers résultats montrent un réel gain de performances sur nos applications (facteur100).
**** *annee_2009  *numText_514
L'analyse formelle de concepts est une méthode fondée sur la correspondance de Galois et qui permet de construire des hiérarchies de concepts formels à partir de tableaux de données binaires. Cependant de nombreux problèmes réels abordés en fouille de données comportent des données plus complexes.Afin de traiter de tels problèmes, nous proposons une conversion de données floues multi-valuées en attributs histogrammes et une correspondance de Galois adaptée à ce format. Notre propos est illustré avec un jeu de données simples. Enfin, nous évaluons brièvement les résultats et les apports de cette correspondance de Galois par rapport à l'approche classique
**** *annee_2009  *numText_516
L'exploitation en temps réel de connaissances complexes est un défi dans de nombreux domaines, tels que le web sémantique, la simulation ou les systèmes multi-agents (SMA). Dans le paradigme multi-agents, des travaux récents montrent que les communications multi-parties (CMP) offrent des opportunités intéressantes en termes de réalisme des communications, diffusion des connaissances et sémantique des actes de langage. Cependant, ces travaux se heurtent à la difficulté de mise en oeuvre des CMP, pour lesquelles les supports de communications classiques sont insuffisants. Dans cet article, nous proposons d'utiliser le formalisme de l'Analyse de Données Symboliques (ADS) pour modéliser les informations et les besoins des agents. Nous appuyons le routage des messages sur cette modélisation dans le cadre d'un environnement de communication pour les systèmes multi-agents. Afin d'illustrer notre propos, nous utiliserons l'exemple de la gestion des communications dans un poste d'appels d'urgence. Nous présentons ensuite notre retour d'expérience, et discutons les perspectives ouvertes par la fertilisation croisée de l'ADS et des SMA.
**** *annee_2009  *numText_517
Nous avons développé un système dont le but est d'obtenir le logiciel de commande d'un robot capable de simuler le comportement d'un humain placé en situation de résolution de problèmes. Nous avons résolu ce problème dans un environnement psychologique particulier où les comportements humains peuvent être interprétés comme des 'observables' de leurs stratégies de résolution de problèmes. Notre solution contient de plus celle d'un autre problème,celui de construire une boucle complète commençant avec le comportement d'un groupe d'humains, son analyse et son interprétation en termes d'observables humaines, la définition des stratégies utilisées par les humains (y compris celles qui sont inefficaces), l'interprétation des observables humaines en terme de mouvements du robot, la définition de ce qu'est une stratégie de robot en terme de stratégies humaines. La boucle est bouclée avec un langage de programmation capable de programmer ces stratégies robotiques, qui deviennent ainsi à leur tour des observables, tout comme l'ont été les stratégies humaines du début de la boucle. Nous expliquons comment nous avons été capables définir de façon objective ce que nous appelons une stratégie de robot.Notre solution assemble deux facteurs différents. L'un permet d'éviter les comportements 'inhumains' et se fonde sur la moyenne des comportements des humains que nous avons observés. L'autre fournit une sorte 'd'humanité' au robot en lui permettant de dévier de cette moyenne par n fois l'écart type observé chez les humains qu'il doit simuler. Il devient alors possible de programmer des comportements complètement humains.
**** *annee_2009  *numText_518
Prometteuses en terme de prévention, de dépistage, de diagnostic et d'actions thérapeutiques, les puces à ADN mesurent l'intensité des expressions de plusieurs milliers de gènes. Dans cet article, nous proposons une nouvelle approche appelée DEMON, pour extraire des motifs séquentiels à partir de don-nées issues des puces ADN et qui utilise des connaissances du domaine.
**** *annee_2009  *numText_521
Récemment, le nombre et le volume des bases de données séquentielles biologiques ont augmenté de manière considérable. Dans ce contexte, l'identification des anomalies est essentielle. La plupart des approches pour les extraire se fondent sur une base d'apprentissage ne contenant pas d'outlier. Or,dans de très nombreuses applications, les experts ne disposent pas d'une telle base. De plus, les méthodes existantes demeurent exigeantes en mémoire, ce qui les rend souvent impossibles à utiliser. Nous présentons dans cet article une nouvelle approche, basée sur un modèle de Markov d'ordre variable et sur une mesure de similarité entre objets séquentiels. Nous ajoutons aux méthodes existantes un critère d'élagage pour contrôler la taille de l'espace de recherche et sa qualité, ainsi qu'une inégalité de concentration précise pour la mesure de similarité, conduisant à une meilleure détection des outliers. Nous démontrons expérimentalement la validité de notre approche.
**** *annee_2009  *numText_522
Pour pallier le problème des attaques sur les réseaux de nouvelles approches de détection d'anomalies ou d'abus ont été proposées ces dernières an-nées et utilisent des signatures d'attaques pour comparer une nouvelle requête et ainsi déterminer s'il s'agit d'une attaque ou pas. Cependant ces systèmes sont mis à défaut quand la requête n'existe pas dans la base de signature. Généralement, ce problème est résolu via une expertise humaine afin de mettre à jour la base de signatures. Toutefois, il arrive fréquemment qu'une attaque ait déjà été détectée dans une autre organisation et il serait utile de pouvoir bénéficier de cette connaissance pour enrichir la base de signatures mais cette information est difficile à obtenir car les organisations ne souhaitent pas forcément indiquer les attaques qui ont eu lieu sur le site. Dans cet article nous proposons une nouvelle approche de détection d'intrusion dans un environnement collaboratif sécurisé.Notre approche permet de considérer toute signature décrite sous la forme d'ex-pressions régulières et de garantir qu'aucune information n'est divulguée sur le contenu des différents sites.
**** *annee_2009  *numText_524
Un des problèmes majeurs de la classification non supervisée est la détermination ou la validation du nombre de classes dans la population. Ce problème s'étend aux méthodes de bipartitionnement ou block clustering. Dans ce papier, nous nous intéressons à l'algorithme CROKI2 de classification croisée des tableaux de contingence proposé par Govaert (1983). Notre objectif est de déterminer le nombre de classes optimal sur les lignes et les colonnes à travers un ensemble de techniques de validation de classes proposés dans la littérature pour les méthodes classiques de classification.
**** *annee_2009  *numText_525
Le but d'un système adaptatif de diagnostic est de surveiller et diagnostiquer un système tout en s'adaptant à son évolution. Ceci passe par l'adaptation des diagnostiqueurs qui précisent ou enrichissent leur propre modèle pour suivre au mieux le système au fil du temps. Pour détecter les besoins d'adaptation,nous proposons un cadre de diagnostic multi-sources s'inspirant de la fusion d'information. Des connaissances fournies par le concepteur sur des relations attendues entre les diagnostiqueurs mono-source forment un méta-modèle du diagnostic. La compatibilité des résultats du diagnostic avec le méta-modèle est vérifiée en ligne. Lorsqu'une de ces relations n'est pas vérifiée, les diagnostiqueurs concernés sont modifiés.Nous appliquons cette approche à la conception d'un système adaptatif de détection d'intrusion à partir d'un flux de connexions à un serveur Web. Les évaluations du système mettent en évidence sa capacité à améliorer la détection des intrusions connues et à découvrir de nouveaux types d'attaque.
**** *annee_2009  *numText_526
Cet article propose une méthode reposant sur l'utilisation de l'Analyse Formelle de Concepts et des treillis de Galois pour l'analyse de systèmes complexes. Des statistiques reposant sur ces treillis permettent de calculer la distribution conceptuelle des objets classifiés par le treillis.L'expérimentation sur des échantillons de trois réseaux sociaux en ligne illustre l'utilisation de ces statistiques pour la caractérisation globale et pour le filtrage automatique de ces systèmes.
**** *annee_2009  *numText_527
La RFID (Radio Frequency IDentification) est une technologie avancée d'enregistrement de données spatio-temporelles de traçabilité. L'objectif de ce travail est de transformer ces données spatio-temporelles en connaissances exploitables par les utilisateurs par l'intermédiaire d'une méthode de classification automatique des données. Les systèmes RFID peuvent être utilisés pour étudier les sociétés animales, qui sont des systèmes dynamiques complexes caractérisés par beaucoup d'interactions entre les individus (Fresneau et al., 1989). Le cadre applicatif choisi pour ce travail est l'étude de la structure d'un groupe d'individus en interaction sociale et en particulier la division du travail au sein d'une colonie de fourmis1.La RFID générant d'importants volumes de données, il est nécessaire de développer des méthodes appropriées afin d'en comprendre le sens. Nous proposons pour cela un algorithme de classification topographique non-supervisée pour l'exploration de ce type de données, capable de détecter les groupes d'individus exprimant le même comportement. L'algorithmeDS2L-SOM (Density-based Simultaneous Two-Level - SOM, Cabanes et Bennani (2008)) est capable de détecter non seulement les groupes définis par une zone vide de donnée, grâce à une estimation de la pertinence des connexions entre référents, mais aussi les groupes définis seulement par une diminution de densité, grâce à une estimation de la densité autour des référents pendant l'apprentissage.
**** *annee_2009  *numText_528
Cet article présente une nouvelle méthode permettant d'explorer les probabilités délivrées par un modèle prédictif de classification. L'augmentation de la probabilité d'occurrence de l'une des classes du problème étudié est analysée en fonction des variables explicatives prises isolément. La méthode proposée est posée et illustrée dans un cadre général, puis explicitement dédiée au classifieur Bayesien naïf. Son illustration sur les données du challenge PAKDD 2007montre que ce type d'exploration permet de créer des indicateurs performants d'aide à la vente.
**** *annee_2009  *numText_530
L'extraction de motifs fermés dans des relations binaires a été très étudiée. Cependant, de nombreuses relations intéressantes sont n-aires avec n >2 et bruitées (nécessité d'une tolérance aux exceptions). Récemment, ces deux problèmes ont été traités indépendamment. Nous introduisons notre proposition pour combiner de telles fonctionnalités au sein d'un même algorithme.
**** *annee_2009  *numText_531
Dans cet article, nous introduisons deux nouveaux concepts : les règles de corrélation décisionnelles et les vecteurs de contingence. Le premier résulte d'un couplage entre les règles de corrélation et les règles de décision. Il permet de mettre en évidence des liens pertinents entre certains ensembles de motifs d'une relation binaire et les valeurs d'un attribut cible (appartenant à cette même relation) en se basant à la fois sur la mesure du Khi-carré et sur le support des motifs extraits. De par la nature du problème, les algorithmes par niveaux font que l'extraction des résultats a lieu avec des temps de réponse élevés et une occupation mémoire importante. Afin de palier à ces deux inconvénients, nous proposons un algorithme basé sur l'ordre lectique et les vecteurs de contingence.
**** *annee_2009  *numText_532
Les règles graduelles suscitent depuis quelques années un intérêt croissant.De telles règles, de la forme Plus (moins) A1 et ... plus (moins) An alors plus (moins) B1 et ... plus (moins) Bn trouvent application dans de nombreux domaines tels que la bioinformatique, les contrôleurs flous, les relevés de capteurs ou encore les flots de données. Ces bases, souvent composées d'un grand nombre d'attributs, restent un verrou pour l'extraction automatique de connaissances,car elles rendent inefficaces les techniques de fouille habituelles (règles d'association, clustering...). Dans cet article, nous proposons un algorithme efficace d'extraction d'itemset graduels basé sur l'utilisation des treillis. Nous définissons formellement les notions de gradualité, ainsi que les algorithmes associés.Des expérimentations menées sur jeux de données synthétiques et réels montrent l'intérêt de notre méthode
**** *annee_2009  *numText_534
De par leur caractère structuré, les bases de données relationnelles sont des sources précieuses pour la construction automatisée d'ontologies. Ce-pendant, une limite persistante des approches existantes est la production d'ontologies de structure calquée sur celles des schémas relationnels sources. Dans cet article, nous décrivons la méthode RTAXON dont la particularité est d'identifier des motifs de catégorisation dans les données afin de produire des ontologies plus structurées, riches en hiérarchies. La méthode formalisée combine analyse classique du schéma relationnel et fouille des données pour l'identification de structures hiérarchiques.
**** *annee_2009  *numText_535
Nous proposons une approche générique pour la fusion d'informations qui repose sur l'utilisation du modèle des Graphes Conceptuels et l'opération de jointure maximale. Nous validons notre approche par le biais d'expérimentations. Ces expérimentations soulignent l'importance des heuristiques mises en place.
**** *annee_2009  *numText_536
La classification supervisée est une tâche de fouille de données (DataMining), qui consiste à construire un classifieur à partir d'un ensemble d'exemples étiquetés par des classes (phase d'apprentissage) et ensuite prédire les classes des nouveaux exemples avec ce classifieur (phase de classification). En classification supervisée, plusieurs approches ont été proposées dont l'approche basée sur l'Analyse de Concepts Formels. L'apprentissage de Concepts Formels est basé généralement sur la structure mathématique du treillis de Galois (ou treillis de concepts). Cependant, la complexité exponentielle de génération d'un treillis de Galois a limité les champs d'application de ces systèmes. Dans cet article, nous présentons plusieurs méthodes de classification supervisée basées sur l'Analyse de Concepts Formels. Nous présentons aussi le boosting (dopage)de classifieurs, une technique de classification innovante. Enfin, nous proposons le boosting de concepts formels, une nouvelle méthode adaptative qui construit seulement une partie du treillis englobant les meilleurs concepts. Ces concepts sont utilisés comme étant des règles de classification. Les résultats expérimentaux réalisés ont prouvé l'intérêt de la méthode proposée par rapport à celles existantes.
**** *annee_2009  *numText_537
La définition du voisinage est un élément central en fouille de données, et de nombreuses définitions ont été avancées. Nous en proposons ici une version statistique issue de notre test de randomisation TourneBool, qui permet, à partir d'un tableau de relations binaires objets décrits/descripteurs, d'établir quelles relations entre descripteurs sont dues au hasard, et lesquelles ne le sont pas, sans faire d'hypothèse sur les lois de répartitions sous-jacentes, c'est à dire en tenant compte de lois de tous types sans avoir besoin de les spécifier. Ce test est basé sur la génération et l'exploitation d'un ensemble de matrices randomisées ayant les mêmes sommes marginales en lignes et colonnes que la matrice d'origine. Après une première application encourageante à un corpus textuel réduit, nous avons opéré le passage à l'échelle adéquat pour traiter des corpus textuels de taille réelle, comme celui des dépêches Reuters. Nous caractérisons le graphe des mots de ce corpus au moyen d'indicateurs classiques comme le coefficient de clustering, la distribution des degrés et de la taille des communautés, etc. Une autre caractéristique de TourneBool est qu'il permet aussi de dégager les anti liens entre mots, à savoir les mots qui s'évitent plus qu'attendu du fait du hasard. Le graphe des liens et celui des anti-liens seront caractérisés de la même façon.
**** *annee_2009  *numText_539
L'objecif de cet article est de faire de la carte auto-organisatrice hiérarchique(GHSOM) un outil utilisable dans le cadre d'une démarche d'analyse exploratoire de données. La visualisation globale est un outil indispensable pour rendre les résultats d'une segmentation intelligibles pour un utilisateur. Nous proposons donc différents outils de visualisation pour la GHSOM équivalents à ceux de la SOM.
**** *annee_2009  *numText_540
Nous présentons une approche à ce que nous appelons la « créativité calculatoire », c'est-à-dire les procédés par lesquels une machine peut faire montre d'une certaine créativité. Dans cet article, nous montrons essentiellement que la synthèse de prédicats multiples en programmation logique inductive (ILP) et la synthèse de programmes à partir de spécifications formelles (SPSF), deux domaines de l'informatique qui s'attaquent à des problèmes où la notion de créativité est centrale, ont été amenés à ajouter à leur formalisme de base (l'ILP pour l'un, les tableaux de Beth pour l'autre)toute une série d'heuristiques. Cet article présente une collection d'heuristiques qui sont destinées à fournir au programme une forme de créativité calculatoire. Dans cette présentation, l'accent est plutôt mis sur les heuristiques de l'ILP mais lorsque cela était possible sans de trop longs développements, nous avons aussi présenté quelques heuristiques de la SPSF. L'outil indispensable de la créativité calculatoire est ce que nous appelons un 'générateur d'atouts' dont une spécification (forcément informelle comme nous le verrons) est fournie comme première conclusion aux exemples décritsdans le corps de l'article.
**** *annee_2009  *numText_543
L'analyse formelle de concepts (AFC, Ganter etWille (1999)) est une méthode pertinente d'extraction de connaissances à partir de données complexes d'expression de gènes (Blachon et al. (2007), Motameny et al. (2008)). Dans ce papier, nous proposons d'extraire des groupes de gènes partageant un comportement similaire montrant des changements significatifs à travers divers environnements biologiques, servant d'hypothèses à la fonction des gènes.
**** *annee_2009  *numText_545
Ce travail s'inscrit dans la problématique de l'apprentissage non supervisé. Dans ce cadre se retrouvent les méthodes de classification automatique non paramétriques qui reposent sur l'hypothèse que plus des individus sont proches dans l'espace de représentation, plus ils ont de chances de faire partie de la même classe. Cet article propose une nouvelle méthode de ce type qui considère la proximité à travers la structure fournie par un graphe de voisinage.
**** *annee_2009  *numText_546
Cet article présente un environnement pour la personnalisation des analyses OLAP afin de réduire la charge de navigation de l'utilisateur. Nous proposons un modèle de préférences contextuelles qui permet de restituer les données en fonction des préférences de l'utilisateur et de son contexted'analyse.
**** *annee_2009  *numText_547
Nous présentons une application innovante de la modélisation des connaissances au domaine des bibliothèques numériques spécialisées. Nous utilisons la spécification experte de la TEI (Text Encoding Initiative) pour modéliser la connaissance apportée par les chercheurs qui travaillent sur des archives manuscrites. Nous montrons les limites de la TEI dans le cas d'une approche diachronique du document, cette dernière impliquant la construction simultanée de structures de données concurrentes. Nous décrivons un modèle qui présente le problème et permet d'envisager des solutions. Enfin, nous justifions les structures arborescentes sur lesquelles se base ce modèle.
**** *annee_2009  *numText_548
Cet article traite de la problématique de la classification recouvrante(overlapping clustering) et propose deux variantes de l'approche OKM : OKMEDet WOKM. OKMED généralise k-médoïdes au cas recouvrant, il permet d'organiser un ensemble d'individus en classes non-disjointes, à partir d'une matrice de distances. La méthode WOKM (Weighted-OKM) étend OKM par une pondération locale des classes ; cette variante autorise chaque individu à appartenir à plusieurs classes sur la base de critères différents. Des expérimentations sont réalisées sur une application cible : la classification de textes. Nous montrons alors que OKMED présente un comportement similaire à OKM pour la métrique euclidienne,et offre la possibilité d'utiliser des métriques plus adaptées et d'obtenir de meilleures performances. Enfin, les résultats obtenus avec WOKM montrent un apport significatif de la pondération locale des classes
**** *annee_2009  *numText_550
L'alignement d'ontologies est une tâche importante dans les systèmes d'intégration puisqu'elle autorise la prise en compte conjointe de ressources décrites par des ontologies différentes, en identifiant des appariements entre concepts. Avec l'apparition de très grandes ontologies dans des domaines comme la médecine ou l'agronomie, les techniques d'alignement, qui mettent souvent en œuvre des calculs complexes, se trouvent face à un défi : passer à l'échelle.Pour relever ce défi, nous proposons dans cet article deux méthodes de partitionnement, conçues pour prendre en compte, le plus tôt possible, l'objectif d'alignement. Ces méthodes permettent de décomposer les deux ontologies à aligner en deux ensembles de blocs de taille limitée et tels que les éléments susceptibles d'être appariés se retrouvent concentrés dans un ensemble minimal de blocs qui seront effectivement comparés. Les résultats des tests effectuées avec nos deux méthodes sur différents couples d'ontologies montrent leur efficacité.
**** *annee_2009  *numText_553
RDBToOnto1 est un logiciel extensible qui permet d'élaborer des ontologies précises à partir de bases de données relationnelles. Le processus supporté est largement automatisé, de l'extraction des données à la génération du modèle de l'ontologie et son instanciation. Pour affiner le résultat, le processus peut être orienté par des contraintes locales définies interactivement. C'est aussi un cadre facilitant la mise en œuvre de nouvelles méthodes d'apprentissage.
**** *annee_2009  *numText_554
L'application présentée permet de regrouper les définitions de sigles issues des sciences du vivant par des mesures de proximité lexicale (approche automatique) et une intervention de l'expert (approche manuelle).
**** *annee_2009  *numText_555
Face à la grande volumétrie des données générées par les systèmes informatiques,l'hypothèse de les stocker en totalité avant leur interrogation n'est plus possible. Une solution consiste à conserver un résumé de l'historique du flux pour répondre à des requêtes et pour effectuer de la fouille de données. Plusieurs techniques de résumé de flux de données ont été développées, telles que l'échantillonnage, le clustering, etc. Selon le champ de requête, ces résumés peuvent être classés en deux catégories: résumés spécialisés et résumés généralistes.Dans ce papier, nous nous intéressons aux résumés généralistes. Notre objectif est de créer un résumé de bonne qualité, sur toute la période temporelle,qui nous permet de traiter une large panoplie de requêtes. Nous utilisons deux algorithmes : CluStream et StreamSamp. L'idée consiste à les combiner afin de tirer profit des avantages de chaque algorithme. Pour tester cette approche, nous utilisons un Benchmark de données réelles KDD_99. Les résultats obtenus sont comparés à ceux obtenus séparément par les deux algorithmes.
**** *annee_2009  *numText_556
Parmi les mesures de similarité classiques utilisables sur des ensembles figure l'indice de Jaccard. Dans le cadre de cet article, nous en proposons une extension pour comparer des ensembles de chaînes de caractères. Cette mesure hybride permet de combiner une distance entre chaînes de caractères, telle que la distance de Levenstein, et l'indice de Jaccard. Elle est particulièrement adaptée pour mettre en correspondance des champs composés de plusieurs chaînes de caractères,comme par exemple, lorsqu'on se propose d'unifier des noms d'entités nommées.
**** *annee_2009  *numText_557
L'extraction de motifs séquentiels fréquents dans les datastreams est un enjeu important traité par la communauté des chercheurs en fouille de données. Plus encore que pour les bases de données, de nombreuses contraintes supplémentaires sont à considérer de par la na-ture intrinsèque des streams. Dans cet article, nous proposons un nouvel algorithme en une passe : SPAMS, basé sur la construction incrémentale,avec une granularité très fine par transaction, d'un automate appelé SPA,permettant l'extraction des motifs séquentiels dans les streams. L'information du stream est apprise à la volée, au fur et à mesure de l'insertion de nouvelles transactions, sans pré-traitement a priori. Les résultats expérimentaux obtenus montrent la pertinence de la structure utilisée ainsi que l'efficience de notre algorithme appliqué à différents jeux de données.
**** *annee_2009  *numText_558
Nous présentons un nouvel algorithme incrémental et parallèle de Séparateur à Vaste Marge (SVM ou Support Vector Machine) pour la classification de très grands ensembles de données en utilisant le processeur de la carte graphique (GPUs, Graphics Processing Units). Les SVMs et les méthodes de noyaux permettent de construire des modèles avec une bonne précision mais ils nécessitent habituellement la résolution d'un programme quadratique ce qui requiert une grande quantité de mémoire et un long temps d'exécution pour les ensembles de données de taille importante. Nous présentons une extension de l'algorithme de Least Squares SVM (LS-SVM) proposé par Suykens et Vandewalle pour obtenir un algorithme incrémental et parallèle. Le nouvel algorithme est exécuté sur le processeur graphique pour obtenir une bonne performance à faible coût. Les résultats numériques sur les ensembles de données de l'UCI et Delve montrent que notre algorithme incrémental et parallèle est environ 70 fois plus rapide sur GPU que sur CPU et significativement plus rapide (plus de 1000 fois) que les algorithmes standards tels que LibSVM, SVM-perf et CB-SVM.
**** *annee_2009  *numText_561
Les réseaux dynamiques soulèvent de nouveaux problèmes d'analyses.Un outils efficace d'analyse doit non seulement permettre de décomposer ces réseaux en groupes d'éléments similaires mais il doit aussi permettre la détection de changements dans le réseau. Nous présentons dans cet article une nouvelle approche pour l'analyse de tels réseaux. Cette technique est basée sur un algorithme de décomposition de graphe en groupes chevauchants (ou chevauchement).La complexité de notre algorithme est O(|E| · deg2max +|V | · log(|V |))).La faible sensibilité de cet algorithme aux changements structuraux du réseau permet d'en détecter les modifications majeures au cours du temps.
**** *annee_2009  *numText_562
Nous présentons dans cet article un nouvel algorithme automatique pour l'apprentissage d'arbres de décision. Nous abordons le problème selon une approche Bayésienne en proposant, sans aucun paramètre, une expression analytique de la probabilité d'un arbre connaissant les données. Nous transformons le problème de construction de l'arbre en un problème d'optimisation : nous recherchons dans l'espace des arbres de décision, l'arbre optimum au sens du critère Bayésien ainsi défini, c'est à dire l'arbre maximum a posteriori (MAP).L'optimisation est effectuée en exploitant une heuristique de pré-élagage. Des expérimentations comparatives sur trente bases de l'UCI montrent que notre méthode obtient des performances prédictives proches de celles de l'état de l'art tout en étant beaucoup moins complexes.
**** *annee_2009  *numText_563
L'algorithme des forêts aléatoires proposé par Breiman permet d'obtenir de bons résultats en fouille de données comparativement à de nombreuses approches. Cependant, en n'utilisant qu'un seul attribut parmi un sous-ensemble d'attributs tiré aléatoirement pour séparer les individus à chaque niveau de l'arbre,cet algorithme perd de l'information. Ceci est particulièrement pénalisant avec les ensembles de données en grandes dimensions où il peut exister de nombreuses dépendances entre attributs. Nous présentons un nouvel algorithme de forêts aléatoires d'arbres obliques obtenus par des séparateurs à vaste marge(SVM). La comparaison des performances de notre algorithme avec celles de l'algorithme de forêts aléatoires des arbres de décision C4.5 et de l'algorithme SVM montre un avantage significatif de notre proposition.
**** *annee_2009  *numText_564
Le domaine « Qualité, Hygiène, Sécurité et Environnement »(QHSE) représente à l'heure actuelle un vecteur de progrès majeur pour l'industrie européenne. Le prototype « Semantic Quality Environment » (SQE)introduit dans cet article vise à démontrer la validité d'une architecture sémantique cross-lingue vouée à la collaboration multi-métiers et multilingue,dans le cadre d'un système banalisé de gestion de contenu d'entreprise dédié à l'industrie navale européenne.
**** *annee_2009  *numText_566
Dans ce papier, nous présentons une méthode de classification supervisée sans paramètre permettant d'attaquer les grandes volumétries. La méthode est basée sur des estimateurs de densités univariés optimaux au sens de Bayes,sur un classifieur Bayesien naïf amélioré par une sélection de variables et un moyennage de modèles exploitant un lissage logarithmique de la distribution a posteriori des modèles. Nous analysons en particulier la complexité algorithmique de la méthode et montrons comment elle permet d'analyser des bases de données nettement plus volumineuses que la mémoire vive disponible. Nous présentons enfin les résultats obtenu lors du récent PASCAL Large Scale Learning Challenge, où notre méthode a obtenu des performances prédictives de premier plan avec des temps de calcul raisonnables.
**** *annee_2009  *numText_567
La segmentation des images en régions est un problème crucial pour l'analyse et la compréhension des images. Parmi les approches existantes pour résoudre ce problème, la classification non supervisée est fréquemment employée lors d'une première étape pour réaliser un partitionnement de l'espace des intensités des pixels (qu'il s'agisse de niveaux de gris, de couleurs ou de réponses spectrales). Puisqu'elle ignore complètement les notions de voisinage des pixels, une seconde étape d'analyse spatiale (étiquetage en composantes connexes par exemple) est ensuite nécessaire pour identifier les régions issues de la segmentation. La non prise en compte de l'information spatiale est une limite majeure de ce type d'approche, ce qui a motivé de nombreux travaux où la classification est couplée à d'autres techniques pour s'affranchir de ce problème.Dans cet article, nous proposons une nouvelle formulation de la classification non supervisée permettant d'effectuer la segmentation des images sans faire appel à des techniques supplémentaires. Plus précisément, nous élaborons une méthode itérative de type k-means où les données à partitionner sont les pixels eux-mêmes (et non plus leurs intensités) et où les distances des points aux centres des classes ne sont plus euclidiennes mais topographiques. La segmentation est alors un processus itératif, et à chaque itération, les classes obtenues peuvent être assimilées à des zones d'influence dans le contexte de la morphologie mathématique. Ce parallèle nous permet de bénéficier des algorithmes efficaces proposés dans ce domaine (tels que ceux basés sur les files d'attente), tout en y ajoutant le caractère itératif des méthodes de classification non supervisée considérées ici. Nous illustrons finalement le potentiel de l'approche proposée par quelques résultats préliminaires de segmentation sur des images artificielles.
**** *annee_2009  *numText_568
Nous nous intéressons à l'utilisation de l'Analyse Factorielle des Correspondances (AFC) pour la recherche d'images par le contenu dans une base de données d'images volumineuse. Nous adaptons l'AFC, méthode originellement développée pour l'Analyse des Données Textuelles (ADT), aux images en utilisant des descripteurs locaux SIFT. En ADT, l'AFC permet de réduire le nombre de dimensions et de trouver des thèmes. Ici, l'AFC nous permettra de limiter le nombre d'images à examiner au cours de la recherche afin d'accélérer le temps de réponse pour une requête. Pour traiter de grandes bases d'images, nous proposons une version incrémentale de l'algorithme AFC. Ce nouvel algorithme découpe une base d'images en blocs et les charge dans la mémoire l'un après l'autre. Nous présentons aussi l'intégration des informations contextuelles (e.g.la Mesure de Dissimilarité Contextuelle (Jegou et al., 2007)) dans notre structure de recherche d'images. Cela améliore considérablement la précision. Nous exploitons cette intégration dans deux axes: (i) hors ligne (la structure de voisinage est corrigée hors ligne) et (ii) à la volée (la structure de voisinage des images est corrigée au cours de la recherche sur un petit ensemble d'images).
**** *annee_2009  *numText_569
Dans le domaine des flux des données, la prise en compte du temps s'avère nécessaire pour l'analyse de ces données car leur distribution sous-jacente peut changer au cours du temps. Un exemple typique concerne les modèles des profils de navigation des internautes. Notre objectif est d'analyser l'évolution de ces profils, celle-ci peut être liée au changement d'effectifs ou aux déplacement de clusters au cours du temps. Afin d'analyser la validité de notre approche,nous mettons en place une méthodologie pour la simulation des données d'usage à partir de laquelle il est possible de contrôler l'occurrence des changements
**** *annee_2009  *numText_571
Nous nous intéressons dans cet article aux représentations des relations spatiales pour l'extraction d'information et la modélisation des données visuelles, en particulier dans le contexte de la catégorisation d'images. Nous montrons comment la prise en compte d'une relation spatiale entre deux éléments entraîne l'apparition d'une information supplémentaire entre ces éléments et le reste de l'ensemble à modéliser, ce qui est rarement exploité explicitement.Une représentation floue des relations dans un modèle graphique est bien adaptée pour les algorithmes d'apprentissage utilisés actuellement et permet d'intégrer ce type d'information complémentaire qui concerne l'absence d'une interaction plutôt que sa présence. Nous tentons d'évaluer les bénéfices de cette approche sur un problème de traitement d'images.
**** *annee_2008  *numText_573
Les algorithmes de boosting de Newton Support Vector Machine (NSVM), Proximal Support Vector Machine (PSVM) et Least-Squares Support Vector Machine (LS-SVM) que nous présentons visent à la classification de très grands ensembles de données sur des machines standard. Nous présentons une extension des algorithmes de NSVM, PSVM et LS-SVM, pour construire des algorithmes de boosting. A cette fin, nous avons utilisé un terme de régularisation de Tikhonov et le théorème Sherman-Morrison- Woodbury pour adapter ces algorithmes au traitement d'ensembles de données ayant un grand nombre de dimensions. Nous les avons ensuite étendus par construction d'algorithmes de boosting de NSVM, PSVM et LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances des algorithmes sont évaluées sur des grands ensembles de données de l'UCI comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et RCV1-binary sur une machine standard (PC-P4, 2,4 GHz, 1024 Mo RAM).
**** *annee_2008  *numText_574
Les sites communautaires sont un endroit privilégié pour s'exprimer et publier des opinions. Le site www.flixster.com est un exemple de site participatif sur lequel se rassemblent plus de 20 millions de cinéphiles qui partagent des commentaires sur les films qu'ils ont ou non aimés. Explorer les contenus auto produits est un challenge pour qui veut comprendre les attentes des internautes. Par une méthode d'apprentissage non supervisée, nous montrerons qu'il est possible de mieux comprendre le vocabulaire utilisé pour décrire des opinions. En particulier, grâce à une méthode de co-clustering, nous montrerons qu'un rapprochement peut être fait entre des films particuliers sur la base de l'usage d'un vocabulaire particulier. L'analyse des résultats peut conduire à retrouver une certaine typologie de films ou encore des rapprochements entre films. Cette étude peut être complémentaire avec des analyses linguistiques des corpus, ou encore être exploitée dans un contexte applicatif de recommandation de contenus multimédias.
**** *annee_2008  *numText_575
Cet article propose une méthode d'alignement automatique de définitions destinée à améliorer la fusion entre des terminologies spécialisées et un vocabulaire médical généraliste par un classifieur de type SVM (Support Vecteur Machine) et une représentation compacte et pertinente d'un couple de définitions par concaténation d'un ensemble de mesures de similarité, afin de tenir compte de leur complémentarité, auquelle nous ajoutons les longueurs de chacune des définitions. Trois niveaux syntaxiques ont été investigués. Le modèle fondé sur un apprentissage à partir des groupes nominaux de type Noms-Adjectifs aboutit aux meilleures performances.
**** *annee_2008  *numText_576
Quotidiennement, plusieurs agences de presse publient des milliers d'articles contenant plusieurs événements de toutes sortes (politiques, économiques, culturels, etc.). Les preneurs de décision, se trouvent face à ce grand nombre d'événements dont seulement quelques uns les concernent. Le traitement automatique de tels événements devient de plus en plus nécessaires. Pour cela, nous proposons une approche, qui se base sur l'apprentissage automatique, et qui permet d'annoter les articles de presse pour générer un résumé automatique contenant les principaux événements. Nous avons validé notre approche par le développement du système AnnotEv.
**** *annee_2008  *numText_577
La recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l'apprentissage automatique. Le problème de classification de visages peut être considéré comme un problème d'apprentissage supervisé où les exemples d'apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d'apprentissage automatique supervisé. Ainsi, en se basant sur le fondement mathématique des treillis de Galois et leur utilisation pour la classification supervisée, nous proposons un nouvel algorithme de classification baptisé CITREC ainsi que son application pour la reconnaissance de visages. L'originalité de notre approche provient de la combinaison de l'analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne ou à plus proches voisins. Une validation expérimentale est décrite sur un benchmark du domaine de la reconnaissance de visages.
**** *annee_2008  *numText_578
Cet article porte sur l'analyse de parcours de vie représentés sous forme de séquences d'événements. Plus spécifiquement, on examine les possibilités d'exploiter des codages de type n-grammes de ces séquences pour en extraire des connaissances. En fait, compte tenu de la simultanéité de certains événements, une procédure stricte de n-grammes comme on peut par exemple l'appliquer sur des textes, n'est pas applicable ici. Nous discutons diverses alternatives qui s'avèrent finalement plus proches de la fouille de séquences fréquentes. Les concepts discutés sont illustrés sur des données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002. Enfin, on précisera sur quels aspects l'approche proposée peut apporter un éclairage complémentaire utile par rapport à d'autres techniques plus classiques d'analyse exploratoire de parcours de vie.
**** *annee_2008  *numText_579
Nous présentons dans cet article un système informatique pour le traitement des plaintes en lien avec des situations de pollution domestique écrites en français. Après la construction automatique d'une base de scenarii de plaintes, un module de recherche apparie la plainte à traiter à la thématique de la plainte la plus similaire. Enfin, il s'agit d'assigner au problème courant la solution correspondante au scénario de pollution auquel est affectée la plainte pertinente. Nous montrons ici l'intérêt de l'introduction dans l'appariement des textes de l'aspect sémantique géré par un dictionnaire généraliste de synonymes et en quoi il n'est pas réalisable pour notre problème particulier de construire une ontologie.
**** *annee_2008  *numText_580
Ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires. Notre approche est basée sur un modèle de mélange de lois de Bernoulli par blocs et les cartes de Kohonen probabilistes. La méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique.
**** *annee_2008  *numText_581
Nautilus est un logiciel d'analyse de bases de données. Le but de cette application est de généraliser l'utilisation de données clients au sein des entreprises. Elle facilite l'accès aux données en permettant de visualiser et manipuler les données du SGBD sous forme de concepts métiers. Elle inclut un générateur de requêtes SQL et un outil de gestion de tâches désignées pour l'agrégation de grands volumes de données. Le principe de fonctionnement est basé sur l'enchaînement de phases permettant la création des données d'analyse : importation des métadonnées du SGBD ; construction d'un dictionnaire de des concepts métiers ; spécification des champs à calculer. Les différents traitements tels que les jointures et l'alimentation des tables sont optimisés afin de rendre l'application utilisable sur des SGBD d'entreprise
**** *annee_2008  *numText_582
Ce travail s'inscrit dans le cadre de l'étude de la division cellulaire assurant la prolifération des cellules. Une meilleure compréhension de ce phénomène biologique nécessite l'identification des gènes caractérisant chaque phase du cycle cellulaire. Le procédé d'identification est généralement basé sur un ensemble de gènes dits gènes de référence, sélectionnés expérimentalement et considérés comme caractérisant les phases du cycle cellulaire. Les niveaux d'expression des gènes étudiés sont mesurés durant le cycle de la division cellulaire et permettent de construire des profils d'expression. Chaque gène étudié est affecté à la phase du cycle cellulaire correspondant au groupe de gènes de référence le plus similaire. Cette approche classique souffre de deux limites. D'une part les mesures de proximité les plus couramment utilisés entre profils d'expression de gènes sont basées sur les écarts en valeurs sans tenir compte de la forme des profils. D'autre part, dans la littérature, il n'y a pas consensus quant à l'ensemble des gènes de référence à considérer. Dans cet article, notre but est de proposer une classification adaptative, basée sur un indice de dissimilarité incluant les proximités en valeurs et en forme des profils d'expression de gènes, permettant d'identifier les phases d'expression des gènes étudiés, et de présenter un nouvel ensemble de gènes de référence validé par une connaissance biologique.
**** *annee_2008  *numText_584
Le clustering est une tâche fondamentale de la fouille de données. Ces dernières années, les méthodes de type cluster ensembles ont été l'objet d'une attention soutenue. Il s'agit d'agréger plusieurs clusterings d'un jeu de données afin d'obtenir un clustering moyen. Les clusterings individuels peuvent être le résultat de différents algorithmes. Ces méthodes sont particulièrement utiles lorsque la dimensionalité des données ne permet pas aux méthodes classiques basées sur la distance et/ou la densité de fonctionner correctement. Dans cet article, nous proposons une méthode pour obtenir des clusterings individuels à faible coût, à partir de projections partielles du jeu de données. Nous évaluons empiriquement notre méthode et la comparons à trois méthodes de différents types. Nous constatons qu'elle donne des résultats sensiblement supérieurs aux autres.
**** *annee_2008  *numText_585
Dans cet article, nous proposons une approche qui combine les méthodes statistiques avancées et la flexibilité des approches interactives manuelles en clustering visuel. Nous présentons l'interface Semi-Supervised Visual Clustering (SSVC). Sa contribution principale est l'apprentissage d'une métrique de projection optimale pour la visualisation en coordonnées en étoiles ainsi que pour l'extension 3D que nous avons développée. La métrique de distance de projection est apprise à partir des retours de l'utilisateur soit en termes de similarité/ dissimilarité entre les items, soit par l'annotation directe. L'interface SSVC permet, de plus, une utilisation hybride dans laquelle un ensemble de paramètres sont manuellement fixés par l'utilisateur tandis que les autres paramètres sont déterminés par un algorithme de distance optimale.
**** *annee_2008  *numText_586
Dans de nombreuses applications, une co-classification est plus facile à interpréter qu'une classification mono-dimensionnelle. Il s'agit de calculer une bi-partition ou collection de co-clusters : chaque co-cluster est un groupe d'objets associé à un groupe d'attributs et les interprétations peuvent s'appuyer naturellement sur ces associations. Pour exploiter la connaissance du domaine et ainsi améliorer la pertinence des partitions, plusieurs méthodes de classification sous contraintes ont été proposées pour le cas mono-dimensionnel, e.g., l'exploitation de contraintes must-link et cannot-link. Nous considérons ici la co-classification sous contraintes avec la gestion de telles contraintes étendues aux dimensions des objets et des attributs, mais aussi l'expression de contraintes de contiguité dans le cas de domaines ordonnés. Nous proposons un algorithme itératif qui minimise la somme des résidus quadratiques et permet l'exploitation active des contraintes spécifiées par les analystes. Nous montrons la valeur ajoutée de ce type d'extraction sur deux applications en analyse du transcriptome.
**** *annee_2008  *numText_587
Les avancées technologiques récentes du Web et du sans fil,conjuguées au succès des applications spatialisées grand public, sont à l'origine d'un accès accru aux systèmes d'information spatio-temporelle(SIST) par une grande diversité d'utilisateurs, munis des dispositifs d'accès et dans des contextes d'utilisation variés. Adapter ces systèmes à l'utilisateur devient donc une nécessité, un gage d'utilisabilité et de pérennité. Cet article présente une approche générique pour la conception et la génération de systèmes d'information spatio-temporelle adaptés à l'utilisateur, appelé ASTIS. ASTIS offre des modalités générales de mise en œuvre de l'adaptation à l'utilisateur, visant tant le contenu que la présentation des applications. Elle permet aux concepteurs d'intégrer ces modalités d'adaptation dans des applications traitant des données spatio-temporelles. Afin de définir les besoins et types d'adaptation propres à leur application, il suffit aux concepteurs de créer des modèles conceptuels, par spécialisation et instanciation des modèles offerts par notre architecture
**** *annee_2008  *numText_589
Les travaux autour de l'extraction de motifs séquentiels se sont particulièrement focalisés sur la définition d'approches efficaces pour extraire, en fonction d'une fréquence d'apparition, des corrélations entre des éléments dans des séquences. Même si ce critère de fréquence est déterminant, le décideur est également de plus en plus intéressé par des connaissances qui sont représentatives d'un comportement inattendu dans ces données (erreurs dans les données, fraudes, nouvelles niches, ... ). Dans cet article, nous introduisons le problème de la détection de motifs séquentiels inattendus par rapport aux croyances du domaine. Nous proposons l'approche USER dont l'objectif est d'extraire les motifs séquentiels et les règles inattendues dans une base de séquences.
**** *annee_2008  *numText_590
Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l'inégalité de Hoeffding pour obtenir une borne probabiliste sur l'écart entre la valeur attendue et la valeur estimée.
**** *annee_2008  *numText_591
Une tâche importante en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d'individus. Quelles sont les catégories d'individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ? Nous présentons le problème d'extraction de tels groupes atypiques vis-à-vis d'une variable cible quantitative, comme par exemple la variable salaire, et plus particulièrement pour les faibles et fortes valeurs d'un intervalle déterminé par l'utilisateur. Il s'agit donc de rechercher des conjonctions de variables dont la distribution diffère significativement de celle de l'ensemble d'apprentissage pour les faibles et fortes valeurs de l'intervalle de cette variable cible. Une adaptation d'une mesure statistique existante, l'intensité d'inclination, nous permet de découvrir de tels groupes atypiques. Cette mesure nous libère de l'étape de transformation des variables quantitatives, à savoir l'étape de discrétisation suivie d'un codage disjonctif complet. Nous proposons donc un algorithme d'extraction de tels groupes avec des règles d'élagage pour réduire la complexité du problème. Cet algorithme a été développé et intégré au logiciel d'extraction de connaissances WEKA. Nous terminons par un exemple d'extraction sur la base de données IPUMS du bureau de recensement américain.
**** *annee_2008  *numText_592
Les arbres de décision sont largement utilisés pour générer des classificateurs à partir d'un ensemble de données. Le processus de construction est une partitionnement récursif de l'ensemble d'apprentissage. Dans ce contexte, les attributs continus sont discrétisés. Il s'agit alors, pour chaque variable à discrétiser de trouver l'ensemble des points de coupure. Dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage, comme le BOOTSTRAP conduit à des meilleurs résultats. Nous avons testé cette approche avec les méthodes principales de discrétisation comme MDLPC, FUSBIN, FUSINTER, CONTRAST, Chi-Merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap. Nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction d'arbres de décision.
**** *annee_2008  *numText_593
Dans nombre d'applications, les données présentent un déséquilibre entre les classes. La prédiction est alors souvent détériorée pour la classe minoritaire. Pour contourner cela, nous proposons un échantillonnage guidé, lors des itérations successives d'une forêt aléatoire, par les besoins de l'utilisateur.
**** *annee_2008  *numText_594
Depuis quelques années, la communauté fouille de données s'est intéressée à la problématique de l'extraction de motifs séquentiels à partir de grandes bases de données en considérant comme hypothèse que les données pouvaient être chargées en mémoire centrale. Cependant, cette hypothèse est mise en défaut lorsque les bases manipulées sont trop volumineuses. Dans cet article, nous étudions une technique d'échantillonnage basée sur des réservoirs et montrons comment cette dernière est particulièrement bien adaptée pour résumer de gros volumes de données. Nous nous intéressons ensuite à la problématique plus récente de la fouille sur des données disponibles sous la forme d'un flot continu et éventuellement infini (data stream). Nous étendons l'approche d'échantillonnage à ce nouveau contexte et montrons que nous sommes à même d'extraire des motifs séquentiels de flots tout en garantissant les taux d'erreurs sur les résultats. Les différentes expérimentations menées confirment nos résultats théoriques.
**** *annee_2008  *numText_595
Ces dernières années, sont apparues de nombreuses applications, utilisant des données potentiellement infinies, provenant de façon continue de capteurs distribués. On retrouve ces capteurs dans des domaines aussi divers que la météorologie (établir des prévisions), le domaine militaire (surveiller des zones sensibles), l'analyse des consommations électriques (transmettre des alertes en cas de consommation anormale),... Pour faire face à la volumétrie et au taux d'arrivée des flux de données, des traitements sont effectués 'à la volée' sur les flux. En particulier, si le système n'est pas assez rapide pour traiter toutes les données d'un flux, il est possible de construire des résumés de l'information. Cette communication a pour objectif de faire un premier point sur nos travaux d'échantillonnage dans un environnement de flux de données fortement distribués. Notre approche est basée sur la théorie des sondages, l'analyse des données fonctionnelles et la gestion de flux de données. Cette approche sera illustrée par un cas réel : celui des mesures de consommations électriques
**** *annee_2008  *numText_597
La classification recouvrante désigne les techniques de regroupements de données en classes pouvant s'intersecter. Particulièrement adaptés à des domaines d'application actuels (e.g. Recherche d'Information, Bioinformatique) quelques modèles théoriques de classification recouvrante ont été proposés très récemment parmi lesquels le modèle MOC (Banerjee et al. (2005a)) utilisant les modèles de mélanges et l'approche OKM (Cleuziou (2007)) consistant à généraliser l'algorithme des k-moyennes. La présente étude vise d'une part à étudier les limites théoriques et pratiques de ces deux modèles, et d'autre part à proposer une formulation de l'approche OKM en terme de modèles de mélanges gaussiens, laissant ainsi entrevoir des perspectives intéressantes quant à la variabilité des schémas de recouvrements envisageables.
**** *annee_2008  *numText_598
Cet article présente une méthode efficace pour l'extraction de règles d'influence quantitatives positives et négatives. Ces règles d'influence introduisent une nouvelle sémantique qui vise à faciliter l'analyse d'un volume important de données. Cette sémantique fixe la direction de la règle entre deux variables en positionnant, au préalable, l'une comme étant l'influent et l'autre comme étant l'influé. Elle permet, de ce fait, d'exprimer la nature de l'influence : positive, en maximisant le nombre d'éléments en commun ou négative, en maximisant le nombre d'éléments qui violent l'influé. Notre approche s'appuie sur une stratégie qui comporte cinq étapes dont deux exécutées en parallèle. Ces deux étapes constituent les étapes clé de notre approche. La première combine une méthode d'élagage et de regroupement tabulaire basée sur les tableaux de contingence. Cette dernière construit et classe les zones potentiellement intéressantes. La seconde, injecte la sémantique et évalue le degré d'influence que produirait l'introduction d'une nouvelle variable sur un ensemble de variables en utilisant une nouvelle mesure d'intérêt, l'Influence. Cette étape vient affiner les résultats de la première étape, et permet de se focaliser sur des zones valides par rapport aux contraintes spécifiées. Enfin, un système de règles d'influence jugées intéressantes est construit basé sur la juxtaposition des résultats des deux étapes clé de notre approche.
**** *annee_2008  *numText_599
Pour construire des arbres de décision sur des données déséquilibrées, des auteurs ont proposés des mesures d'entropie asymétriques. Le problème de l'évaluation de ces arbres se pose ensuite. Cet article propose d'évaluer la qualité d'arbres de décision basés sur une mesure d'entropie asymétrique.
**** *annee_2008  *numText_600
L'analyse sémantique latente (LSA - Latent Semantic Analysis) est aujourd'hui utilisée dans de nombreux domaines comme la modélisation cognitive, les applications éducatives mais aussi pour la classification. L'approche présentée dans cet article consiste à ajouter des informations grammaticales à LSA. Différentes méthodes pour exploiter ces informations grammaticales sont étudiées dans le cadre d'une tâche de classification conceptuelle.
**** *annee_2008  *numText_601
L'extraction d'itemsets fréquents est un sujet majeur de l'ECD et son but est de découvrir des corrélations entre les enregistrements d'un ensemble de données. Cependant, le support est calculé en fonction de la taille de la base dans son intégralité. Dans cet article, nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans l'organisation des données et qui contiennent des itemsets fréquents sur ces périodes. Nous proposons ainsi la définition des itemsets compacts, qui représentent un comportement cohérent sur une période spécifique et nous présentons l'algorithme DEICO qui permet leur découverte.
**** *annee_2008  *numText_602
L'extraction de motifs séquentiels permet de découvrir des corrélations entre événements au cours du temps. Introduisant plusieurs dimensions d'analyse, les motifs séquentiels multidimensionnels permettent de découvrir des motifs plus pertinents. Mais le nombre de motifs obtenus peut devenir très important. C'est pourquoi nous proposons, dans cet article, de définir une représentation condensée garantie sans perte d'information : les motifs séquentiels multidimensionnels clos extraits ici sans gestion d'ensemble de candidats.
**** *annee_2008  *numText_603
Dans le suivi et la modélisation de l'érosion en montagne, la représentation fine du relief est une composante importante. En effet, la connaissance des zones de concentration des eaux, notamment à travers l'apparition de rigoles élémentaires, est fondamentale pour bien décrire les connectivités entre les zones de mobilisation des sédiments sur le versant et le réseau hydrographique stabilisé. La résolution au sol permise par les photographies aériennes classiques ne permet pas d'accéder à une représentation 3D suffisamment fine des ravines élémentaires. Nous testons l'utilisation de photographies stéréoscopiques à résolution centimétrique prises à basse altitude par un drone pour obtenir un MNT précis. La question majeure concerne les règles à suivre pour un meilleur compromis entre précision et facilité d'élaboration, et l'évaluation de l'importance relative de chaque étape sur la qualité finale de la restitution. La zone d'étude est située dans les Badlands de Draix (Alpes de Haute Provence).
**** *annee_2008  *numText_604
Dans la perspective d'offrir un web sémantique, des travaux ont cherché à automatiser l'extraction des annotations sémantiques à partir de textes pour représenter au mieux la sémantique que vise à transmettre une page web. Dans cet article nous proposons une approche d'extraction des annotations qui représentent le plus précisément possible le contenu d'un document. Nous proposons de prendre en compte la notion de contexte modélisé par des relations contextuelles émanant, à la fois, de la structure et de la sémantique du texte.
**** *annee_2008  *numText_606
Nous présentons dans cet article un nouvel algorithme permettant la construction et la mise à jour incrémentale du FIA : FIASCO. Notre algorithme effectue un seul passage sur les données et permet de prendre en compte les nouveaux batches, itemset par itemset et pour chaque itemset, item par item.
**** *annee_2008  *numText_607
Cet article présente une contribution à la modélisation acoustique des mots à partir de grands corpus oraux, faisant appel aux techniques de fouilles de données. En transcription automatique, de nombreuses erreurs concernent des mots fréquents homophones. Deux paires de mots (quasi-)homophones à/a et et/est sont sélectionnées dans les corpus, pour lesquels sont définis et examinés 41 descripteurs acoustiques permettant potentiellement de les distinguer. 17 algorithmes de classification, mis à l'épreuve pour la discrimination automatique de ces deux paires de mots, donnent en moyenne 77% de classification correcte sur les 5 meilleurs algorithmes. En réduisant le nombre de descripteurs à 10 (sélectionnés par l'algorithme le plus performant), les résultats de classification restent proches du résultat obtenu avec 41 attributs. Cette comparaison met en évidence le caractère discriminant de certains attributs, qui pourront venir enrichir à la fois la modélisation acoustique et nos connaissances des prononciations de l'oral.
**** *annee_2008  *numText_609
Cet article propose une approche d'abstraction des séquences vidéo basée sur le soft computing. Etant donné une longueur cible du condensé vidéo, on cherche les segments vidéo qui couvrent le maximum du visuel de la vidéo originale en respectant la longueur du condensé.
**** *annee_2008  *numText_610
Longtemps les ontologies ont été limitées à des domaines scientifiques et techniques, favorisant au passage l'essor du concept de « connaissances universelles et objectives ». Avec l'émergence et l'engouement actuel pour les sciences cognitives, couplés à l'application des ontologies à des domaines relatifs aux Sciences Humaines et Sociales (SHS), la subjectivité des connaissances devient une dimension incontournable qui se doit d'être intégrée et prise en compte dans le processus d'ingénierie ontologique (IO). L'objectif de nos travaux est de développer la notion d'Ontologie Pragmatisée Vernaculaire de Domaine (OPVD). Le principe sous-jacent à de telles ressources consiste à considérer que chaque ontologie est non seulement propre à un domaine, mais également à un endogroupe donné, doté d'une pragmatique qui est fonction tant de la culture que de l'apprentissage et de l'état émotionnel du dit endogroupe. Cette pragmatique, qui traduit un processus d'appropriation et de personnalisation de l'ontologie considérée, est qualifiée à l'aide de deux mesures : un gradient de prototypicalité conceptuelle et un gradient de prototypicalité lexicale
**** *annee_2008  *numText_611
Le groupe de recherche Hypercarte propose HyperSmooth,un nouvel outil cartographique pour l'analyse spatiale de phénomènes sociaux économiques mettant en œuvre une méthode de calcul de potentiel. L'objectif est de pouvoir représenter de façon continue et en changeant d'échelle d'analyse une information statistique échantillonnée sur toutes sortes de maillages, réguliers ou non. Le défi technologique est de fournir un outil accessible sur le Web, interactif et rapide, ceci malgré le coût élevé du calcul, et qui assure la confidentialité des données. Nous présentons notre solution basée sur une architecture client serveur : le serveur calcule les cartes de potentiel en utilisant des techniques d'optimisation particulières, alors que le client est en charge de la visualisation et du paramétrage de l'analyse, et les deux parties communiquent via un protocole Web.
**** *annee_2008  *numText_612
L'informatique décisionnelle est un secteur en forte croissance dans toutes les entreprises. Les techniques classiques (reporting simple & Olap), qui s'intéressent essentiellement à présenter les données, sont aujourd'hui très largement déployées. Le data mining commence à se répandre, apportant des capacités de prévision à forte valeur ajoutée pour les entreprises les plus compétitives. Ce développement est rendu possible par la disponibilité croissante de masses de données importantes et la puissance de calcul dorénavant disponible. Cependant, la mise en œuvre industrielle des projets de data mining pose des contraintes tant théoriques (quels algorithmes utiliser pour produire des modèles d'analyses exploitant des milliers de variables pour des millions d'exemples) qu'opérationnelles (comment mettre en production et contrôler le bon fonctionnement de centaines de modèles). Je présenterai ces contraintes issues des besoins des entreprises ; je montrerai comment exploiter des résultats théoriques (provenant des travaux de Vladimir Vapnik) pour produire des modèles robustes ; je donnerai des exemples d'applications réelles en gestion de la relation client et en analyse de qualité. Je conclurai en présentant quelques perspectives (utilisation du texte et des réseaux sociaux).
**** *annee_2008  *numText_613
Le travail présenté dans cet article décrit une nouvelle version des cartes topologiques que nous appelons CrTM. Cette version consiste à modifier l'algorithme de Kohonen de telle façon à ce qu'il contrôle les violations des contraintes lors de la construction de la topologie de la carte. Nous validons notre approche sur des données connues de la littérature en utilisant des contraintes artificielles. Une validation supplémentaire sera faite sur des données réelles issues d'images médicales pour la classification des mélanomes chez l'humain sous contraintes médicales.
**** *annee_2008  *numText_614
En fouille de textes comme en recherche d'information, différents modèles, de type probabiliste, vectoriel ou booléen, se sont révélés bien adaptés pour représenter des documents textuels mais, ces modèles présentent l'inconvénient de ne pas tenir compte de la structure du document. Or la plupart des informations disponibles aujourd'hui sur Internet ou dans des bases documentaires sont fortement structurées. Dans cet article, nous proposons d'étendre le modèle probabiliste de représentation des documents de façon à tenir compte du poids d'une certaine catégorie d'éléments structurels : les balises représentant la structure logique et la structure de mise en forme. Ce modèle a été évalué à l'aide de la collection de la campagne d'évaluation INEX 2006.
**** *annee_2008  *numText_615
De larges corpus à fort ancrage territorial deviennent disponibles sous forme numérique dans les médiathèques et plus particulièrement dans les médiathèques de dimension régionale. Les défis qu'offrent ces gigas octets de documents bruts sont énormes en terme de traitement automatique des contenus.Nous proposons dans cet article deux modèles computationnels et une méthode complète permettant de réaliser un traitement automatique afin d'extraire des itinéraires dans des textes relatant des récits de voyage. Le premier modèle est un modèle des attendus. Il s'intéresse au concept d'itinéraire et adopte le point de vue du pédagogue et fait intervenir très tôt les usages envisagés. Le deuxième modèle est un modèle d'extraction, il permet de modéliser l'expression du déplacement dans des textes du genre récit de voyage. Nous proposons alors une méthode automatique pour : d'une part extraire et interpréter automatiquement les déplacements d'un récit et d'autre part passer des déplacements à l'itinéraire,c'est-à-dire alimenter de manière automatique le modèle des attendus à partir du modèle d'extraction. Nous montrons également comment les itinéraires extraits interviennent soit dans la phase de construction d'activités pédagogiques soit directement comme matériau dans une activité d'apprentissage. Nous présentons enfin ¼R, un Prototype pour l'Interprétation d'Itinéraires dans des Récits de voyages, qui implémente notre approche. Il prend en entrée un texte brut et fournit l'interprétation de l'itinéraire décrit dans le texte. Il permet également de visualiser sur un fond cartographique l'itinéraire extrait.
**** *annee_2008  *numText_616
Les approches de fouille et d'interprétation d'images consistant à considérer les pixels de façon indépendante ont montré leurs limites pour l'analyse d'images complexes. Pour résoudre ce problème, de nouvelles méthodes s'appuient sur une segmentation préalable de l'image qui consiste en une agrégation des pixels connexes afin de former des régions homogènes au sens d'un certain critère. Cependant le lien est souvent complexe entre la connaissance de l'expert sur les objets qu'il souhaite identifier dans l'image et les paramètres nécessaires à l'étape segmentation permettant de les identifier. Dans cet article la connaissance de l'expert est modélisée dans une ontologie qui est ensuite utilisée pour guider un processus de segmentation par une approche évolutive. Cette méthode trouve automatiquement des paramètres de segmentation permettant d'identifier les objets décrits par l'expert dans l'ontologie.
**** *annee_2008  *numText_617
Khiops est un outil de préparation des données et de modélisation pour l'apprentissage supervisé et non supervisé. L'outil permet d'évaluer de façon non paramétrique la corrélation entre tous types de variables dans le cas non supervisé et l'importance prédictive des variables et paires de variables dans le cas de la classification supervisée. Ces évaluations sont effectuées au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel, ce qui permet de rechercher une représentation des données efficace au moyen d'un recodage des variables. L'outil produit également un modèle de scoring pour les tâches d'apprentissage supervisé, selon un classifieur Bayesien naif avec sélection de variables et moyennage de modèles. L'outil est adapté à l'analyse des grandes bases de données, avec des centaines de milliers d'individus et des dizaines de milliers de variables, et a permis de participer avec succès à plusieurs challenges internationaux récents.
**** *annee_2008  *numText_618
Dans un contexte d'ingénierie de la connaissance, l'analyse des données relationnelles évolutives est une question centrale. La représentation de ce type de données sous forme de graphe optimisé en facilite l'analyse et l'interprétation par l'utilisateur non expert. Cependant, ces graphes peuvent rapidement devenir trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à en faciliter la lecture et l'analyse. Pour cela, une solution est de les simplifier, dans un premier temps, en un graphe réduit dont les sommets représentent chacun un groupe distinct de sommets : acteurs ou termes du domaine étudié. Dans un second temps, il faut les décomposer en instances (un graphe par période) afin de prendre en compte la dimension temporelle.La plate-forme de veille stratégique Tétralogie, développée dans notre laboratoire, permet de synthétiser les données relationnelles évolutives sous forme de matrices de cooccurrence 3D et VisuGraph, son module de visualisation, permet de les représenter sous forme de graphes évolutifs. VisuGraph assimile les différentes périodes à des repères temporels et chaque sommet est placé en fonction de son degré d'appartenance aux différentes périodes. Ce prototype est aussi doté d'un module de la classification interactive de données relationnelles basé sur une technique de Markov Clustering, qui conduit à une visualisation sous forme de graphe réduit. Nous proposons ici de prendre en compte la dimension temporelle dans notre processus de classification des données. Ainsi, par la visualisation successive des différentes instances, il devient plus facile d'analyser l'évolution des classes au niveau intra mais aussi au niveau inter classes.
**** *annee_2008  *numText_619
Le FIA (Frequent Itemset Automaton) est un nouvel automate qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. Cette structure de données est très compacte et informative, et elle présente également des propriétés incrémentales intéressantes pour les mises à jour avec une granularité très fine. L'algorithme développé pour la mise à jour du FIA effectue un unique passage sur les données qui sont prises en compte tout d'abord par batch (i.e., itemset par itemset), puis pour chaque itemset, item par item. Nous montrons que dans le cadre d'une approche prédictive et par l'intermédiaire de la bordure statistique, le FIA permet d'indexer les itemsets véritablement fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur la pertinence statistique des itemsets indexés avec la P-valeur.
**** *annee_2008  *numText_620
L'exploitation des réseaux sociaux pour l'extraction de connaissances n'est pas nouvelle. Les anthropologues, sociologues et épidémiologies se sont déjà penchés sur la question. C'est probablement le succès du moteur de recherche Google qui a vulgarisé l'utilisation des parcours aléatoires des réseaux sociaux pour l'ordonnancement par pertinence. Plusieurs applications ont depuis vu naissance. La découverte des communautés dans les réseaux sociaux est aussi une nouvelle tendance de recherche très prisée. Durant cet exposé nous parlerons de l'analyse des réseaux sociaux, la découverte de communautés, et présenterons quelques applications dont l'ordonnancement dans les bases de données
**** *annee_2008  *numText_622
Une carte cognitive fournit une représentation graphique d'un réseau d'influence entre des concepts. Les cartes cognitives de dimensions importantes ont l'inconvénient d'être difficiles à appréhender, interpréter et exploiter. Cet article présente un modèle de cartes cognitives hiérarchiques permettant au concepteur d'effectuer des regroupements de concepts qui sont ensuite utilisés dans un mécanisme permettant à l'utilisateur d'obtenir des vues partielles et synthétiques d'une carte.
**** *annee_2008  *numText_623
Le diagnostic de territoire constitue une étape obligatoire dans tout projet d'aménagement ou dans toute volonté politique de modifier durablement l'espace. Les décideurs politiques doivent avoir une vision objective des actions à mener en fondant leurs réflexions sur des études et des documents ;qu'ils soient à caractère géographique ou non. Il est donc fondamental d'améliorer l'accès et la consultation, par les décideurs stratégiques, de ce que l'on peut appeler des documents géographiques. Le but de cet article est de présenter certains concepts et solutions technologiques qui peuvent être utilisés afin de mieux organiser, de naviguer (dans) et de visualiser ces documents. Il propose une mise en perspective commune de certaines de ces approches, sur laquelle est fondée la conception d'une première maquette d'un outil de visualisation(et de navigation) de documents géographiques nommé GEOdoc.
**** *annee_2008  *numText_624
L'annotation d'une protéine consiste, entre autres, à lui attribuer une classe dans une hiérarchie fonctionnelle. Celle-ci permet d'organiser les connaissances biologiques et d'utiliser un vocabulaire contrôlé. Pour estimer la pertinence des annotations, des mesures telles que la précision, le rappel, la spécificité et le Fscore sont utilisées. Cependant ces mesures ne sont pas toujours bien adaptées à l'évaluation de données hiérarchiques, car elles ne permettent pas de distinguer les erreurs faites aux différents niveaux de la hiérarchie. Nous proposons ici une représentation formelle pour les différents types d'erreurs adaptés à notre problème.
**** *annee_2008  *numText_625
En s'appuyant sur la théorie de l'activité, nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages. Nous l'avons testé avec la réalisation d'un e-service d'apprentissage instrumental de pièces de musique à la guitare (E-guitare).
**** *annee_2008  *numText_626
Un des problèmes majeurs dans la gestion des ontologies est son évaluation. Cet article traite l'évaluation des concepts ontologiques qui sont extraits de pages Web. Pour cela, nous avons proposé une méthodologie d'évaluation des concepts basée trois critères révélateurs : le degré de crédibilité; le degré de cohésion et le degré d'éligibilité. Chaque critère correspond à un apport de connaissance pour la tâche d'évaluation. Notre méthode d'évaluation assure une évaluation qualitative grâce aux associations de mots ainsi qu'une évaluation quantitative par le biais des trois degrés. Nos résultats et discussions avec les experts et les utilisateurs ont montré que notre méthode facilite la tâche d'évaluation.
**** *annee_2008  *numText_628
Une perception intelligente du mouvement d'objets mobiles(personnes, voitures, colis, etc.) est à la base de nombreuses applications (par exemple le suivi d'une distribution postale à travers le monde, l'optimisation du trafic routier ou l'étude de la migration d'animaux). Les systèmes de gestion de bases de données actuels n'offrent ni les concepts ni les fonctions nécessaires à une analyse sémantique du mouvement, se limitant au stockage et à l'interrogation de positions spatiales individuelles, hors contexte temporel. Destravaux de recherche précédents ont introduit et développé le concept d'objet mobile ou spatio-temporel. Dans cet article nous allons plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquelle se construit la vision applicative. Nous proposons de décrire les trajectoires, au niveau conceptuel, avec leurs aspects géométriques, temporels et sémantiques et leurs composants structurels : point de départ, point d'arrivée, arrêts et déplacements intermédiaires. Chaque élément, trajectoire, arrêt, déplacement,voire partie de déplacement, peut recevoir des annotations sémantiques sous forme de valeurs d'attributs ou de liens vers des objets de la base. L'approche de modélisation décrite dans cet article est basée sur les patrons de modélisation, qui permettent une solution générique pour modéliser les caractéristiques standard des trajectoires tout en étant ouverte aux caractéristiques spécifiques à l'application envisagée. Enfin, l'implémentation dans une base de données relationnelle étendue est présentée.
**** *annee_2008  *numText_629
On utilise l'analyse factorielle des correspondances (AFC) pour la recherche d'images par le contenu en s'inspirant directement de son utilisation en analyse des données textuelles (ADT). L'AFC permet ici de réduire les dimensions du problème et de sélectionner des indicateurs pertinents pour la recherche par le contenu. En ADT, l'AFC est appliquée à un tableau de contingence croisant mots et documents. La première étape consiste donc à définir des « mots visuels » dans les images (analogue des mots dans les textes). Ces mots sont construits à partir des descripteurs locaux (SIFT) des images. La méthode a été testée sur la base Caltech4 (Sivic et al., 2005) sur laquelle elle fournit de meilleurs résultats (qualité des résultats de recherche et temps d'exécution) que des méthodes plus classiques comme TF*IDF/Rocchio (Rocchio, 1971) ou pLSA (Hofmann, 1999a, 1999b). Enfin, pour passer à l'échelle et améliorer la qualité de recherche, nous proposons un nouveau prototype de recherche qui utilise des fichiers inversés basés sur la qualité de représentation des images sur les axes après avoir fait une AFC. Chaque fichier inversé est associé à une partie d'un axe (positive ou négative) et contient des images ayant une bonne qualité de représentation sur cet axe. Les tests réalisés montrent que ce nouveau prototype réduit le temps de recherche sans perte de qualité de résultat et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive.
**** *annee_2008  *numText_630
L'analyse de risques est un processus visant à décrire les scénarios conduisant à des phénomènes dangereux et à des accidents potentiels sur une installation industrielle. Pour réaliser une analyse de risques, un expert dispose de nombreuses ressources : rapports, études de dangers, bases d'accidents, etc. Ces ressources sont cependant souvent difficiles à exploiter parce qu'elles ne sont pas suffisamment structurées ni formalisées. Dans le cadre du projet KMGR (Knowledge Management pour la Gestion des Risques), mené en partenariat avec l'Institut National de l'Environnement industriel et des RISques (INERIS), nous proposons de traiter ce problème en développant un système de recherche d'information basé sur des ontologies, et de le compléter par un système de raisonnement à partir de cas (RàPC) pour tenir compte des expériences passées.
**** *annee_2008  *numText_631
L'apprentissage de SVM par optimisation directe du primal est très étudié depuis quelques temps car il ouvre de nouvelles perspectives notamment pour le traitement de données structurées. Nous proposons un nouvel algorithme de ce type qui combine de façon originale un certain nombre de techniques et idées comme la méthode du sous-gradient, l'optimisation de fonctions continues non partout différentiables, et une heuristique de shrinking.
**** *annee_2008  *numText_632
Les réseaux de neurones RBF sont d'excellents régresseurs. Ils sont cependant difficiles à utiliser en raison du nombre de paramètres libres : nombre de neurones, poids des connexions, ... Des algorithmes évolutionnaires permettent de les optimiser mais ils sont peu nombreux et complexes.Nous proposons ici un nouvel algorithme, RBF-Gene, qui permet d'optimiser la structure et les poids du réseau, grâce à une inspiration biologique. Il est compétitif avec les autres techniques de régression mais surtout l'évolution peut choisir dynamiquement le nombre de neurones et la précision des différents paramètres.
**** *annee_2008  *numText_633
Dans cet article, nous proposons une nouvelle approche de pondérations des variables durant un processus d'apprentissage non supervisé. Cette méthode se base sur l'algorithme « batch » des cartes auto-organisatrices. L'estimation des coefficients de pondération se fait en parallèle avec la classification automatique. Ces pondérations sont locales et associées à chaque référent de la carte auto-organisatrice. Elles reflètent l'importance locale de chaque variable pour la classification. Les pondérations locales sont utilisées pour la segmentation de la carte topologique permettant ainsi un découpage plus riche tenant compte des pertinences des variables. Les résultats de l'évaluation montrent que l'approche proposée, comparée à d'autres méthodes de classification, offre une segmentation plus fine de la carte et de meilleure qualité.
**** *annee_2008  *numText_634
Un grand nombre de réactions chimiques sont aujourd'hui répertoriées dans des bases de données. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents. Deux obstacles s'opposent à cela : d'une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d'extraire les schémas de réactions fréquents. D'autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Le présent article décrit un processus de prétraitement opérationnel qui permet de filtrer, compléter puis transformer le contenu d'une base de réactions en des données fiables constituées de graphes abstraits répondant au problème de la fouille de schémas de réactions. Le processus place ainsi les bases de réactions à portée des techniques de fouille de graphes comme en attestent les résultats expérimentaux.
**** *annee_2008  *numText_635
L'analyse des données Symboliques a pour objectif de fournir des résultats complémentaires à ceux fournis par la fouille de données classique en créant des concepts issus de données simples ou complexes puis en analysant ces concepts par des descriptions symboliques où les variables expriment la variation des instances de ces concepts en prenant des valeurs intervalle, histogramme,suites, munies de règles et de taxonomies, etc.
**** *annee_2008  *numText_636
Le logiciel présenté dans cet article s'appuie sur une approche d'acquisition de sigles à partir de données textuelles
**** *annee_2008  *numText_637
Les systèmes de détection d'intrusions (SDIs) ont pour objectif la sécurité des réseaux informatiques. Dans ce papier, nous proposons une nouvelle approche de détection d'intrusions basée sur des règles associatives génériques de classification pour améliorer la qualité de la détection d'intrusions.
**** *annee_2008  *numText_638
L'introduction de l'information spatiale dans les modèles multidimensionnels a donné naissance au concept de Spatial OLAP (SOLAP).Dans cet article, nous montrons en quoi les spécificités de l'information géographique et de l'analyse spatiale ne sont pas entièrement prises en compte dans l'analyse et les modèles multidimensionnels SOLAP. Pour pallier ces limites, nous proposons le concept de dimension géographique et décrivons les différents types de hiérarchies associées. Nous proposons l'introduction de nouveaux opérateurs qui permettent d'adapter les opérateurs d'analyse spatialeau paradigme multidimensionnel. Enfin, nous présentons notre prototype qui offre une interface web de navigation spatiale et multidimensionnelle, et permet l'intégration de ces nouveaux concepts.
**** *annee_2008  *numText_639
Nous avons proposé un algorithme original de Fouille de Données, LICORN, afin d'inférer des relations de régulation coopérative à partir de données d'expression. LICORN donne de bons résultats s'il est appliqué à des données de levure, mais le passage à l'échelle sur des données plus complexes (e.g., humaines) est difficile. Dans cet article, nous proposons une extension de LICORN afin qu'il puisse gérer une contrainte de co-régulation adaptative. Une évaluation préliminaire sur des données de transcriptome de tumeurs de vessie montre que les réseaux significatifs sont obtenus à l'aide d'une contrainte de corégulation adaptative de manière beaucoup plus efficace, et qu'ils ont des performances de prédiction équivalentes voire meilleures que celles obtenues par LICORN.
**** *annee_2008  *numText_640
Un large panel de domaines d'application utilise des réseaux de capteurs géoréférencés pour mesurer divers événements. Les séries temporelles fournies par ces réseaux peuvent être utilisées dans le but de dégager des connaissances sur les relations spatio-temporelles de l'activité mesurée. Dans cet article, nous proposons une méthode permettant d'abord de détecter des situations atypiques (au sens de l'occurrence) puis de construire des motifs spatio-temporels relatant leur propagation sur un réseau. Le cas étudié est celui du trafic routier urbain. Notre raisonnement se fonde sur l'application de la méthode Space-Time Principal Component Analysis (STPCA) et de la combinaison entre l'information mutuelle et l'algorithme Isomap. Les résultats expérimentaux exécutés sur des données réelles de trafic routier démontrent l'efficacité de la méthode introduite à identifier la propagation de cas atypiques fournissant ainsi un outil performant de prédiction de la circulation intraday à court et moyen terme.
**** *annee_2008  *numText_641
Dans le cadre de la recherche interactive d'images dans une base de données, nous nous intéressons à des mesures de similarité d'image qui permettent d'améliorer l'apprentissage et utilisables en temps réel lors de la recherche. Les images sont représentées sous la forme de graphes d'adjacence de régions floues. Pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes, extraites des graphes comparés. Nous proposons un cadre général permettant l'emploi de différents noyaux et différents types de chaînes(sans cycle, avec boucles) autorisant des appariements inexacts. Nous avons effectué des comparaisons sur deux bases issues de Columbia et Caltech et montré que des chaînes de très faible dimension (longueur inférieur à 3) sont les plus efficaces pour retrouver des classes d'objets.
**** *annee_2008  *numText_642
Dans cet article nous présentons nos travaux sur la recherche d'information personnalisée dans les bibliothèques numériques. Nous utilisons des profils utilisateurs qui représentent des intérêts et des préférences des utilisateurs. Les résultats de recherche peuvent être retriés en tenant compte des besoins d'informations spécifiques de différentes personnes, ce qui donne une meilleure précision. Nous étudions différentes méthodes basées sur les citations, sur le contenu textuel des documents et des approches hybrides. Les résultats des expérimentations montrent que nos approches sont efficaces et applicables dans le cadre des bibliothèques numériques.
**** *annee_2008  *numText_643
Afin d'aider les biologistes à annoter des génomes, ce qui nécessite l'analyse, le croisement, et la comparaison de données provenant de sources diverses, nous avons conçu un entrepôt de données de génomique microbienne. Nous présentons la structure globale flexible de l'entrepôt et son architecture multi-niveaux et définissons des correspondances entre ces niveaux. Nous introduisons ensuite la notion de requête alternative et montrons comment le système peut construire l'ensemble des requêtes alternatives à une requête initiale. Pour cela, nous introduisons un mécanisme d'interrogation qui repose sur l'architecture multi-niveaux, et donnons un algorithme de calcul des requêtes alternatives.
**** *annee_2008  *numText_644
Dans ce papier, nous présentons une nouvelle mesure de similarité pour la classification des référents de la carte auto-organisatrice qui sera réalisée à l'aide d'une nouvelle approche de classification hiérarchique. (1) La mesure de similarité est composée de deux termes : la distance de Ward pondérée et la distance euclidienne pondérée par la fonction de voisinage sur la carte topologique. (2) Un algorithme à base de fourmis artificielles nommé AntTree sera utilisé pour segmenter la carte auto-organisatrice.Cet algorithme a l'avantage de prendre en compte le voisinage entre les référents et de fournir une hiérarchie des référents avec une complexité proche du nlog(n). La segmentation incluant la nouvelle mesure est validée sur plusieurs bases de données publiques.
**** *annee_2008  *numText_646
Dans ce papier, nous enrichissons la méthode Terminae de construction d'ontologie à partir de textes en proposant une semi-automatisation de la construction du modèle conceptuel. Nous présentons un algorithme permettant la conceptualisation d'un terme en s'appuyant sur les informations linguistiques contenues dans l'ontologie générique de référence.
**** *annee_2008  *numText_647
Dans cet article, nous proposons la méthode des SOM (cartes auto-organisatrices de Kohonen) pour la classification non supervisée de documents textuels basés sur les n-grammes. La même méthode basée sur les synsets de WordNet comme termes pour la représentation des documents est étudiée par la suite. Ces combinaisons sont évaluées et comparées.
**** *annee_2008  *numText_648
Un problème majeur se pose dans le domaine des flux de données : la distribution sous-jacente des données peut changer sur le temps. Dans cet article, nous proposons trois stratégies de classification non supervisée basée sur des fenêtres superposées. Notre objectif est de pouvoir repérer ces changements dans le temps. Notre approche est appliquée sur un benchmark de données réelles et les conclusions obtenues sont basées sur deux indices de comparaison de partitions.
**** *annee_2008  *numText_651
Dans cet article, nous proposons une approche multi-agent argumentative permettant d'automatiser la résolution des conflits entre décideurs dans un système d'aide à l'identification des connaissances cruciales nommé K-DSS. En effet, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant de résoudre les conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.
**** *annee_2008  *numText_652
Une des questions les plus importantes pour la plupart des applications réelles de la classification est de déterminer un nombre approprié de groupes (clusters). Déterminer le nombre optimal de groupes est un problème difficile, puisqu'il n'y a pas de moyen simple pour connaître ce nombre sans connaissance a priori. Dans cet article, nous proposons un nouvel algorithme de classification non supervisée à deux niveaux, appelé S2L-SOM (Simultaneous Twolevel Clustering - Self Organizing Map), qui permet de déterminer automatiquement le nombre optimal de groupes, pendant l'apprentissage d'une carte auto-organisatrice. L'estimation du nombre correct de groupes est en relation avec la stabilité de la segmentation et la validité des groupes générés. Pour mesurer cette stabilité nous utilisons une méthode de sous-échantillonnage. Le principal avantage de l'algorithme proposé, comparé aux méthodes classiques de classification, est qu'il n'est pas limité à la détection de groupes convexes, mais est capable de détecter des groupes de formes arbitraires. La validation expérimentale de cet algorithme sur un ensemble de problèmes fondamentaux pour la classification montre sa supériorité sur les méthodes standards de classification à deux niveaux comme SOM+K-Moyennes et SOM+Hierarchical- Agglomerative-Clustering.
**** *annee_2008  *numText_653
Le cartogramme présenté dans cet article est destiné à faciliter l'analyse visuelle de données spatiotemporelles complexes. Pour cela, il offre la possibilité de représenter simultanément les trois dimensions nécessaires à toute forme d'analyse géographique que sont les dimensions spatiale (où),thématique (quoi) et temporelle (quand), à partir de trois composantes principales: (1) une représentation unidimensionnelle (1D) de l'espace géographique de forme semi-circulaire centrée sur une origine (ex. le Canada) ; (2) des entités géographiques (ex. pays) qui viennent graviter autour de cette origine en fonction de valeurs attributaires ; et (3) une ligne de temps interactive permettant d'explorer la dimension temporelle de l'information représentée. La combinaison de ces trois composantes offre de multiples potentialités pour l'analyse spatio-temporelle de différentes formes de proximités qu'elles soient économiques, culturelles, sociales ou démographiques. Les fonctionnalités et potentialités de ce cartogramme développé en source ouverte sont illustrées à partir d'exemples issus de l'atlas cybercartographique du commerce Canadien. Cet article reprend les grandes lignes d'une communication présentée lors de la conférence SAGEO 2007.
**** *annee_2008  *numText_654
Les noyaux ont été largement utilisés pour le traitement de données textuelles comme mesure de similarité pour des algorithmes tels que les Séparateurs à Vaste Marge (SVM). Le modèle de l'espace vectoriel (VSM) a été amplement utilisé pour la représentation spatiale des documents. Cependant, le VSM est une représentation purement statistique. Dans ce papier, nous présentons un modèle d'espace vectoriel de concepts (CVSM) qui se base sur des connaissances linguistiques a priori pour capturer le sens des documents. Nous proposons aussi un noyau linéaire et un noyau latent pour cet espace. Le noyau linéaire exploite les concepts linguistiques pour l'extraction du sens alors que le noyau latent combine les concepts statistiques et linguistiques. En effet, le noyau latent utilise des concepts latents extraits par l'Analyse Sémantique Latente (LSA) dans le CVSM. Les noyaux sont évalués sur une tâche de catégorisation de texte dans le domaine biomédical. Le corpus Ohsumed, bien connu pour sa difficulté de catégorisation, a été utilisé. Les résultats ont montré que les performances de catégorisation sont améliorées dans le CSVM.
**** *annee_2008  *numText_655
Nous présentons ici une approche pour la gestion de bases d'ontologies basée sur un modèle comprenant, outre la définition formelle des concepts (sous forme d'axiomes de logique de description), d'autres éléments descriptifs (termes, commentaires et arguments), ainsi que leurs liens d'alignement avec des concepts d'autres ontologies. L'adaptation ou la combinaison d'ontologies se font grâce à une algèbre comprenant des opérations telles que la sélection, la projection, l'union ou la jointure d'ontologies. Ces opérations agissent au niveau des axiomes, des éléments descriptifs et des liens d'alignement.
**** *annee_2008  *numText_656
Nous proposons dans ce papier un nouveau système immunitaire artificiel (SIA) appelé système NK, pour la détection de comportement du soi non soi avec une approche non supervisée basée sur le mécanisme de cellule NK (Naturel Killer). Dans ce papier, le système NK est appliqué à la détection de fraude en téléphonie mobile.
**** *annee_2008  *numText_657
Les données constituent l'élément central d'un Système d'Information Géographiques (SIG) et leur coût est souvent élevé en raison de l'investissement substantiel qui permet leur production. Cependant, ces données sont souvent restreintes à un service ou pour une catégorie d'utilisateurs. Ce qui a fait ressortir la nécessité de proposer des moyens d'enrichissement en informations pertinentes pour un nombre plus important d'utilisateurs. Nous présentons dans ce papier notre approche d'enrichissement de données qui se déroule selon trois étapes : une identification de segments et de thèmes associés, une délégation et enfin, un filtrage textuel. Un processus de raffinement est également offert. Notre approche globale a été intégrée à un SIG. Son évaluation a été accomplie montrant ainsi sa performance.
**** *annee_2008  *numText_658
Les tâches de classification textuelle ont souvent pour objectif de regrouper thématiquement différents textes. Dans cet article, nous nous sommes intéressés à la classification de documents en fonction des opinions et jugements de valeurs qu'ils contiennent. L'approche proposée est fondée sur un système de vote utilisant plusieurs méthodes de classification.
**** *annee_2008  *numText_659
Dans cet article, nous nous intéressons à la découverte de mises en correspondance entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P SomeRDFS. Plus précisément, nous montrons comment exploiter les mécanismes de raisonnement mis en oeuvre dans SomeRDFS pour aider à découvrir des mappings entre ontologies. Ce travail est réalisé dans le cadre du projet MediaD en partenariat avec France Telecom R&D.
**** *annee_2008  *numText_660
En classification supervisée, de nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses de base afin de créer une règle de décision finale plus performante. Ainsi, il a été montré que des méthodes comme le bagging ou le boosting pouvaient se révéler intéressantes, tant dans la phase d'apprentissage qu'en généralisation. Dès lors, il est tentant de vouloir s'inspirer des grands principes d'une méthode comme le boosting en classification non supervisée. Or, il convient préalablement de se confronter aux difficultés connues de la thématique des ensembles de regroupeurs (correspondance des classes, agrégation des résultats, qualité) puis d'introduire l'idée du boosting dans un processus itératif. Cet article propose une méthode ensembliste inspirée du boosting, qui, à partir d'un partitionnement flou obtenu par les c-moyennes floues (fuzzy-c-means), va insister itérativement sur les exemples difficiles pour former une partition dure finale plus pertinente.
**** *annee_2008  *numText_661
Cet article présente la méthode et le système C3R pour vérifier de façon semi-automatique la conformité d'un projet de construction par rapport à des normes du bâtiment. Les projets de construction sont représentés par des graphes RDF et les normes par des requêtes SPARQL ; le processus de contrôle consiste en l'appariement des requêtes et des graphes. Son efficacité repose sur l'acquisition de connaissances ontologiques et sur un processus d'extraction de connaissances guidé par ce but spécifique de contrôle de conformité qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même : les requêtes représentant les normes sont annotées et organisées selon ces annotations. Ces annotations sont également utilisées dans les interactions avec l'utilisateur de C3R pour expliquer les résultats du processus de validation, en particulier en cas d'échec.
**** *annee_2008  *numText_663
La recherche d'information et la navigation dans les pages web s'avèrent complexes du fait du volume croissant des données et de leur manque de structure. La formalisation conceptuelle d'un contexte associé à une ontologie rend possible l'amélioration de ce processus. Nous définissons un contexte conceptuel comme étant l'association d'un treillis de concepts construit à partir de pages web avec des ontologies. La recherche et la navigation peuvent alors s'effectuer à plusieurs niveaux d'abstraction : le niveau des données, le niveau conceptuel et le niveau sémantique. Cet article s'intéresse essentiellement au niveau conceptuel grâce à une représentation par les treillis de concepts des documents selon les termes qu'ils ont en commun. Notre objectif est de proposer une mesure de similarité permettant à l'utilisateur de mieux naviguer dans le treillis. En effet, une bonne interprétation du treillis devrait passer par un choix rigoureux des concepts, objets, relations et propriétés les plus intéressants. Pour faciliter la navigation, il faut pouvoir indiquer à l'utilisateur les concepts les plus pertinents par rapport au concept correspondant à sa requête ou pouvoir lui proposer un point de départ. L'originalité de notre proposition réside dans le fait de considérer un lien sémantique entre les concepts du treillis, basé sur une extension des mesures de similarité utilisées dans le cadre des ontologies, afin de permettre une meilleure exploitation de ce treillis. Nous présentons les résultats expérimentaux de l'application de cette mesure sur des treillis construits à partir de pages web dans le domaine du tourisme.
**** *annee_2008  *numText_664
La réduction de l'erreur en généralisation est l'une des principales motivations de la recherche en apprentissage automatique. De ce fait, un grand nombre de travaux ont été menés sur les méthodes d'agrégation de classifieurs afin d'améliorer, par des techniques de vote, les performances d'un classifieur unique. Parmi ces méthodes d'agrégation, le boosting est sans doute le plus performant grâce à la mise à jour adaptative de la distribution des exemples visant à augmenter de façon exponentielle le poids des exemples mal classés. Cependant, en cas de données fortement bruitées, cette méthode est sensible au sur-apprentissage et sa vitesse de convergence est affectée. Dans cet article, nous proposons une nouvelle approche basée sur des modifications de la mise à jour des exemples et du calcul de l'erreur apparente effectuées au sein de l'algorithme classique d'AdaBoost. Une étude expérimentale montre l'intérêt de cette nouvelle approche, appelée Approche Hybride, face à AdaBoost et à BrownBoost, une version d'AdaBoost adaptée aux données bruitées.
**** *annee_2008  *numText_665
Dans cet article nous présentons une nouvelle méthode de classification non supervisée pour des données symboliques intervalles. Il s'agit de l'extension d'une méthode de classification non supervisée classique à des données intervalles. La méthode classique suppose que les points observés sont la réalisation d'un processus de Poisson homogène dans k domaines convexes disjoints de Rp. La première partie de la nouvelle méthode est une procédure monothétique divisive. La règle de coupure est basée sur une extension à des données intervalles du critère de classification des Hypervolumes. L'étape d'élagage utilise un test statistique basé sur le processus de Poisson homogène. Le résultat est un arbre de décision. La seconde partie de la méthode consiste en une étape de recollement, qui permet, dans certains cas, d'améliorer la classification obtenue à la fin de la première partie de l'algorithme. La méthode est évaluée sur un ensemble de données réelles.
**** *annee_2008  *numText_666
Les relations sémantiques généralement reconnues par les méthodes d'extraction sont portées par des structures de type prédicats-arguments. Or, l'information recherchée est souvent répartie sur plusieurs phrases. Pour détecter ces relations dites complexes, nous proposons un modèle de représentation des connaissances basé sur les graphes conceptuels.
**** *annee_2008  *numText_667
Cet article décrit une approche de création semi-automatique d'ontologies et d'annotations sémantiques à partir de messages électroniques échangés dans une liste de diffusion dédiée au support informatique. Les ressources sémantiques générées permettront d'identifier les questions fréquemment posées (FAQ) à travers une recherche guidée par cette ontologie.
**** *annee_2008  *numText_668
Une des principales critiques que l'on puisse faire aux Séparateurs à Vaste Marge (SVM) est le manque d'intelligibilité des résultats. En effet, il s'agit d'une technique boite noire qui ne fournit pas d'explications ni d'indices quant aux raisons d'une classification. Les résultats doivent être pris tels quels en faisant confiance au système qui les a produits. Pourtant selon notre expérience pratique, les experts du domaine préfèrent largement une méthode d'apprentissage avec explications et recommandation d'actions plutôt qu'une boite noire, aussi performante et prédictive soit-elle. Dans cette thématique, nous proposons une nouvelle approche qui consiste a rendre les SVM plus actionnables. Ce but est atteint en couplant des modèles de classement des résultats des SVM à des méthodes d'apprentissage de concepts. Nous présentons une application de notre méthode sur diverses données dont des données médicales concernant des patients de l'athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l'utilité de notre approche quant à l'intelligibilité et l'actionnabilité des résultats produits par SVM.
**** *annee_2008  *numText_669
Une tendance lourde depuis la fin du siècle dernier est l'augmentation exponentielle du volume des données stockées. Cette augmentation ne se traduit pas nécessairement par une information plus riche puisque la capacité à traiter ces données ne progresse pas aussi rapidement. Avec les technologies actuelles, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite. Nous proposons une approche industrielle permettant d'augmenter considérablement notre capacité à transformer des données en information grâce à l'automatisation des traitements et à la focalisation sur les seules données pertinentes.
**** *annee_2008  *numText_671
Les brevets sont une source d'information très riche puisque ce sont des documents qui servent à décrire les inventions. L'accès aux documents de brevets en ligne est possible grâce aux efforts des offices nationaux de la propriété intellectuelle. Par ailleurs, ayant des objectifs différents, la présentation de ces documents a pris des formes variées loin d'être unifiées. Ce papier présente une méthode et un système permettant l'analyse de brevets Patent Mining pour générer des annotations sémantiques. L'idée principale est de pouvoir prendre en considération la structure des brevets pour pouvoir trouver un lien entre le contenu du brevet et les concepts des différentes ontologies.
**** *annee_2008  *numText_672
Cet article présente une interface permettant de visualiser des motifs séquentiels extraits à partir de données textuelles en Ancien Français.
**** *annee_2008  *numText_673
Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d'individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l'enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces événements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d'états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d'optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l'aide de techniques de « Multi Dimensional Scaling ». Cet article s'intéresse en particulier aux problématiques entourant l'application de ces méthodes aux données de parcours de vie.
**** *annee_2008  *numText_674
Notre objectif dans cet article est l'analyse textuelle d'un site Web indépendamment de son usage. Notre approche se déroule en trois étapes. La première étape consiste au typage des pages afin de distinguer les pages de navigation ou pages « auxiliaires » des pages de contenu. La deuxième étape consiste au prétraitement du contenu des pages de contenu afin de représenter chaque page par un vecteur de descripteurs. La dernière étape consiste au block clustering ou la classification simultanée des lignes et des colonnes de la matrice croisant les pages aux descripteurs de pages afin de découvrir des biclasses de pages et de descripteurs. L'application de cette approche au site de tourisme de Metz prouve son efficacité et son applicabilité. L'ensemble de classes de pages groupés en thèmes facilitera l'analyse ultérieure de l'usage du site.
**** *annee_2007  *numText_675
Ce papier présente une approche automatique pour aligner des ressources sémantiques. L'alignement se traduit par la mise en correspondance des entités (termes, concepts, rôles) appartenant à des ressources d'un même domaine qui peuvent avoir des niveaux de formalisation différents. Les entités correspondantes sont de même nature et un coefficient caractérise leur degré de ressemblance.L'approche proposée est fondée sur des règles d'appariement entre les entités des deux ressources. Dans une première phase, ces règles d'appariement sont identifiées empiriquement. Des algorithmes combinant les différentes règles identifiées sont ensuite définis afin d'établir des correspondances entre les entités des ressources considérées.Ce papier présente un ensemble de règles d'appariement exploitant des éléments situés à différents niveaux conceptuels. Cet ensemble constitue un cadre pour l'alignement automatique des ressources sémantiques. Les résultats d'une première expérimentation qui a porté sur l'alignement de deux ressources du domaine de l'accidentologie sont également présentés.
**** *annee_2007  *numText_676
Dans cet article, nous proposons un cadre et un outil pour l'annotation et la navigation de données archéologiques. L'objectif principal est de structurer les annotations de façon à permettre une navigation incrémentale où l'utilisateur peut, à partir d'un ensemble d'objets initialement retournés par une requête, découvrir des liens approximatifs avec d'autres objets de la base. L'approche a été implémentée et est en cours de validation.
**** *annee_2007  *numText_677
Nous présentons dans cet article différentes étapes de l'annotation de tableaux de données à l'aide d'une ontologie. Tout d'abord, nous distinguons les colonnes de données numériques et symboliques. Les données symboliques sont ensuite annotées de manière floue à l'aide des termes de l'ontologie. Cette annotation nous permet de déduire le type des colonnes de données symboliques. Pour trouver le type des colonnes de données numériques, nous utilisons à la fois le titre de la colonne et les valeurs numériques et unités présentes dans la colonne. Chaque étape de notre annotation est validée expérimentalement.
**** *annee_2007  *numText_678
L'apprentissage de la structure des réseaux bayésien à partir de données est un problème NP-difficile. Une nouvelle heuristique de complexité polynomiale, intitulée Polynomial Max-Min Skeleton (PMMS), a été proposée en 2005 par Tsamardinos et al. et validée avec succès sur de nombreux bancs d'essai. PMMS présente, en outre, l'avantage d'être performant avec des jeux de données réduits. Néanmoins, comme tous les algorithmes sous contraintes, celui-ci échoue lorsque des dépendances fonctionnelles (déterministes) existent entre des groupes de variables. Il ne s'applique, par ailleurs, qu'aux données complètes. Aussi, dans cet article, nous apportons quelques modifications pour remédier à ces deux problèmes. Après validation sur le banc d'essai Asia, nous l'appliquons aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) de 1289 observations, 61 variables et 5% de données manquantes issues d'un questionnaire. L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC
**** *annee_2007  *numText_679
La recherche d'une entreprise sur le Web, relative à un savoir-faire particulier, n'est pas une tâche toujours facile à mener. Les outils mis à la disposition de l'internaute ne donnent pas entièrement satisfaction. D'un côté les moteurs de recherche éprouvent des difficultés à faire ressortir clairement le résultat escompté. De l'autre côté, les annuaires spécialisés (type Pages Jaunes) sont tributaires d'une organisation figée, nuisant à leur efficacité. Face à ce constat, nous nous proposons de créer un nouveau moteur spécialisé dans la recherche d'entreprise, associant Web sémantique et géo-localisation. Cette approche novatrice nécessite l'implémentation d'une ontologie ayant pour objectif la formalisation des connaissances du domaine. Cette tâche a mis en évidence l'intérêt des structures économiques, maintenues par l'INSEE, et leur utilisation au sein de l'ontologie. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises. La structure des unités administratives, telle que gérée au sein du fichier SIRENE, s'est avérée judicieuse pour répondre à la problématique de géo-localisation des entreprises. Une opération de désambiguïsation est réalisée en associant à chaque noeud d'activité les mots clés et synonymes lui correspondant. Enfin, nous comparons les résultats obtenus par notre moteur à ceux obtenu par le principal moteur de recherche d'activités géo-localisées en France : les Pages jaunes. Que ce soit au niveau de la précision et du rappel, notre moteur obtient des résultats significativement meilleurs.
**** *annee_2007  *numText_680
La prise en compte des émotions dans les interactions Homme-machine permet de concevoir des systèmes intelligents, capables de s'adapter aux utilisateurs. Les techniques de redirection d'appels dans les centres téléphoniques automatisés se basent sur la détection des émotions dans la parole. Les principales difficultés pour mettre en œuvre de tels systèmes sont l'acquisition et l'étiquetage des données d'apprentissage. Cet article propose l'application de deux stratégies d'apprentissage actif à la détection d'émotions dans des dialogues en interaction homme-machine. L'étude porte sur des données réelles issues de l'utilisation d'un serveur vocal et propose des outils adaptés à la conception de systèmes automatisés de redirection d'appels.
**** *annee_2007  *numText_681
Nous présentons dans cet article un algorithme inductif semi-supervisé pour la tâche d'ordonnancement bipartite. Les algorithmes semi-supervisés proposés jusqu'à maintenant ont été étudiés dans le cadre strict de la classification. Récemment des travaux ont été réalisés dans le cadre transductif pour étendre les modèles existants en classification au cadre d'ordonnancement. L'originalité de notre approche est qu'elle est capable d'inférer un ordre sur une base test non- utilisée pendant la phase d'apprentissage, ce qui la rend plus générique qu'une méthode transductive pure. Les résultats empiriques sur la base CACM contenant les titres et les résumés du journal Communications of the Association for Computer Machinery montrent que les données non-étiquetées sont bénéfiques pour l'apprentissage de fonctions d'ordonnancement.
**** *annee_2007  *numText_682
Découvrir la topologie d'un ensemble de données étiquetées dans un espace Euclidien peut aider à construire un meilleur système de décision. Dans ce papier, nous proposons un modèle génératif basé sur le graphe de Delaunay de plusieurs prototypes représentant les données étiquetées dans le but d'extraire de ce graphe la topologie des classes.
**** *annee_2007  *numText_683
Dans cet article, nous présentons un système de découverte de connaissances à partir de données issues d'une étude épidémiologique cas-témoins du cancer du Nasopharynx (NPC). Ces données étant obtenues par une collecte de questionnaires, elles ont d'une part, la particularité d'être qualitatives et, d'autre part, de présenter des valeurs manquantes. Prenant en compte ces deux dernières contraintes, le système que nous proposons suit une démarche d'exploration de données qui consiste à (1) définir une procédure de codage des données qualitatives en présence de valeurs manquantes ; (2) étudier les propriétés de l'algorithme des cartes auto-organisatrices de Kohonen et son adaptation à ce type de données dans un cadre de découverte et de visualisation de groupes homogènes des cas cancer / non-cancer ; (3) post-traiter le résultat de cet algorithme par une classification automatique pour optimiser le nombre de groupes ainsi trouvés, et (4) donner une interprétation sémantique des profils extraits de chaque groupe. L'objectif général de cette étude est d'éclater le profil statistique global de la population étudiée en un ensemble de profils types (cancer ou non-cancer) et d'extraire pour chaque profil l'ensemble de variables explicatives du NPC à partir d'une cartographie bidimensionnelle.
**** *annee_2007  *numText_684
Le problème de réconciliation de références consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel. Dans cet article, nous étudions ce problème quand le schéma des données est décrit en RDFS étendu par certaines primitives de OWL-DL. Nous décrivons et montrons l'intérêt d'une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Les premiers résultats ont été obtenus sur des données réelles dans le cadre du projet PICSEL 3 en collaboration avec France Telecom R&D.
**** *annee_2007  *numText_685
Les cubes de données fournissent une aide non négligeable lorsqu'il s'agit d'interroger des entrepôts de données. Un cube de données représente un pré-calcul de toutes les requêtes OLAP et ainsi améliore leur temps de réponses. Les approches proposées jusqu'à présent réduisent les temps de calcul et d'entrée sortie mais leur utilisation reste très coûteuse. D'autres travaux de recherche se sont intéressés à la visualisation de données pour les exploiter de façon interactive.Nous proposons une adaptation de la représentation condensée des cubes de données basée sur le modèle partitionnel. Cette technique nous permet de calculer efficacement un cube de données et de représenter les liens entre les données pour la visualisation. La visualisation proposée dans cet article est basée sur des techniques de visualisation orientée pixel et sur des techniques de diagramme de liens entre nœuds pour offrir à la fois une vision globale et locale pour l'exploitation. Cette nouvelle approche utilise d'une part les calculs efficaces de cubes de données et d'autre part les techniques avancées de visualisation.
**** *annee_2007  *numText_686
La connaissance du protocole de conversation d'un service Web est importante pour les utilisateurs et les fournisseurs, car il en modélise le comportement externe ; mais, il n'est souvent pas spécifié lors de la conception. Notre travail s'inscrit dans une thématique d'extraction du protocole de conversation d'un service existant à partir de ses données d'exécution. Nous en étudions un sous-problème important qui est la découverte des transitions temporisées (i.e. les changements d'état liés à des contraintes temporelles). Nous proposons un cadre formel aboutissant à la définition des expirations propres, qui représentent un équivalent dans les logs des transitions temporisées. A notre connaissance, ceci représente la première contribution à la résolution de ce problème.
**** *annee_2007  *numText_687
La gestion des connaissances est devenue aujourd'hui un enjeu majeur pour toute organisation. Celle-ci a pour but de capitaliser et de rendre accessible à ses acteurs la connaissance détenue par l'organisation. Cet article s'intéresse particulièrement à la visualisation à deux niveaux de ces connaissances (macroscopique - relatif aux connaissances globales détenues par l'organisation - et microscopique - relatif aux connaissances locales détenues par chaque membre organisationnel). La caractérisation des connaissances détenues par les acteurs repose sur quatre dimensions complémentaires (formelle, conative, cognitive, et socio-cognitive). Les deux types de visualisation proposés s'appuient sur les cartes auto-organisatrices et permettent une navigation dans différentes représentations des connaissances de l'organisation.
**** *annee_2007  *numText_688
Cet article traite de la validation de règles dans un contexte de ciblage où il s'agit de déterminer les profils type des différentes valeurs de la variable à prédire. Les concepts de l'analyse statistique implicative fondée sur la différence entre nombre observé de contre-exemples et nombre moyen que produirait le hasard, s'avèrent particulièrement bien adaptés à ce contexte. Le papier montre comment les notions d'indice et d'intensité d'implication de Gras s'appliquent aux règles produites par les arbres de décision et présente des alternatives inspirées de résidus utilisés en modélisation de tables de contingence. Nous discutons ensuite sur un jeu de données réelles deux usages de ces indicateurs de force d'implication pour les règles issues d'arbres. Il s'agit d'une part de l'évaluation individuelle des règles, et d'autre part de leur utilisation comme critère pour le choix de la conclusion de la règle.
**** *annee_2007  *numText_689
Vu l'accroissement constant du volume d'information accessible en ligne sous format XML, il devient primordial de proposer des modèles adaptés à la recherche d'information dans les documents XML. Tandis que la recherche d'information classique repose sur l'indexation du contenu des documents, la recherche d'information dans les documents XML tente d'améliorer la qualité des résultats en tirant profit de la sémantique véhiculée par la structure des documents. Dans cet article, nous présentons une méthode de classement des items (éléments XML) retournés lors d'une recherche dans une collection de documents XML. Le classement repose sur la prise en compte d'un ensemble de critères discriminants. La particularité de notre approche réside dans la façon dont nous les utilisons : Nous employons une méthode décisionnelle pour classer les items en les comparant deux-à-deux là où en général une fonction de scoring globale est utilisée.
**** *annee_2007  *numText_690
Il n'est pas rare que des données individu soient caractérisées par une distribution continue et non une seule valeur. Ces données fonctionnelles peuvent être utilisées pour classer les individus. Une solution élémentaire est de réduire les distributions à leurs moyennes et variances. Une solution plus riche a été proposée par Diday (2002) et mise en œuvre par Vrac et al. (2001) et Cuvelier et Noirhomme-Fraiture (2005). Elle utilise des points de coupures dans les distributions et modélise ces valeurs conjointes par une distribution multidimensionnelle construite à l'aide d'une copule. Nous avons montré dans un précédent travail que, si cette technique apporte de bons résultats, la qualité de la classification dépend néanmoins du nombre et de l'emplacement des coupures. Les questions du choix du nombre et de l'emplacement des coupures restaient des questions ouvertes. Nous proposons une solution à ces questions, lorsque le nombre de coupures tend vers l'infini, en proposant une nouvelle distribution de probabilité adaptée à l'espace de dimension infinie que forment les données fonctionnelles. Nous proposons aussi une densité de probabilité adaptée à la nature de cette distribution en utilisant la dérivée directionnelle de Gâteaux. La direction choisie pour cette dérivée est celle de la dispersion des fonctions à classer. Les résultats sont encourageants et offrent des perspectives multiples dans tous les domaines où une distribution de données fonctionnelles est nécessaire.
**** *annee_2007  *numText_691
Le nouvel algorithme de boosting de Least-Squares Support Vector Machine (LS-SVM) que nous présentons vise à la classification de très grands ensembles de données sur des machines standard. Les méthodes de SVM et de noyaux permettent d'obtenir de bons résultats en ce qui concerne la précision mais la tâche d'apprentissage pour de grands ensembles de données demande une grande capacité mémoire et un temps relativement long. Nous présentons une extension de l'algorithme de LS-SVM proposé par Suykens et Vandewalle pour le boosting de LS-SVM. A cette fin, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour traiter des ensembles de données ayant un grand nombre de dimensions. Nous l'avons ensuite étendu par application du boosting de LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances de l'algorithme sont évaluées sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et NDC sur une machine standard (PC-P4, 3GHz, 512 Mo RAM).
**** *annee_2007  *numText_692
La classification des séquences biologiques est l'un des importants défis ouverts dans la bioinformatique, tant pour les séquences protéiques que pour les séquences nucléiques. Cependant, la présence de ces données sous la forme de chaînes de caractères ne permet pas de les traiter par les outils standards de classification supervisée, qui utilisent souvent le format relationnel. Pour remédier à ce problème de codage, plusieurs travaux se sont basés sur l'extraction des motifs pour construire une nouvelle représentation des séquences biologiques sous la forme d'un tableau binaire. Nous décrivons une nouvelle approche qui étend les méthodes précédents par l'utilisation de matrices de substitution dans les cas des séquences protéiques. Nous présentons ensuite une étude comparative qui prend en compte l'effet de chaque méthode sur la précision de la classification mais aussi le nombre d'attributs générés et le temps de calcul.
**** *annee_2007  *numText_693
Les méthodes du 'clustering' ont pour but de diviser un ensemble (large) d'objets dans un petit nombre de groupes homogènes (clusters), basé sur des données relevées ou observées qui décrivent les (dis-)similarités qui existent entre les objets - en espérant que ces clusters soient utiles pour l'application concernée. Il existe une multitude d'approches, et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles.
**** *annee_2007  *numText_694
Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d'améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité réalisée à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.
**** *annee_2007  *numText_695
Nous présentons dans cette contribution un cadre de modélisation recourant conjointement au modèle Hypertopic (Cahier et al., 2004) pour la représentation des connaissances de domaine et au modèle SeeMe (Herrmann et al., 1999) pour la représentation de l'activité. Ces deux approches apparaissent complémentaires, et nous montrons comment elles peuvent être combinées, pour mieux ancrer, sur les plans formel et méthodologique, les approches de cartographie collective des connaissances.
**** *annee_2007  *numText_696
Cet article présente une méthode semi-automatique de construction d'ontologie à partir de corpus de textes sur un domaine spécifique. Cette méthode repose en premier lieu sur un analyseur syntaxique partiel et robuste des textes, et en second lieu, sur l'utilisation de l'analyse formelle de concepts FCA pour la construction de classes d'objets en un treillis de Galois. La construction de l'ontologie, c'est à dire d'une hiérarchie de concepts et d'instances, est réalisée par une transformation formelle de la structure du treillis. Cette méthode s'applique dans le domaine de l'astronomie.
**** *annee_2007  *numText_697
La manière dont une visite est réalisée sur un site Web peut changer en raison de modifications liées à la structure et au contenu du site lui-même, ou bien en raison du changement de comportement de certains groupes d'utilisateurs ou de l'émergence de nouveaux comportements. Ainsi, les modèles associés à ces comportements dans la fouille d'usage du Web doivent être mis à jour continuellement afin de mieux refléter le comportement actuel des internautes. Une solution, proposée dans cet article, est de mettre à jour ces modèles à l'aide des résumés obtenus par une approche évolutive des méthodes de classification.
**** *annee_2007  *numText_698
Cet article décrit un nouvel algorithme incrémental nommé AntGraph pour la construction de graphes de voisinage. Il s'inspire du comportement d'autoassemblage observé chez des fourmis réelles où ces dernières se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de créer une structure vivante. Nous utilisons ainsi une approche à base de fourmis artificielles où chaque fourmi représente une donnée. Nous indiquons comment ce comportement peut être utilisé pour construire de manière incrémentale un graphe à partir d'une mesure de similarité entre les données. Nous montrons finalement que notre algorithme obtient de meilleurs résultats en comparaison avec le graphe de Voisins Relatifs, notamment en terme de temps de calcul.
**** *annee_2007  *numText_699
Ce papier adresse le problème de la découverte de connaissances temporelles à partir des données datées, générées par le système de supervision d'un processus de fabrication. Par rapport aux approches existantes qui s'appliquent directement aux données, notre méthode d'extraction des connaissances se base sur un modèle global construit à partir des données. L'approche de modélisation adoptée, dite stochastique, considère les données datées comme une séquence d'occurrences de classes d'événements discrets. Cette séquence est représentée sous les formes duales d'une chaîne de Markov homogène et d'une superposition de processus de Poisson. L'algorithme proposé, appelé BJT4R, permet d'identifier les motifs séquentiels, les plus probables entre deux classes d'événements discrets et les représentent sous la forme de modèles de chroniques. Ce papier présente les premiers résultats de l'application de cet algorithme sur des données générées par un processus de fabrication de semi-conducteur d'un site de production du groupe STMicroelectronics.
**** *annee_2007  *numText_700
Les entrepôts de données stockent des quantités de données de plus en plus massives et arrivent vite à saturation. Un langage de spécifications de fonctions d'oubli est défini pour résoudre ce problème. Dans le but d'offrir la possibilité d'effectuer des analyses sur l'historique des données, les spécifications définissent des résumés par agrégation et par échantillonnage à conserver parmi les données à oublier. Cette communication présente le langage de spécifications ainsi que les principes et les algorithmes pour assurer de façon mécanique la gestion des fonctions d'oubli.
**** *annee_2007  *numText_701
La détermination du niveau de consommation chez les clients est essentielle pour tout objectif de segmentation stratégique et de churn. Nous présentons sur un cas réel l'utilisation de la théorie des ensembles flous pour la définition d'une fonction d'appartenance permettant d'évaluer, de manière précise, le niveau de consommation, des abonnés en téléphonie mobile.
**** *annee_2007  *numText_702
Le stockage massif des données noie l'information pertinente et engendre des problèmes théoriques liés à la volumétrie des données disponibles. Ces problèmes dégradent la capacité prédictive des algorithmes d'extraction des connaissances à partir des données. Dans cet article, nous proposons une méthodologie adaptée à la représentation et à la prédiction des données volumineuses. A cette fin, suite à un partitionnement des attributs, des groupes d'attributs non-corrélés sont créés qui permettent de contourner les problèmes liés aux espaces de grandes dimensions. Un Ensemble est alors mis en place, apprenant chaque groupe par une carte auto-organisatrice. Outre la prédiction, ces cartes ont pour objectif une représentation pertinente des données. Enfin, la prédiction est réalisée par un vote des différentes cartes. Une expérimentation est menée qui confirme le bien-fondé de cette approche.
**** *annee_2007  *numText_703
L'objectif de ce travail est d'évaluer la perte d'information au sens de l'inertie entre des méthodes de partitionnement ou de classification hiérarchiques et une approche de classification conceptuelle. Nous voulons répondre à la question suivante : l'aspect simpliste du processus monothétique d'une méthode conceptuelle implique-t-il des partitions de moins bonne qualité au sens du critère de l'inertie ? Nous proposons de réaliser cette expérience sur 6 bases de l'UCI, trois de ces bases sont des tableaux de données quantitatives, les trois autres sont des tableaux de données qualitatives.
**** *annee_2007  *numText_704
De nos jours, le statisticien n'a plus nécessairement le contrôle sur la récolte des données. Le besoin d'une analyse statistique vient dans un second temps, une fois les données récoltées. Par conséquent, un travail est à fournir lors de la phase de préparation des données afin de passer d'une représentation informatique à une représentation statistique adaptée au problème considéré. Dans cet article, nous étudions un procédé de sélection d'une bonne représentation en nous basant sur des travaux antérieurs. Nous proposons un protocole d'évaluation de la pertinence d'une représentation par l'intermédiaire d'une métrique, dans le cas de la classification supervisée. Ce protocole exploite une méthode de classification non paramétrique régularisée, garantissant l'automaticité et la fiabilité de l'évaluation. Nous illustrons le fonctionnement et les apports de ce protocole par un problème réel de préparation de données de consommation téléphonique. Nous montrons également la fiabilité et l'interprétabilité des décisions qui en résultent.
**** *annee_2007  *numText_705
Les ontologies et les annotations sémantiques sont deux composants importants dans un système de gestion des connaissances basé sur le Web sémantique. Dans l’environnement dynamique et distribué du Web sémantique, les ontologies et les annotations pourraient être changées pour s'adapter à l'évolution de l'organisation concernée. Ces changements peuvent donc entraîner des inconsistances à détecter et traiter. Dans cet article, nous nous focalisons principalement sur l'évolution des annotations sémantiques en soulignant le contexte où les modifications de l'ontologie entraînent des inconsistances sur ces annotations. Nous présentons une approche basée sur des règles permettant de détecter les inconsistances dans les annotations sémantiques devenues obsolètes par rapport à l'ontologie modifiée. Nous décrivons aussi les stratégies d'évolution nécessaires pour guider le processus de résolution de ces inconsistances grâce à des règles correctives.
**** *annee_2007  *numText_706
Le modèle flou de proximité repose sur l'hypothèse que plus les occurrences des termes d'une requête se trouvent proches dans un document, plus ce dernier est pertinent. Cette mesure floue est très avantageuse dans le traitement des documents à textes courts, toutefois elle ne tient pas compte de la sémantique des termes. Nous présentons dans cet article l'intégration d'une métrique conceptuelle au modèle de proximité floue des termes pour la formalisation de notre propre modèle.
**** *annee_2007  *numText_707
Nous nous intéressons à l'extraction d'entités nommées avec comme but d'exploiter un ensemble de rapports pour en extraire une liste de partenaires. À partir d'une liste initiale, nous utilisons un premier ensemble de documents pour identifier des schémas de phrase qui sont ensuite validés par apprentissage supervisé sur des documents annotés pour en mesurer l'efficacité avant d'être utilisés sur l'ensemble des documents à explorer. Cette approche est inspirée de celle utilisée pour l'extraction de données dans les documents semi-structurés (wrappers) et ne nécessite pas de ressources linguistiques particulières ni de larges collections de tests. Notre collection de documents évoluant annuellement, nous espérons de plus une amélioration de notre extraction dans le temps.
**** *annee_2007  *numText_708
En raisonnement à partir de cas, l'adaptation d'un cas source pour résoudre un problème cible est une étape à la fois cruciale et difficile à réaliser. Une des raisons de cette difficulté tient au fait que les connaissances d'adaptation sont généralement dépendantes du domaine d'application. C'est ce qui motive la recherche sur l'acquisition de connaissances d'adaptation (ACA). Cet article propose une approche originale de l'ACA fondée sur des techniques d'extraction de connaissances dans des bases de données (ECBD). Nous présentons CABAMAKA, une application qui réalise l'ACA par analyse de la base de cas, en utilisant comme technique d'apprentissage l'extraction de motifs fermés fréquents. L'ensemble du processus d'extraction des connaissances est détaillé, puis nous examinons comment organiser les résultats obtenus de façon à faciliter la validation des connaissances extraites par l'analyste.
**** *annee_2007  *numText_710
Les motifs séquentiels sont un domaine de la fouille de données très étudié depuis leur introduction par Agrawal et Srikant. Même s'il existe de nombreux travaux (algorithmes, domaines d'application), peu d'entre eux se situent dans un contexte multidimensionnel avec la prise en compte de ses spécificités : plusieurs dimensions, relations hiérarchiques entre les éléments de chaque dimension, etc. Dans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de séquences multidimensionnelles convergentes ou divergentes ainsi que l'algorithme associé, M2S_CD, basé sur le paradigme pattern growth. Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits.
**** *annee_2007  *numText_711
Cet article porte sur l'extraction de motifs sous contraintes globales. Contrairement aux contraintes usuelles comme celle de fréquence minimale, leur vérification est problématique car elle entraine de multiples comparaisons entre les motifs. Typiquement, la localisation des k motifs maximisant une mesure d'intérêt, i.e. satisfaisant la contrainte top-k, est difficile. Pourtant, cette contrainte globale se révèle très utile pour trouver les motifs les plus significatifs au regard d'un critère choisi par l'utilisateur. Dans cet article, nous proposons une méthode générale d'extraction de motifs sous contraintes globales, appelée Approximer-et-Pousser. Cette méthode peut être vue comme une méthode de relaxation d'une contrainte globale en une contrainte locale évolutive. Nous appliquons alors cette approche à l'extraction des top-k motifs selon une mesure d'intérêt. Les expérimentations montrent l'efficacité de l'approche Approximer-et-Pousser.
**** *annee_2007  *numText_712
Dans cet article, nous proposons une solution pour la classification et le filtrage des sites Web à caractère violent. A la différence de la majorité de systèmes commerciaux basés essentiellement sur la détection de mots indicatifs ou l'utilisation d'une liste noire manuellement collectée, notre solution baptisée, WebAngels Filter, s'appuie sur un apprentissage automatique par des techniques de data mining et une analyse conjointe du contenu textuel et structurel de la page Web. Les résultats expérimentaux obtenus lors de l'évaluation de notre approche sur une base de test sont assez bons. Comparé avec des logiciels, parmi les plus populaires, WebAngels Filter montre sa performance en terme de classification.
**** *annee_2007  *numText_713
La découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces. Nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte d'associations utiles entre elles. Plus spécifiquement, nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace.
**** *annee_2007  *numText_714
Dans le contexte de la recherche d'information sur Internet, nous proposons une architecture d'annotation automatique des images médicales, extraites à partir des documents de santé en ligne. Notre système est conçu pour extraire des informations médicales spécifiques (i.e. modalité médicale, région anatomique) à partir du contenu et du contexte des images. Nous proposons une architecture de fusion des approches contenu/contexte adaptée aux images médicales. L'approche orientée sur le contenu des images, consiste à annoter des images inconnues par la catégorisation des représentations visuelles compactes. Nous utilisons en même temps le contexte des images (les régions textuelles) ainsi que des ontologies médicales spécialement adaptées aux informations recherchées. Finalement, nous démontrons qu'en fusionnant les décisions des deux approches, nous améliorons les performances globales du système d'annotation.
**** *annee_2007  *numText_715
Nous utilisons un algorithme d'amorce mutuelle (Riloff et Jones 99), entre des couples de termes d'une relation et des patrons de phrase. À partir de couples d'amorce, le système génère des listes de patrons qui sont ensuite enrichies de façon semi-supervisée, puis utilisées pour trouver de nouveaux couples. Ces couples sont à leur tour réutilisés pour générer, par itérations successives, de nouveaux patrons. L'originalité de l'étude réside dans l'interprétation du rappel, estimé comme la couverture d'un patron sur l'ensemble des exemples auxquels il s'applique
**** *annee_2007  *numText_716
Dans cet article, nous proposons une approche d'évolution de schéma dans les entrepôts de données qui permet aux utilisateurs d'intégrer leurs propres connaissances du domaine afin d'enrichir les possibilités d'analyse de l'entrepôt. Nous représentons cette connaissance sous la forme de règles de type si-alors. Ces règles sont utilisées pour créer de nouveaux axes d'analyse en générant de nouveaux niveaux de granularité dans les hiérarchies de dimension. Notre approche est fondée sur un modèle formel d'entrepôts de données évolutif qui permet de gérer la mise à jour des hiérarchies de dimension.
**** *annee_2007  *numText_718
Cet article est le résultat d'une recherche sur le processus, peu explicité dans la littérature, de création de connaissances dans les communautés de pratique. Nous commençons par établir une définition de travail pour ce concept de communauté de pratique qui permet l'échange et le partage de connaissances au sein de groupes de plus en plus virtuels. Nous analysons ensuite les communautés de pratique sous l'angle de la théorie de l'émergence. Nous proposons, alors, la modélisation d'un outil de support pour ces communautés qui améliore les échanges entre les membres et favorise l'émergence de nouvelles connaissances. Cet outil manipule les connaissances implicites ainsi qu'explicites et propose des possibilités pour la publication et la recherche d'informations. De plus, il s'adapte à chaque membre de la communauté par un processus de personnalisation.
**** *annee_2007  *numText_719
L'enrichissement des bases de données est un moyen visant à offrir un supplément informationnel aux utilisateurs. Dans le cas des données géographiques, cette activité représente de nos jours un problème crucial. Sa résolution permettrait de meilleures prises de décisions ne reposant pas uniquement sur les informations limitées. Notre outil SDET (Semantic Data Enrichment Tool) vient proposer une solution d'enrichissement faisant du Système d'Information Géographiques (SIG) initial une source riche d'informations.
**** *annee_2007  *numText_720
Devant l'accroissement constant des grandes bases de données, plusieurs travaux de recherche en fouille de données s'orientent vers le développement de techniques de représentation compacte. Ces recherches se développent suivant deux axes complémentaires : l'extraction de bases génériques de règles d'association et l'extraction de représentations concises d'itemsets fréquents.Dans ce papier, nous introduisons une nouvelle représentation concise exacte des itemsets fréquents. Elle se situe au croisement de chemins de deux autres représentations concises, à savoir les itemsets fermés et ceux dits essentiels. L'idée intuitive est de profiter du fait que tout opérateur de fermeture induit une fonction surjective. Dans ce contexte, nous introduisons un nouvel opérateur de fermeture permettant de calculer les fermetures des itemsets essentiels. Ceci a pour but d'avoir une représentation concise de taille réduite tout en permettant l'extraction des supports négatif et disjonctif d'un itemset en plus de son support conjonctif. Un nouvel algorithme appelé D-CLOSURE permettant d'extraire les itemsets essentiels fermés est aussi présenté. L'étude expérimentale que nous avons menée a permis de confirmer que la nouvelle approche présente un bon taux de compacité comparativement aux autres représentations concises exactes.
**** *annee_2007  *numText_721
Les méthodes de classification automatique sont employées dans des domaines variés et de nombreux algorithmes ont été proposés dans la littérature. Au milieu de cette jungle, il semble parfois difficile à un simple utilisateur de choisir quel algorithme est le plus adapté à ses besoins. Depuis le milieu des années 90, une nouvelle thématique de recherches, appelée clustering validity, tente de répondre à ce genre d'interrogation en proposant des indices pour juger de la qualité des catégorisations obtenues. Mais le choix est parfois difficile entre ces indices et il peut s'avérer délicat de prendre la bonne décision. C'est pourquoi nous proposons un logiciel adapté à cette problématique d'évaluation.
**** *annee_2007  *numText_722
Les mesures d'entropie, dont la plus connue est celle de Shannon, ont été proposées dans un contexte de codage et de transmission d'information. Néanmoins, dès le milieu des années soixante, elles ont été utilisées dans d'autres domaines comme l'apprentissage et plus particulièrement pour construire des graphes d'induction et des arbres de décision. L'usage brut de ces mesures n'est cependant pas toujours bien approprié pour engendrer des modèles de prédiction ou d'explication pertinents. Cette faiblesse résulte des propriétés des entropies, en particulier le maximum nécessairement atteint pour la distribution uniforme et l'insensibilité à la taille de l'échantillon. Nous commençons par rappeler ces propriétés classiques. Nous définissons ensuite une nouvelle axiomatique mieux adaptée à nos besoins et proposons une mesure empirique d'entropie plus flexible vérifiant ces axiomes.
**** *annee_2007  *numText_723
Les critères servant à l'évaluation de modèles d'apprentissage supervisé ainsi que ceux utilisés pour bâtir des arbres de décision sont, pour la plupart, symétriques. De manière pragmatique, cela signifie que chacune des modalités de la variable endogène se voit assigner une importance identique. Or, dans nombre de cas pratiques cela n'est pas le cas. Ainsi, on peut notamment prendre l'exemple de jeux de données fortement déséquilibrés pour lesquels l'objectif principal est l'identification des objets représentatifs de la modalité minoritaire (Aide au diagnostic, identification de phénomènes inhabituels : fraudes, pannes...). Dans ce type de situation il apparaît clairement qu'assigner une importance identique aux erreurs de prédiction ne constitue pas la meilleure des solutions. Nous proposons dans cet article un critère (pouvant servir à la fois pour l'évaluation de modèles d'apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision) prenant en compte cet aspect non symétrique de l'importance associée à chacune des modalités de la variable endogène. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.
**** *annee_2007  *numText_724
Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces méthodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l'aide de mesures ou de simulation. Dans ce contexte d'identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.
**** *annee_2007  *numText_725
L'ACI FoDoMuSt se propose d'élaborer un processus de fouille de données multi-stratégies pour la reconnaissance automatique d'objets géographiques sur des images satellitaires ou aériennes. Ces dernières sont segmentées afin d'isoler des polygones définis par un ensemble de descripteurs de bas niveaux. Afin de leur affecter une sémantique, on applique dans un premier temps une classification. Si aucun objet géographique n'est identifié, on tente alors un appariement du polygone avec les concepts d'une ontologie d'objets géographiques. Un algorithme de navigation dans l'ontologie et une mesure de comparaison sémantique ont ainsi été développés, paramétrables selon le contexte d'appariement. Cette mesure évalue la pertinence d'un appariement et comprend une composante locale (comparaison au niveau du concept) et une composante globale (combinaison linéaire de mesures locales). La méthode proposée a été développée en JAVA et intégrée à la plate-forme FoDoMuSt. Les premières expérimentations et évaluations humaines sont très encourageantes.
**** *annee_2007  *numText_726
Dans cet article nous étudions la contribution des techniques de fouille de données à l'amélioration des services de communications instantanées sur IP tel que la messagerie instantanée (IM) et la téléphonie sur IP (ToIP).
**** *annee_2007  *numText_729
Afin de comparer l'organisation sociale d'une paysannerie médiévale avant et après la guerre de Cent Ans nous étudions la structure de réseaux sociaux construits à partir d'un corpus de contrats agraires. Faibles diamètres et fort clustering révèlent des graphes en petit monde. Comme beaucoup de grands réseaux d'interaction étudiés ces dernières années ces graphes sont sans échelle typique. Les distributions des degrés de leurs sommets sont bien ajustées par une loi de puissance tronquée par une coupure exponentielle. Ils possèdent en outre un club-huppé, c'est à dire un noyau dense et de faible diamètre regroupant les individus à forts degrés. La forme particulière des éléments propres du laplacien permet d'extraire des communautés qui se répartissent en étoile autour du club huppé.
**** *annee_2007  *numText_730
L'objectif de la fouille de données est la découverte sophistiquée de connaissances lisibles, surprenantes et possiblement utiles. Les aspects surprenant et utile font partie de la sémantique et nécessitent l'utilisation des connaissances du domaine, ce qui cause souvent le problème d'acquisition de la connaissance. Notre découverte des règles d'exception simultanée peut être une réponse à ce problème. Nous envisageons de trouver les connaissances surprenantes et possiblement utiles à travers notre forme de paire de règles d'exception. Les autres méthodes inventées concernent l'index d'évaluation et la recherche exhaustive. Plusieurs applications médicales seront présentées sur lesquelles nos propositions ont été appliquées.
**** *annee_2007  *numText_731
Le travail présenté dans cet article, rentre dans le cadre de la gestion des données privées en vue de la substitution, appelée remplaçabilité, dynamique des services Web. Trois contributions sont apportées, (1) modélisation des politiques privées spécifiant les règles d'utilisation des données privées, prenant en compte des aspects se rapportant aux services Web, (2) étendre les protocoles de conversations des services Web par le modèle proposé, afin d'apporter les primitives nécessaires pour l'analyse des protocoles en présence de ces règles, (3) définition d'un mécanisme d'analyse de la remplaçabilité d'un service par un autre en vue de ses politiques privées.
**** *annee_2007  *numText_732
RAS (Reference Annotation System) est un outil d'annotation de documents. Cet outil est le résultat de l'implémentation de notre approche d'annotation basée sur le contexte de citation. L'approche est indépendante du contenu et utilise un regroupement thématique des références construit à partir d'une classification floue non-supervisée. L'outil présenté dans cet article a été expérimentée et évaluée avec la base de documents scientifiques Citeseer.
**** *annee_2007  *numText_733
Notre objectif est de transformer les documents Web vers un schéma médiateur XML défini a priori. C'est une étape nécessaire pour de nombreuses tâches de recherche d'information concernant le Web Sémantique, les documents semi-structurés, le traitement de sources hétérogènes, etc. Elle permet d'associer une structure sémantiquement riche à des documents dont le formats ne contient que des informations de présentation. Nous proposons de traiter ce problème comme un problème d'apprentissage structuré en le formalisant comme une transformation d'arbre en arbre.Notre méthode de transformation comporte deux étapes. Dans une première étape, une grammaire hors-contexte probabiliste permet de générer un ensemble de solutions candidates. Dans une deuxième étape, ces solutions candidates sont ordonnées grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau. Cette étape d'ordonnancement nous permet d'utiliser de manière efficace des caractéristiques complexes définies à partir du document d'entrée et de la solution candidate.
**** *annee_2007  *numText_734
Les données vidéo ont la particularité d'être très volumineuses alors qu'elles contiennent peu d'information sémantique. Pour les analyser, il faut réduire la quantité d'information dans l'espace de recherche. Les données vidéo sont souvent considérées comme l'ensemble des pixels d'une succession d'images analysées séquentiellement. Dans cet article, nous proposons d'utiliser une analyse en composantes principales (ACP) pour réduire la dimensionnalité des informations sans perdre la nature tridimensionnelle des données initiales. Nous commençons par considérer des sous-séquences, dont le nombre de trames est le nombre de dimensions dans l'espace de représentation. Nous appliquons une ACP pour obtenir un espace de faible dimension où les points similaires sémantiquement sont proches. La sous-séquence est ensuite divisée en blocs tridimensionnels dont on projette l'ellipsoïde d'inertie dans le premier plan factoriel. Nous déduisons enfin le mouvement présent dans les blocs à partir des ellipses ainsi obtenues. Nous présenterons les résultats obtenus pour un problème de vidéosurveillance.
**** *annee_2007  *numText_735
La classification des images sonar est d'une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d'imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d'utiliser les théories de l'incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l'algorithme de classification. L'approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar.
**** *annee_2007  *numText_736
Dans cet article, nous présentons une approche de la segmentation thématique fondée sur une représentation en vecteurs sémantiques des phrases et des calculs de distance entre ces vecteurs. Les vecteurs sémantiques sont générés par le système SYGFRAN, un analyseur morpho-syntaxique et conceptuel de la langue française. La segmentation thématique s'effectue elle en recherchant des zones de transition au sein du texte grâce aux vecteurs sémantiques. L'évaluation de cette méthode s'est faite sur les données du défi DEFT'06.
**** *annee_2007  *numText_737
Cet article propose une méthodologie de recherche d'information qui utilise l'analyse conceptuelle conjointement avec la sémantique dans le but de fournir des réponses contextuelles à des requêtes sur le web. Le contexte conceptuel défini dans cet article peut être global - c'est-à-dire stable - ou instantané - c'est-à-dire borné par le contexte global. Notre méthodologie consiste en une première phase de pré traitement permettant de construire le contexte global, et une seconde phase de traitement en ligne des requêtes des utilisateurs, associées au contexte instantané. Notre processus de recherche d'information est illustré à travers une expérimentation dans le domaine du tourisme.
**** *annee_2007  *numText_738
Nous introduisons la notion de sous-base k-faible pour les règles d'association valides au sens de la confiance. Ces sous-bases k-faibles sont caractérisées en termes d'opérateurs de fermeture correspondant à des familles de Moore k-faiblement hiérarchiques.
**** *annee_2007  *numText_739
Les bases de données issues du monde réel contiennent souvent de nombreuses informations non renseignées. Durant le processus d'extraction de connaissances dans les bases de données, une phase de traitement spécifique de ces données est souvent nécessaire, permettant de les supprimer ou de les compléter. Lors de l'extraction de séquences fréquentes, ces données incomplètes sont la plupart du temps occultées. Ceci conduit parfois à l'élimination de plus de la moitié de la base et l'information extraite n'est plus représentative. Nous proposons donc de ne plus éliminer les enregistrements incomplets, mais d'utiliser l'information partielle qu'ils contiennent. La méthode proposée ignore en fait temporairement certaines données incomplètes pour les séquences recherchées. Les expérimentations sur jeux de données synthétiques montrent la validité de notre proposition aussi bien en terme de qualité des motifs extraits que de robustesse aux valeurs manquantes.
**** *annee_2007  *numText_740
Nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes RDF. Nous imposons, par souci de cohérence, que cette combinaison soit réalisée uniquement si les graphes RDF ne se contredisent pas. Pour déterminer la non-contradiction entre deux graphes RDF nous utilisons une mesure de similarité, calculée au moment de l'ajout de documents RDF dans la base de documents.
**** *annee_2007  *numText_741
Avec le développement de compteurs communicants, les consommations d'énergie électrique pourront à terme être télérelevées par les fournisseurs d'électricité à des pas de temps pouvant aller jusqu'à la seconde. Ceci générera des informations en continu, à un rythme rapide et en quantité importante. Les Systèmes de Gestion de Flux de Données (SGFD), aujourd'hui disponibles sous forme de prototypes, ont vocation à faciliter la gestion de tels flux. Cette communication décrit une étude expérimentale pour analyser les avantages et limites de l'utilisation de deux prototypes de SGFD (STREAM et TelegraphCQ) pour la gestion de données de consommation électrique.
**** *annee_2007  *numText_742
Le but dans ce travail consiste à concevoir et réaliser un Outil Logiciel, en utilisant les concepts du Web Usage Mining pour offrir aux web masters l'ensemble des connaissances, y inclut les statistiques sur leurs sites, afin de prendre les décisions adéquates. Il s'agit en fait, d'extraire de l'information à partir du fichier log du serveur Web, hébergeant le site Web, et de prendre les décisions pour découvrir les habitudes des internautes, et de répondre à leurs besoins en adaptant le contenu, la forme et l'agencement des pages web.
**** *annee_2007  *numText_743
La construction d'ontologies à partir de textes reste une tâche coûteuse en temps qui justifie l'émergence de l'Ontology Learning. Notre système, Dynamo, s'inscrit dans cette mouvance, en apportant une approche originale basée sur une architecture multi-agent adaptative. En particulier, l'article présente le cœur de notre approche, un algorithme distribué de classification hiérarchique qui s'applique sur les résultats d'un analyseur syntaxique. Cet algorithme est évalué et comparé à un algorithme centralisé plus conventionnel. Forts de ces résultats, nous discutons ses limites et dressons en perspective les aménagements à effectuer pour aller vers une solution complète de construction d'ontologies.
**** *annee_2007  *numText_744
Les algorithmes de fouille de données sont maintenant capables de traiter de grands volumes de données mais les utilisateurs sont souvent submergés par la quantité de motifs générés. En outre, dans certains cas, que ce soit pour des raisons de confidentialité ou de coûts, les utilisateurs peuvent ne pas avoir accès directement aux données et ne disposer que des motifs. Les utilisateurs n'ont plus alors la possibilité d'approfondir à partir des données initiales le processus de fouille de façon à extraire des motifs plus spécifiques. Pour remédier à cette situation, une solution consiste à gérer les motifs. Ainsi, dans cet article, nous présentons un cadre théorique permettant à un utilisateur de manipuler, en post-traitement, une collection de motifs préalablement extraite. Nous proposons de représenter la collection sous la forme d'un graphe qu'un utilisateur pourra ensuite exploiter à l'aide d'opérateurs algébriques pour y retrouver des motifs ou en chercher de nouveaux.
**** *annee_2007  *numText_745
La reconstruction de réseaux de gènes est un des défis majeurs de la post-génomique. A partir de données d'expression issues de puces à ADN, différentes techniques existent pour inférer des réseaux de gènes. Nous proposons dans ce papier une approche pour la visualisation de réseaux d'interactions entre gènes à partir de données d'expression. L'originalité de notre approche est de superposer des règles avec des sémantiques différentes au sein d'un même support visuel et de ne générer que les règles qui impliquent des gènes dits centraux. Ceux-ci sont spécifiés en amont par les experts et permettent de limiter la génération des règles aux seuls gènes qui intéressent les spécialistes. Une implémentation a été réalisée dans le logiciel libre MeV de l'institut TIGR.
**** *annee_2007  *numText_746
Ce programme effectue une segmentation en phrases d'un texte. Contrairement aux procédures classiques, nous n'utilisons pas d'annotations préliminaires et tirons parti d'un apprentissage guidé par l'utilisateur.
**** *annee_2007  *numText_747
Nous proposons dans cet article une méthode de clustering qui combine l'analyse dynamique et l'analyse statistique pour caractériser des états. Il s'agit d'une méthode de fouille de données qui travaille sur des ensembles de séries temporelles pour détecter des états; ces états représentent les informations les plus significatives du système. L'objectif de cette méthode non supervisée est d'extraire de la connaissance à partir de l'analyse des séries temporelles multiples. Elle s'appuie sur la détection de singularités dans les séries temporelles et sur l'analyse des corrélations des séries entre les intervalles définis par ces singularités. Pour l'application présentée, les séries temporelles sont des signaux biochimiques mesurés durant un bioprocédé. Cette approche est donc utilisée pour confirmer et enrichir la connaissance des experts du domaine des bioprocédés sans utiliser la connaissance a priori de ces experts. Elle est appliquée à la recherche d'états physiologiques dans un bioprocédé de type fed-batch.
**** *annee_2007  *numText_748
Nous nous intéressons à l'estimation de la distribution des rangs d'une variable cible numérique conditionnellement à un ensemble de prédicteurs numériques. Pour cela, nous proposons une nouvelle approche non paramétrique Bayesienne pour effectuer une partition rectangulaire optimale de chaque couple (cible, prédicteur) uniquement à partir des rangs des individus. Nous montrons ensuite comment les effectifs de ces grilles nous permettent de construire un estimateur univarié de la densité conditionnelle sur les rangs et un estimateur multivarié utilisant l'hypothèse Bayesienne naïve. Ces estimateurs sont comparés aux meilleures méthodes évaluées lors d'un récent Challenge sur l'estimation d'une densité prédictive. Si l'estimateur Bayésien naïf utilisant l'ensemble des prédicteurs se révèle peu performant, l'estimateur univarié et l'estimateur combinant deux prédicteurs donne de très bons résultats malgré leur simplicité.
**** *annee_2007  *numText_749
Cet article présente un cadre sociotechnique pour le KM. Cette vision sociotechnique du KM permet : (1) d'écarter le KM d'un souci commercial ; (2) faire le clivage des différentes technologies du KM ; et (3) de s'interroger sur les paradigmes associés aux composants social et technique du KM. C'est précisément ce dernier point que cet article développe afin d'identifier les mécanismes génériques du KM. Plus précisément, l'aspect social est décrit à travers l'approche organisationnelle du KM, l'approche managériale du KM, et l'approche biologique du KM, alors que l'aspect technique est décrit à travers l'approche ingénierie des connaissances et compétences du KM. Ces approches nous conduisent aussi à donner un tableau comparatif entre ces visions organisationnelles, managériales et biologiques du KM.
**** *annee_2007  *numText_750
Le problème de choix d'architecture d'un réseau de neurones multicouches reste toujours très difficile à résoudre dans un processus de fouille de données. Ce papier recense quelques algorithmes de recherche d'architectures d'un réseau de neurones pour les tâches de classification. Il présente également une analyse théorique et expérimentale de ces algorithmes. Ce travail confirme les difficultés de choix des paramètres d'apprentissage (modèle, nombre de couches, nombre de neurones par couches, taux d'apprentissage, algorithme d'apprentissage,...) communs à tout processus de construction de réseaux de neurones et les difficultés de choix de paramètres propres à certains algorithmes.
**** *annee_2007  *numText_751
Nous présentons dans cet article une extension de XQuery que nous avons développée pour interroger le contenu et la structure de documents XML. Cette extension consiste à intégrer dans XQuery le langage NEXI, un sous-ensemble de XPath, défini dans le cadre de l'initiative INEX. Notre proposition est double : (i) équiper NEXI d'une sémantique floue, (ii) intégrer NEXI dans XQuery au moyen d'une métafonction appelée nexi, ayant une requête NEXI comme paramètre, et d'une extension de la clause for de l'opérateur FLWOR de XQuery. De plus, nous décrivons le prototype paramétrable que nous avons développé au dessus de deux moteurs XQuery classiques : Galax et Saxon.
**** *annee_2007  *numText_752
Cet article présente une méthode permettant d'interpréter la sortie d'un modèle de classification ou de régression. L'interprétation se base sur l'importance de la variable et l'importance de la valeur de la variable. Cette approche permet d'interpréter la sortie du modèle pour chaque instance.
**** *annee_2007  *numText_753
En préparation des données pour la classification supervisée, les méthodes filtres usuellement utilisées pour la sélection de variables sont efficaces en temps de calcul. Néanmoins, leur nature univariée ne permet pas de détecter les redondances ou les interactions constructives entre variables. Cet article présente une nouvelle méthode permettant d'évaluer l'importance prédictive jointe d'une paire de variables de façon automatique, rapide et fiable. Elle est basée sur un partitionnement de chaque variable exogène, en intervalles dans le cas numérique et groupes de valeurs dans le cas catégoriel. La grille de données exogène résultante permet alors d'évaluer la corrélation entre la paire de variables exogènes et la variable endogène. Le meilleur partitionnement bivarié est recherché au moyen d'une approche Bayésienne de la sélection de modèle. Les expérimentations démontrent les apports de la méthode, notamment une amélioration significative des performances en classification.
**** *annee_2007  *numText_754
Dans cet article, nous nous intéressons à Fuzzy C-Means (FCM), une technique très connue pour la classification floue. Nous proposons un algorithme efficace basé sur la programmation DC (Difference of Convexe functions) et DCA (DC Algorithm) pour résoudre ce problème. Les expériences numériques comparatives avec l'algorithme standard FCM sur les données réelles montrent la robustesse, la performance de cet nouvel algorithme DCA et sa supériorité par rapport à FCM.
**** *annee_2007  *numText_755
Dans ce papier, une nouvelle plate-forme d'alignement et de visualisation des ontologies, appelée POVA (Prototype OWL-Lite Visual Alignment), est décrite. Le module d'alignement implémente une nouvelle approche d'alignement d'ontologies remédiant au problème de la circularité et de l'intervention de l'utilisateur.
**** *annee_2007  *numText_756
En fouille de règles, certaines situations exceptionnelles défient le bon sens. C'est le cas de la règle R : a --> c et b --> c et (a et b) --> non c. Une telle règle, que nous étudions dans l'article, est appelée règle d'exception. A la suite des travaux précurseurs de E. Suzuki et Y. Kodratoff (1999), qui ont étudié un autre type de règle d'exception, nous cherchons ici à caractériser les conditions d'apparition de la règle R dans le cadre de l'Analyse Statistique Implicative.
**** *annee_2007  *numText_757
Cet article est consacré au problème de la catégorisation multilingue qui consiste à catégoriser des documents de différentes langues en utilisant le même classifieur. L'approche que nous proposons est basée sur l'idée d'étendre l'utilisation de WordNet dans la catégorisation monolingue vers la catégorisation multilingue.
**** *annee_2007  *numText_758
Parmi les outils de visualisation de données multidimensionnelles figurent d'une part les méthodes fondées sur la décomposition aux valeurs singulières, et d'autre part les méthodes de classification, incluant les cartes auto-organisées de Kohonen. Comment valider ces visualisations ? On présente sept procédures de validation par bootstrap qui dépendent des données, des hypothèses, des outils : a) le bootstrap partiel, qui considère les réplications comme des variables supplémentaires; b) le bootstrap total de type 1, qui réanalyse les réplications avec changements éventuels de signes des axes; c) le bootstrap total de type 2 qui corrige aussi les interversions d'axes; d) le bootstrap total de type 3, sur lequel on insistera, qui corrige les réplications par rotations procrustéenne; e) le bootstrap spécifique (cas des hiérarchies d'individus statistiques et des données textuelles). f) le bootstrap sur variables. g) les extensions des procédures précédentes à certaines cartes auto-organisées.
**** *annee_2007  *numText_759
Dans cet article, nous présentons un algorithme multi-agents de clustering dynamique. Ce type de clustering doit permettre de gérer des données évolutives et donc être capable d'adapter en permanence les clusters construits.
**** *annee_2007  *numText_760
Cet article montre l'intérêt de combiner des méthodes numériques et symboliques pour obtenir une annotation sémantique des images IRM du cerveau humain. Il s'agit d'identifier des structures anatomiques du cortex cérébral humain, en utilisant conjointement des connaissances a priori de nature numérique et une ontologie des structures corticales du cerveau représentée en OWL DL, étendue par des règles SWRL. Ces connaissances symboliques a priori représentées dans des langages standards du Web deviennent non seulement partageables mais permettent aussi un raisonnement automatique qui aide l'utilisateur à la labellisation des structures anatomiques mises en évidence dans des images IRM du cerveau d'un individu donné.
**** *annee_2007  *numText_761
Le projet B-Ontology a pour but l'extraction, l'organisation et l'exploitation de connaissances biographiques à partir de dépêches de presse. Sa réalisation requiert l'intégration de diverses technologies, principalement l'extraction d'information, les ontologies et bases de connaissances, les techniques de data mining. Cet article propose un aperçu des choix réalisés dans le cadre du projet. Cette démarche permet également de définir un environnement d'outils utiles pour les applications d'extraction et de gestion de connaissances.
**** *annee_2007  *numText_762
L'extraction de motifs séquentiels est un défi important pour la communauté fouille de données. Même si les représentations condensées ont montré leur intérêt dans le domaine des itemsets, à l'heure actuelle peu de travaux considèrent ce type de représentation pour extraire des motifs. Cet article propose d'établir les premières bases formelles pour obtenir les bornes inférieures et supérieures du support d'une séquence S. Nous démontrons que ces bornes peuvent être dérivées à partir des sous-séquences de S et prouvons que ces règles de dérivation permettent la construction d'une nouvelle représentation condensée de l'ensemble des motifs fréquents. Les différentes expérimentations menées montrent que notre approche offre une meilleure représentation condensée que celles des motifs clos et cela sans perte d'information.
**** *annee_2007  *numText_763
La recherche de règles d'association est une question centrale en Extraction de Connaissances dans les Données (ECD). Dans cet article, nous nous intéressons plus particulièrement à la restitution visuelle de règles pertinentes dans un corpus très important. Nous proposons ainsi un prototype basé sur une approche de type wrapper par intégration des phases d'extraction et de visualisation de l'ECD. Tout d'abord, le processus d'extraction génère une base générique de règles et dans un second temps, la tâche de visualisation s'appuie sur un processus de regroupement (clustering) permettant de grouper et de visualiser un sous-ensemble de règles d'association génériques. Le rendu visuel à l'écran exploite une représentation de type Fisheye view de manière à obtenir simultanément une représentation globale des différents groupes de règles et une vue détaillée du groupe sélectionné.
**** *annee_2007  *numText_764
Cet article décrit une étude de cas exhibant les qualités de la plate-forme de visualisation de graphes Tulip, démontrant l'apport de la visualisation à la fouille de données interactive et à l'extraction de connaissances. Le calcul d'un graphe à partir d'indices de similarité est un exemple typique où l'exploration visuelle et interactive de graphes vient en appui au travail de fouille de données. Nous penchons sur le cas où l'on souhaite étudier une collection de documents afin d'avoir une idée des thématiques abordées dans la collection.
**** *annee_2007  *numText_765
Nous présentons une méthode d'exploration des résultats des algorithmes d'apprentissage par arbre de décision (comme C4.5). La méthode présentée utilise simultanément une visualisation radiale, focus+context, fisheye et hiérarchique pour la représentation et l'exploration des résultats des algorithmes d'arbre de décision. L'utilisateur peut ainsi extraire facilement des règles d'induction et élaguer l'arbre obtenu dans une phase de post-traitement. Cela lui permet d'avoir une meilleure compréhension des résultats obtenus. Les résultats des tests numériques avec des ensembles de données réelles montrent que la méthode proposée permet une bien meilleure compréhension des résultats des arbres de décision.
**** *annee_2007  *numText_766
WebdocEnrich est une approche d'enrichissement sémantique automatique de documents HTML hétérogènes qui exploite une description du domaine pour enrichir le contenu des documents et les représenter en XML.
**** *annee_2006  *numText_767
Le temps nécessaire pour écouter un flux audio est un facteur réduisant l'accès efficace à de grandes archives de parole. Une première approche, la structuration automatique des données,permet d'utiliser un moteur de recherche pour cibler plus rapidement l'information. Les listes de résultats générées sont longues dans un souci d'exhaustivité. Alors que pour des documents textuels, un coup d'oeil discrimine un résultat intéressant d'un résultat non pertinent, il faut écouter l'audio dans son intégralité pour en capturer le contenu. Nous proposons donc d'utiliser le résumé automatique afin de structurer les résultats des recherches et d'en réduire la redondance.
**** *annee_2006  *numText_768
On s'intéresse à la construction d'arbres de décision sur des données symboliques de type intervalle en utilisant le critère de découpage binaire de Kolmogorov-Smirnov. Nous proposons une approche permettant d'affecter un individu à la fois aux deux nœuds fils générés par le partitionnement d'un noeud non terminal. Le but de cette méthode est de prendre en compte le positionnement de la donnée à classer par rapport à la donnée seuil de coupure.
**** *annee_2006  *numText_770
Nous présentons un algorithme génétique semi-interactif de sélection de dimensions dans les grands ensembles de données pour la détection d'individus atypiques (outliers). Les ensembles de données possédant un nombre élevé de dimensions posent de nombreux problèmes aux algorithmes de fouille de données, une solution est d'effectuer un pré-traitement afin de ne retenir que les dimensions intéressantes. Nous utilisons un algorithme génétique pour le choix du sous-ensemble de dimensions à retenir. Par ailleurs nous souhaitons donner un rôle plus important à l'utilisateur dans le processus de fouille, nous avons donc développé un algorithme génétique semi-interactif où l'évaluation des solutions n'élimine pas complètement la fonction d'évaluation mais la couple avec une évaluation de l'utilisateur. Enfin,l'importante réduction du nombre de dimensions nous permet de visualiser lesrésultats de l'algorithme de détection d'outlier. Cette visualisation permet à l'expert des données d'étiqueter les éléments atypiques (erreurs ou simplement des individus différents de la masse).
**** *annee_2006  *numText_771
Dans la littérature, de nombreux travaux traitent de méthodes d'alignement d'ontologies. Ils utilisent, pour la plupart, des relations basées sur des mesures de similarité qui ont la particularité d'être symétriques. Cependant, peu de travaux évaluent l'intérêt d'utiliser des mesures d'appariement asymétriques dans le but d'enrichir l'alignement produit. Ainsi, nous proposons dans ce papier une méthode d'alignement extensionnelle et asymétrique basée sur la découverte des implications significatives entre deux ontologies. Notre approche,basée sur le modèle probabiliste d'écart à l'indépendance appelé intensité d'implication,est divisée en deux parties consécutives : (1) l'extraction, à partir du corpus textuel associé à l'ontologie, et l'association des termes aux concepts;(2) la découverte et sélection des implications génératrices les plus significatives entre les concepts. La méthode proposée est évaluée sur deux jeux de données réels portant respectivement sur des profils d'entreprises et sur des catalogues de cours d'universités. Les résultats obtenus montrent que l'on peut trouver des relations pertinentes qui sont ignorées par un alignement basé seulement sur des mesures de similarité.
**** *annee_2006  *numText_772
La technique des motifs fréquents a été utilisée pour améliorer le pouvoir prédictif des stratégies quantitatives. Innovant dans le contexte des marchés financiers, notre méthode associe une signature aux configurations de marché fréquentes. Un système de « trading » automatique sélectionne les meilleures signatures par une procédure de « back testing » itérative et les utilise en combinaison avec l'indicateur technique pour améliorer sa performance.L'application des motifs fréquents à cette problématique des indicateurs techniques est une contribution originale. Au sens du test t de Student, notre méthode améliore nettement les approches sans signatures. La technique a été testé sur des données journalières type taux d'intérêt et actions. Notre analyse des indicateurs (Williams%R, BN et croisement des moments) a montré que qu'une approche par signatures est particulièrement bien adaptée aux stratégies à mémoire courte.
**** *annee_2006  *numText_773
Dans cet article, nous présentons un modèle de fouille des usages de la vidéo pour améliorer la qualité de l'indexation. Nous proposons une approche basée sur un modèle à deux niveaux représentant le comportement des utilisateurs exploitant un moteur de recherche vidéo. Le premier niveau consiste à modéliser le comportement lors de la lecture d'une vidéo unique (comportement intra vidéo), le second à modéliser le comportement sur l'ensemble d'une session(comportement inter video). A partir de cette représentation, nous avons développé un algorithme de regroupement, adapté à la nature particulière de ces données. L'analyse des usages de la vidéo nous permet d'affiner l'indexation vidéo sur la base de l'intérêt des utilisateurs.
**** *annee_2006  *numText_774
Cet article présente un système automatique d'annotation sémantique de pages web. Les systèmes d'annotation automatique existants sont essentiellement syntaxiques, même lorsque les travaux visent à produire une annotation sémantique. La prise en compte d'informations sémantiques sur le domaine pour l'annotation d'un élément dans une page web à partir d'une ontologie suppose d'aborder conjointement deux problèmes : (1) l'identification de la structure syntaxique caractérisant cet élément dans la page web et (2) l'identification du concept le plus spécifique (en termes de subsumption) dans l'ontologie dont l'instance sera utilisée pour annoter cet élément. Notre démarche repose sur la mise en œuvre d'une technique d'apprentissage issue initialement des wrappers que nous avons articulée avec des raisonnements exploitant la structure formelle de l'ontologie.
**** *annee_2006  *numText_775
L'apprentissage de structure des réseaux bayésien à partir de données est un problème NP-difficile pour lequel de nombreuses heuristiques ont été proposées. Dans cet article, nous proposons une nouvelle méthode inspirée des travaux sur la recherche de motifs fréquents corrélés pour identifier les causalités entre les variables. L'algorithme opère en quatre temps : (1) la découverte par niveau des motifs fréquents corrélés minimaux ; (2) la construction d'un graphe non orienté à partir de ces motifs ; (3) la détection des V_structures et l'orientation partielle du graphe ; (4) l'élimination des arêtes superflues par des tests d'indépendance conditionnelle. La méthode, appliquée au réseau Asia, permet de retrouver la structure du graphe initial. Nous l'appliquons ensuite aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx(NPC). L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC.
**** *annee_2006  *numText_776
Cet article propose d'utiliser l'entropie informationnelle pour analyser des modèles de chroniques découverts selon une approche stochastique (Bouché et Le Goc, 2005). Il décrit une adaptation de l'algorithme TemporalID3 (Console et Picardi, 2003) permettant de découvrir des modèles de chroniques à partir d'un ensemble d'apprentissage contenant des séquences d'occurrences d'événements discrets. Ces séquences représentent des suites d'alarmes générées par un système à base de connaissance de monitoring et de diagnostic de systèmes dynamiques. On montre sur un exemple que l'approche entropique complète l'approche stochastique en identifiant les classes d'événements qui contribuent le plus significativement à la prédiction d'une occurrence d'une classe particulière.
**** *annee_2006  *numText_778
Nous présentons une nouvelle méthode d'induction d'arbre de décision appelée MuMTree (pour Multi Models Tree) utilisable pour les modes d'apprentissage supervisé, non supervisé, supervisé à plusieurs variables cibles. Nous présentons les différents principes nécessaires pour réaliser un tel arbre de décision. Nous illustrons ensuite, sur un cas de modélisation multi-cibles, les avantages de cette méthode par rapport à un arbre de décision classique.
**** *annee_2006  *numText_781
Récemment la communauté Extraction de Connaissances s'est intéressée à de nouveaux modèles où les données arrivent séquentiellement sous la forme d'un flot rapide et continu, i.e. les data streams. L'une des particularités importantes de ces flots est que seule une quantité d'information partielle est disponible au cours du temps. Ainsi après différentes mises à jour successives, il devient indispensable de considérer l'incertitude inhérente à l'information retenue. Dans cet article, nous introduisons une nouvelle approche statistique en biaisant les valeurs supports pour les motifs fréquents. Cette dernière a l'avantage de maximiser l'un des deux paramètres (précision ou rappel) déterminés par l'utilisateur tout en limitant la dégradation sur le paramètre non choisi. Pour cela, nous définissons les notions de bordures statistiques. Celles-ci constituent les ensembles de motifs candidats qui s'avèrent très pertinents à utiliser dans le cas de la mise à jour incrémentale des streams. Les différentes expérimentations effectuées dans le cadre de recherche de motifs séquentiels ont montré l'intérêt de l'approche et le potentiel des techniques utilisées.
**** *annee_2006  *numText_782
Les méthodes factorielles d'analyse exploratoire statistique définissent des directions orthogonales informatives à partir d'un ensemble de données.Elles conduisent par exemple à expliquer les proximités entre individus à l'aide d'un groupe de variables caractéristiques.Dans le contexte du datamining lorsque les tableaux de données sont de grande taille, une méthode de cartographie synthétique s'avère intéressante. Ainsi une carte auto-organisatrice (SOM) est une méthode de partitionnement munie d'une structure de graphe de voisinage -sur les classes- le plus souvent planaire. Des travaux récents sont développés pour étendre le SOM probabiliste Generative Topographic Mapping (GTM) aux modèles de mélanges classiques pour données discrètes. Dans ce papier nous présentons et étudions un modèle génératif symétrique de carte auto-organisatrice pour données binaires que nous appelons Bernoulli Aspect Topological Model(BATM). Nous introduisons un nouveau lissage et accélérons la convergence de l'estimation par une initialisation originale des probabilités en jeu.
**** *annee_2006  *numText_783
Les modèles conditionnels du type modèles de Markov d'entropie maximale et champs de Markov conditionnels apportent des réponses aux lacunes des modèles de Markov cachés traditionnellement employés pour la classification et la segmentation de séquences. Ces modèles conditionnels ont été essentiellement utilisés jusqu'à présent dans des tâches d'extraction d'information ou d'étiquetage morphosyntaxique. Cette contribution explore l'emploi de ces modèles pour des données de nature différente, de type« signal », telles que la parole ou l'écriture en ligne. Nous proposons des architectures de modèles adaptées à ces tâches pour lesquelles nous avons dérivé les algorithmes d'inférence et d'apprentissage correspondant. Nous fournissons des résultats expérimentaux pour deux tâches de classification et d'étiquetage de séquences.
**** *annee_2006  *numText_784
Le choix du taux d'élagage est crucial dans le but d'acquérir une terminologie de qualité à partir de corpus de spécialité. Cet article présente une étude expérimentale consistant à déterminer le taux d'élagage le plus adapté.Plusieurs mesures d'évaluation peuvent être utilisées pour déterminer ce taux tels que la précision, le rappel et le Fscore. Cette étude s'appuie sur une autre mesure d'évaluation qui semble particulièrement bien adaptée pour l'extraction de la terminologie : les courbes ROC (Receiver Operating Characteristics).
**** *annee_2006  *numText_785
Cet article présente un nouveau modèle de représentation pour la classification de documents XML. Notre approche permet de prendre en compte soit la structure seule, soit la structure et le contenu de ces documents. L'idée est de représenter un document par l'ensemble des sous-chemins de l'arbre XML de longueur comprise entre n et m, deux valeurs fixées a priori. Ces chemins sont ensuite considérés comme de simples mots sur lesquels on peut appliquer des méthodes standards de classification, par exemple K-means. Nous évaluons notre méthode sur deux collections: la collection INEX et les rapports d'activité de l'INRIA. Nous utilisons un ensemble de mesures bien connues dans le domaine de la recherche d'information lorsque les classes sont connues a priori. Lorsqu'elles ne sont pas connues, nous proposons une analyse qualitative des résultats qui s'appuie sur les mots (chemins) les plus caractéristiques des classes générées.
**** *annee_2006  *numText_786
Dans cet article, nous proposons un système de classification des comptes-rendus mammographiques, reposant sur une ontologie radiologique décrivant les signes radiologiques et les différentes classes de la classification ACR des systèmes BIRADS dans le langage OWL. Le système est conçu pour extraire les faits issus des textes libres de comptes-rendus en étant dirigé par l'ontologie, puis inférer la classe correspondante et en déduire l'attitude à tenir à partir de la classification ACR. Ce travail présente la construction d'une ontologie radiologique mammaire dans le langage OWL et son intérêt pour classer automatiquement les comptes-rendus de mammographies.
**** *annee_2006  *numText_787
Ces dernières années, la classification croisée ou classification par blocs, c'est-à-dire la recherche simultanée d'une partition des lignes et d'une partition des colonnes d'un tableau de données, est devenue un outil très utilisé en fouille de données. Dans ce domaine, l'information se présente souvent sous forme de tableaux de contingence ou tableaux de co-occurrence croisant les modalités de deux variables qualitatives. Dans cet article, nous étudions le problème de la classification croisée de ce type de données en nous appuyant sur un modèle de mélange probabiliste. En utilisant l'approche vraisemblance classifiante, nous proposons un algorithme de classification croisée basé sur la maximisation alternée de la vraisemblance associée à deux mélanges multinomiaux classiqueset nous montrons alors que sous certaines contraintes restrictives, on retrouve les critères du Chi2 et de l'information mutuelle. Des résultats sur des données simulées et des données réelles illustrent et confirment l'efficacité et l'intérêt de cette approche.
**** *annee_2006  *numText_789
On présente deux méthodes de classification hiérarchique ascendante de variables quantitatives et de fréquences. Chaque nœud de ces hiérarchies regroupe deux classes de variables à partir d'une analyse factorielle particulière basée sur les variables représentatives de ces deux classes. Par cette méthode,on dispose, à chaque pas, d'un plan factoriel permettant de représenter à la fois les variables des deux classes fusionnées et l'ensemble des individus.Ces derniers se positionnent dans ce plan suivant leurs valeurs pour les variables considérées. Ainsi, l'interprétation des nœuds obtenus s'effectue facilement à partir de l'examen de ces représentations factorielles. La répartition des individus observée dans chacun de ces plans factoriels permet également de définir une segmentation des individus en total accord avec la hiérarchie des variables obtenues. On montre le fonctionnement des méthodes sur des exemples réels.
**** *annee_2006  *numText_790
L'extraction non supervisée et incrémentale de classes sur un flot de données (data stream clustering) est un domaine en pleine expansion. La plupart des approches visent l'efficacité informatique. La nôtre, bien que se prêtant à un passage à l'échelle en mode distribué, relève d'une problématique qualitative, applicable en particulier au domaine de la veille informationnelle :faire apparaître les évolutions fines, les « signaux faibles », à partir des thématiques extraites d'un flot de documents. Notre méthode GERMEN localise de façon exhaustive les maxima du paysage de densité des données à l'instant t,en identifiant les perturbations locales du paysage à t-1 et modifications de frontières induites par le document présenté. Son caractère optimal provient de son exhaustivité (à une valeur du paramètre de localité correspond un ensemble unique de maxima, et un découpage unique des classes qui la rend indépendante de tout paramètre d'initialisation et de l'ordre des données.
**** *annee_2006  *numText_791
Les étiqueteurs morphosyntaxiques sont de plus en plus performants et cependant, un véritable problème apparaît lorsque nous voulons étiqueter des corpus de spécialité pour lesquels nous n'avons pas de corpus annotés. La correction des ambiguïtés difficiles est une étape importante pour obtenir un corpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuer le nombre de fautes, nous utilisons une approche itérative appelée Induction Progressive. Cette approche est une combinaison d'apprentissage automatique,de règles rédigées par l'expert et de corrections manuelles qui se combinent itérativement afin d'obtenir une amélioration de l'étiquetage tout en restreignant les actions de l'expert à la résolution de problèmes de plus en plus délicats. L'approche proposée nous a permis d'obtenir un corpus de biologie moléculaire « correctement » étiqueté. En utilisant ce corpus, nous avons effectué une étude comparative de quatre étiqueteurs supervisés.
**** *annee_2006  *numText_793
Cet article présente deux modes de représentation de l'information dans le cadre d'une problématique en sciences du vivant. Le premier, appliqué à la microbiologie prévisionnelle, s'appuie sur deux formalismes, le modèle relationnel et les graphes conceptuels, interrogés uniformément via une même interface.Le second, appliqué aux technologies des céréales, utilise le seul modèle relationnel. Cet article décrit les caractéristiques des données et compare les solutions de représentation adoptées dans les deux systèmes.
**** *annee_2006  *numText_794
L'obtention d'une classification des pages d'un site web en fonction des navigations extraites des fichiers logs du serveur peut s'avérer très utile pour évaluer l'adéquation entre la structure du site et l'attente des utilisateurs. On construit une telle typologie en s'appuyant une mesure de dissimilarité entre les pages, définie à partir des navigations. Le choix de la mesure la plus appropriée à l'analyse du site est donc fondamental. Dans cet article, nous présentons un site de petite taille dont les pages sont classées en catégories sémantiques par un expert. Nous confrontons ce classement aux partitions obtenues à partir de diverses dissimilarités afin d'en étudier les avantages et inconvénients.
**** *annee_2006  *numText_796
Le choix des mesures d'intérêt (MI) afin d'évaluer les règles d'association est devenu une question importante pour le post-traitement des connaissance en ECD. Dans la littérature, de nombreux auteurs ont discuté et comparé les propriétés des MI afin d'améliorer le choix des meilleures mesures. Cependant,il s'avère que la qualité d'une règle est contextuelle : elle dépend à la fois de la structure de données et des buts du décideur. Ainsi, certaines mesures peuvent être appropriées dans un certain contexte, mais pas dans d'autres. Dans cet article,nous présentons une nouvelle approche contextuelle mise en application par un nouvel outil, ARQAT, permettant à un décideur d'évaluer et de comparerle comportement des MI sur ses jeux de données spécifiques. Cette approche est basée sur l'analyse visuelle d'un graphe de corrélation entre des MI objectives.Nous employons ensuite cette approche afin de comparer et de discuter le comportement de trente-six mesures d'intérêt sur deux ensembles de données a priori très opposés : un premier dont les données sont fortement corrélées et un second aux données faiblement corrélées. Alors que nous attendions des différences importantes entre les graphes de corrélation de ces deux jeux d'essai, nous avons pu observer des stabilités de corrélation entre certaines MI qui sont révélatrices de propriétés indépendantes de la nature des données observées. Ces stabilités sont récapitulées et analysées.
**** *annee_2006  *numText_799
L'extraction de règles d'association génère souvent un grand nombre de règles. Pour les classer et les valider, de nombreuses mesures statistiques ont été proposées ; elles permettent de mettre en avant telles ou telles caractéristiques des règles extraites. Elles ont pour point commun d'être fonction croissante du nombre de transactions et aboutissent bien souvent à l'acceptation de toutes les règles lorsque la base de données est de grande taille. Dans cet article, nous proposons une mesure inspirée de la notion de valeur-test. Elle présente comme principale caractéristique d'être insensible à la taille de la base, évitant ainsi l'écueil des règles fallacieusement significatives.Elle permet également de mettre sur un même pied, et donc de les comparer,des règles qui auront été extraites de bases de données différentes. Elle permet enfin de gérer différents seuils de signification des règles. Le comportement de la mesure est détaillé sur un exemple.
**** *annee_2006  *numText_800
L'objet de la recherche présentée est de concevoir un environnement informatique d'apprentissage qui permette de réduire l'écart entre la formation théorique des chirurgiens et leur formation pratique, qui se déroule principalement sur le mode du compagnonnage. L'article expose la méthodologie et quelques illustrations du travail didactique d'analyse des connaissances et du système d'enseignement / apprentissage en milieu hospitalier (chirurgie orthopédique) ainsi que partie de la formalisation informatique de cette connaissance. Cette modélisation permet la prise en compte dans l'environnement informatique de connaissances pragmatiques pour le diagnostic des connaissances de l'utilisateur en fonction des actions qu'il effectue à l'interface pendant la résolution d'un problème (pose de vis dans le bassin), et la prise de décision didactique qui suit : quelle rétroaction fournir pour affiner le diagnostic, et/ou permettre l'apprentissage souhaité.
**** *annee_2006  *numText_801
Les systèmes pair-à-pair (peer-to-peer, P2P, égal-à-égal) se sont popularisés ces dernières années avec les systèmes de partage de fichiers sur Internet.De nombreuses recherches concernant l'optimisation de la localisation des données ont émergé et constituent un axe de recherche très actif. La prise en compte de la sémantique du contenu des pairs dans le routage des requêtes permet d'améliorer considérablement la localisation des données. Nous nous concentrons sur l'approche PlanetP, faisant usage de la notion de filtre de Bloom, qui consiste à propager une signature sémantique des pairs (filtres de Bloom) à travers le réseau. Nous présentons cette approche et en proposons une amélioration: la création de filtres de Bloom dynamiques, dans le sens où leur taille dépend de la charge des pairs (nombre de documents partagés).
**** *annee_2006  *numText_802
Dans de nombreux domaines, la recherche de connaissances temporelles est très appréciée. Des techniques ont été proposées aussi bien en fouille de données qu'en apprentissage, afin d'extraire et de gérer de telles connaissances,en les associant également à la spécification de contraintes temporelles (e.g.: fenêtre temporelle maximale), notamment dans le contexte de la recherche de motifs séquentiels. Cependant, ces contraintes sont souvent trop rigides ou nécessitent une bonne connaissance du domaine pour ne pas extraire des informations erronées. C'est pourquoi nous proposons une approche basée sur la construction de graphes de séquences afin de prendre en compte des contraintes de temps plus souples. Ces contraintes sont relâchées par rapport aux contraintes de temps précédemment proposées. Elles permettent donc d'extraire plus de motifs pertinents. Afin de guider l'analyse des motifs obtenus, nous proposons également un niveau de précision des contraintes temporelles pour les motifs extraits.
**** *annee_2006  *numText_806
Les méthodes de représentation sémantique des mots à partir d'une analyse statistique sont basées sur des comptes de co-occurences entre mots et unités textuelles. Ces méthodes ont des paramétrages complexes, notamment le type d'unité textuelle utilisée comme contexte. Ces paramètres déterminent fortement la qualité des résultats obtenus. Dans cet article, nous nous intéressons au paramétrage de la technique dite Hyperspace Analogue to Language (HAL).Nous proposons une nouvelle méthode pour explorer ses paramètres discriminants. Cette méthode est basée sur l'analyse d'un graphe de voisinage d'une liste de mots de référence pré-classés. Nous expérimentons cette méthode et en donnons les premiers résultats qui renforcent et complètent des résultats issus de travaux précédents.
**** *annee_2006  *numText_807
La navigation au sein de bases de connaissances reste un problème ouvert. S'il existe plusieurs paradigmes de visualisation, peu de travaux sur les retours d'expérience sont disponibles. Dans le cadre de cet article nous nous sommes intéressés aux différents paradigmes de navigation interactive au sein de bases documentaires annotées sémantiquement ; l'accès à la base de connaissances s'effectuant à travers l'ontologie du domaine d'application. Ces paradigmes ont été évalués dans le cadre d'une application industrielle(mécanique des fluides et échangeurs thermiques) en fonction de critères définis par les utilisateurs. L'analyse des retours d'expérience1 nous a permis de spécifier et de réaliser un nouveau navigateur dédié à la gestion de documents techniques annotés par une ontologie de domaine : le « Eye Tree »,navigateur de type « polar fisheye view ».
**** *annee_2006  *numText_809
Nous décrivons dans cet article une chaîne de traitement complète et générique permettant d'extraire automatiquement les champs numériques (numéros de téléphone, codes clients, codes postaux) dans des documents manuscrits libres. Notre chaîne de traitement est constituée des trois étapes suivantes:localisation des champs numériques potentiels selon une approche markovienne sans reconnaissance chiffre ni segmentation, reconnaissance des séquences extraites,et vérification des hypothèses de localisation / reconnaissance en vue de limiter la fausse alarme générée lors de l'étape de localisation. L'évaluation de notre système sur une base de 300 courriers manuscrits montre des performances en rappel-précision intéressantes.
**** *annee_2006  *numText_810
Dans cet article, nous présentons une méthode mixte de segmentation d'objets visuels dans une séquence d'images d'une vidéo combinant à la fois une segmentation basée régions et l'estimation de mouvement par flot optique.L'approche développée est basé sur une minimisation d'une fonctionnelle d 'énergie (E) qui fait intervenir les probabilités d'appartenance (densité) avec une gaussienne, en tenant compte des informations perceptuelles de couleur et de texture des régions d'intérêt. Pour améliorer la méthode de détection et de suivi, nous avons étendu la formulation énergétique de notre modèle de contour actif en incluant une force supplémentaire issue du calcul du flot optique.Nous montrons l'intérêt de cette approche mixte en terme de temps de calcul et d'extraction d'objets vidéo complexes, et nous présentons les résultatso btenus sur des séquences de corpus vidéo couleur.
**** *annee_2006  *numText_811
Ces dernières années, de nouvelles contraintes sont apparues pour les techniques de fouille de données. Ces contraintes sont typiques d'un nouveau genre de données : les data streams. Dans un processus de fouille appliqué sur un data stream, l'utilisation de la mémoire est limitée, de nouveaux éléments sont générés en permanence et doivent être traités le plus rapidement possible,aucun opérateur bloquant ne peut être appliqué sur les données et celles-ci ne peuvent être observées qu'une seule fois. A l'heure actuelle, la majorité des travaux relatifs à l'extraction de motifs dans les data streams ne concernent pas les motifs temporels. Nous montrons dans cet article que cela est principalement dû au phénomène combinatoire qui est lié à l'extraction de motifs séquentiels. Nous proposons alors un algorithme basé sur l'alignement de séquences pour extraire les motifs séquentiels dans les data streams. Afin de respecter la contrainte d'une passe unique sur les données, une heuristique gloutonne est proposée pour segmenter les séquences. Nous montrons enfin que notre proposition est capable d'extraire des motifs pertinents avec un support très faible.
**** *annee_2006  *numText_812
Nous présentons un système pour l'inférence de programmes d'extraction de relations dans les documents Web. Il utilise les vues textuelle et structurelle sur les documents. L'extraction des relations est incrémentale et utilise des méthodes de composition et d'enrichissement. Nous montrons que notre système est capable d'extraire des relations pour les organisations existantes dans les documents Web (listes, tables, tables tournées, tables croisées).
**** *annee_2006  *numText_813
Nous présentons ici un système d'extraction et d'identification d'entités nommées complexes à l'intention des corpus de spécialité biomédicale. Nous avons développé une méthode qui repose sur une approche mixte à base d'ensemble de règles a priori et de dictionnaires contrôlés. Cet article expose les techniques que nous avons mises en place pour éviter ou minimiser les problèmes de synonymie, de variabilité des termes et pour limiter la présence de noms ambigus. Nous décrivons l'intégration de ces méthodes au sein du processus de reconnaissance des entités nommées. L'intérêt de cet outil réside dans la complexité et l'hétérogénéité des entités extraites. Cette méthode ne se limite pas à la détection des noms des gènes ou des protéines, mais s'adapte à d'autres descripteurs biomédicaux. Nous avons expérimenté cette approche en mesurant les performances obtenues sur le corpus de référence GENIA.
**** *annee_2006  *numText_815
Dans cet article, nous proposons une méthode de classification croisée permettant de classer des protéines, d'une part, et de classer des descripteurs (3-grammes) selon leurs pertinences par rapport aux groupes de protéines obtenus,d'autres part.
**** *annee_2006  *numText_816
La gestion des connaissances, enjeu majeur pour l'industrie, est entrée dans une phase concrète de déploiement. La conjonction d'une maturité des organisations dans la maîtrise de leur métier, la consolidation de méthodes et les outils évolutifs pour faire vivre un patrimoine de connaissances favorisent l'émergence de projets significatifs et leur diffusion opérationnelle au sein de grands groupes industriels. ICARE chez PSA Peugeot Citroën réalisé avec l'environnement Ardans Knowledge Maker en est ici l'exemple.
**** *annee_2006  *numText_817
Le problème de l'exploitation des règles associatives est devenu primordial,puisque le nombre des règles associatives extraites des jeux de données réelles devient très élevé. Une solution possible consiste à ne dériver qu'une base générique de règles associatives. Cet ensemble de taille réduite permet de générer toutes les règles associatives via un système axiomatique adéquat. Dans cet article, nous proposons une nouvelle approche FAST-MGB qui permet de dériver, directement à partir du contexte d'extraction formel, une base générique minimale de règles associatives.
**** *annee_2006  *numText_819
La quantité de sources d'information disponible sur Internet fait des systèmes d'échanges pair-à-pair (P2P) un genre nouveau d'architecture qui offre à une large communauté des applications pour partager des fichiers, des calculs,dialoguer ou communiquer en temps réel. Dans cet article, nous proposons une nouvelle approche pour améliorer la localisation d'une ressource sur un réseauP2P non structuré. En utilisant une nouvelle heuristique, nous proposons d'extraire des motifs qui apparaissent dans un grand nombre de nœuds du réseau.Cette connaissance est très utile pour proposer aux utilisateurs des fichiers souvent demandés (en requête ou en téléchargement) et éviter une trop grandeconsommation de la bande passante.
**** *annee_2006  *numText_820
Ce qui caractérise la fouille de données spatiales est la nécessité de prendre en compte les interactions des objets dans l'espace. Les méthodes classiques de fouille de données sont mal adaptées pour ce type d'analyse. Nous proposons dans cet article une approche basée sur la programmation logique inductive. Elle se base sur deux idées. La première consiste à matérialiser ces interactions spatiales dans des tables de distances, ramenant ainsi la fouille de données spatiales à la fouille de données multi-tables. La seconde transforme les données en logique du premier ordre et applique ensuite la programmation logique inductive. Cet article présentera cette approche. Il décrira son application à la classification supervisée par arbre de décision spatial. Il présentera aussi les expérimentations réalisées et les résultats obtenus sur l'analyse de la contamination des coquillages dans la lagune de Thau.
**** *annee_2006  *numText_822
La découverte d'informations cachées dans les bases de données multimédias est une tâche difficile à cause de leur structure complexe et à la subjectivité liée à leur interprétation. Face à cette situation, l'utilisation d'un index est primordiale. Un index multimédia permet de regrouper les données selon des critères de similarité. Nous proposons dans cet article d'apporter une amélioration à une approche déjà existante d'interrogation d'images par le contenu .Nous proposons une méthode efficace pour mettre à jour, localement, les graphes de voisinage qui constituent notre structure d'index multimédia. Cette méthode est basée sur une manière intelligente de localisation de points dans un espace multidimensionnel. Des résultats prometteurs sont obtenus après des expérimentations sur diverses bases de données.
**** *annee_2006  *numText_824
Intégrer le traitement de requêtes de recherche d'information dans un médiateur XML est un problème difficile. Ceci est notamment dû au fait que certaines sources de données ne permettent pas de recherche sur mot-clefs et distance ni de classer les résultats suivant leur pertinence. Dans cet article nous abordons l'intégration des fonctionnalités principales du standard XQuery Text dans XLive, un médiateur XML/XQuery. Pour cela nous avons choisi d'indexer des vues virtuelles de documents. Les documents virtuels sélectionnés sont transformés en objets des sources. L'opérateur de sélection du médiateur est étendu pour supporter des recherches d'information sur les documents de la vue. La recherche sur mots-clefs et le classement de résultat sont ainsi supportés. Notre formule de classement de résultats est adaptée au format de données semi-structurées, basé sur le nombre de mots-clefs dans les différents éléments et la distance entre les éléments d'un résultat.
**** *annee_2006  *numText_825
OWL est un langage pour la description d'ontologies sur le Web. Cependant,en tant que langage, OWL ne fournit aucun moyen pour interpréter les ontologies qu'il décrit, et étant orienté machine, il reste difficilement compréhensible par l'humain. On propose une approche de visualisation, d'interrogation et de vérification de documents OWL, regroupées dans un unique environnement graphique : le modèle des graphes conceptuels.
**** *annee_2006  *numText_826
La synthèse en chimie organique consiste à concevoir de nouvelles molécules à partir de réactifs et de réactions. Les experts de la synthèse s'appuient sur de très grandes bases de données de réactions qu'ils consultent à travers des procédures d'interrogation standard. Un processus de découverte de nouvelles réactions leur permettrait de mettre au point de nouveaux procédés de synthèse. Cet article présente une modélisation des réactions par des graphes et introduit une méthode de fouille de ces graphes de réaction qui permet de faire émerger des motifs génériques utiles à la prédiction de nouvelles réactions. Enfin l'article fait le point sur l'état actuel de ce travail de recherche en présentant le modèle général dans lequel s'intégrera un nouvel algorithme de fouille de réactions chimiques.
**** *annee_2006  *numText_827
Dans cet article nous nous attaquons au problème du forage de très grandes bases de données distribuées. Le résultat visé est un modèle qui soit et prédictif et descriptif, appelé méta-classificateur. Pour ce faire, nous proposons de miner à distance chaque base de données indépendamment. Puis, il s'agit de regrouper les modèles produits (appelés classificateurs de base), sachant que chaque forage produira un modèle prédictif et descriptif, représenté pour nos besoins par un ensemble de règles de classification. Afin de guider l'assemblage de l'ensemble final de règles, qui sera l'union des ensembles individuels de règles,un coefficient de confiance est attribué à chaque règle de chaque ensemble. Ce coefficient, calculé par des moyens statistiques, représente la confiance que nous pouvons avoir dans chaque règle en fonction de sa couverture et de son taux d'erreur face à sa capacité d'être appliquée correctement sur de nouvelles données. Nous démontrons dans cet article que, grâce à ce coefficient de confiance, l'agrégation pure et simple de tous les classificateurs de base pour obtenir un agrégat de règles produit un méta-classificateur rapide et efficace par rapport aux techniques existantes.
**** *annee_2006  *numText_830
Nos travaux visent à proposer une mémoire d'expertises décisionnelles permettant de conserver et de manipuler non seulement les données décisionnelles mais aussi l'expertise analytique des décideurs. Les données décisionnelles sont représentées au travers de concepts multidimensionnels et l'expertise associée est matérialisée grâce au concept d'annotation
**** *annee_2006  *numText_831
Dans cet article nous proposons d'exploiter des mesures décrivant la qualité des données pour définir la qualité des règles d'associations résultant d'un processus de fouille. Nous proposons un modèle décisionnel probabiliste basé sur le coût de la sélection de règles légitimement, potentiellement intéressantes ou inintéressantes si la qualité des données à l'origine de leur calcul est bonne, moyenne ou douteuse. Les expériences sur les données de KDD-CUP-98 montrent que les 10 meilleures règles sélectionnées d'après leurs mesures de support et confiance ne sont intéressantes que dans le cas où la qualité de leurs données est correcte voire améliorée.
**** *annee_2006  *numText_832
Pour comprendre et représenter les évolutions du bâti, question renouvelée avec le développement des NTIC, l'analyste s'appuie sur des connaissances évolutives ayant dans notre champ d'application - le patrimoine architectural - un caractère spatialisable (par l'attachement à un lieu lambda) mais aussi des caractéristiques handicapantes (hétérogénéité, incertitudes et contradictions, etc.). En réponse, nous utilisons ce caractère spatialisable pour intégrer les ressources constituant le jeu de connaissances propre à chaque édifice: théorie, sources documentaires, observations. Cette démarche que nous nommons modélisation informationnelle a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution introduit les filiations de cette démarche, le cadre méthodologique qui la matérialise, et discute de son application au cas concret de la place centrale de Cracovie (Rynek Glowny) pour en évaluer l'apport potentiel en matière de gestion et de visualisation de connaissances.
**** *annee_2006  *numText_833
La fouille de données textuelles constitue un champ majeur du traitement automatique des données. Une large variété de conférences, comme TREC, lui sont consacrées. Dans cette étude, nous nous intéressons à la fouille de textes juridiques, dans l'objectif est le classement automatique de ces textes.Nous utilisons des outils d'analyses linguistiques (extraction de terminologie)dans le but de repérer les concepts présents dans le corpus. Ces concepts permettent de construire un espace de représentation de faible dimensionnalité,ce qui nous permet d'utiliser des algorithmes d'apprentissage basés sur des mesures de similarité entre individus, comme les graphes de voisinage. Nous comparons les résultats issus du graphe et de C4.5 avec les SVM qui eux sont utilisés sans réduction de la dimensionnalité.
**** *annee_2006  *numText_835
La recherche de médicaments passe par la synthèse de molécules candidates dont l'efficacité est ensuite testée. Ce processus peut être accéléré en identifiant les molécules non solubles, car celles-ci ne peuvent entrer dans la composition d'un médicament et ne devraient donc pas être étudiées. Des techniques ont été développées pour induire un modèle de prédiction de l'indice de solubilité, utilisant principalement des réseaux de neurones ou des régressions linéaires multiples. La plupart des travaux actuels visent à enrichir les données de caractéristiques supplémentaires sur les molécules. Dans cet article, nous étudions l'intérêt de la construction automatique d'attributs basée sur la structure intrinsèquement multi-relationnelle des données. Les attributs obtenus sont utilisés dans un algorithme d'arbre de modèles, auquel on associe une méthode de bagging. Les tests réalisés montrent que ces méthodes donnent des résultats comparables aux meilleures méthodes du domaine qui travaillent sur des attributs construits par les experts.
**** *annee_2006  *numText_836
La problématique générale présentée dans ce papier concerne les systèmes intelligents, dédiés pour l'aide à la prise de décision dans le domaine radar. Les premiers travaux ont donc consisté après avoir adapté le processus d'extraction de connaissances à partir de données (ECD) au domaine radar, à mettre en œuvre les étapes en amont de la phase de fouille de données. Nous nous limitons dans ce papier à la phase de préparation des données (images ISAR : Inverse Synthetic Aperture Radar). Nous introduisons ainsi la notion de qualité comme moyen d'évaluer l'imperfection dans les données radars expérimentales.
**** *annee_2006  *numText_837
Nous présentons une nouvelle approche pour le traitement des ensembles de données de très grande taille en fouille visuelle de données. Les limites de l'approche visuelle concernant le nombre d'individus et le nombre de dimensions sont connues de tous. Pour pouvoir traiter des ensembles de donnéesde grande taille, une solution possible est d'effectuer un prétraitement de l'ensemble de données avant d'appliquer l'algorithme interactif de fouille visuelle.Pour ce faire, nous utilisons la théorie du consensus (avec une affectation visuelle des poids). Nous évaluons les performances de notre nouvelle approche sur des ensembles de données de l'UCI et du Kent Ridge Bio MedicalDataset Repository.
**** *annee_2006  *numText_839
La recherche d'un schéma médiateur à partir d'un ensemble de schémas XML est une problématique actuelle où les résultats de recherche issus de la fouille de données arborescentes peuvent être adoptés. Dans ce contexte,plusieurs propositions ont été réalisées mais les méthodes de représentation des arborescences sont souvent trop coûteuses pour permettre un véritable passage à l'échelle. Dans cet article, nous proposons des algorithmes de recherche de sous-schémas fréquents basés sur une méthode originale de représentation de schémas XML. Nous décrivons brièvement la structure adoptée pour ensuite détailler les algorithmes de recherche de sous-arbres fréquents s'appuyant sur une telle structure. La représentation proposée et les algorithmes associés ont été évalués sur différentes bases synthétiques de schémas XML montrant ainsi l'intérêt de l'approche proposée
**** *annee_2006  *numText_840
Au cours de ces dernières années, de nombreuses techniques de stream mining ont été proposées afin d'analyser des flux de données en temps réel.Dans cet article, nous montrons comment nous avons utilisé des techniques de stream mining permettant la recherche d'objets massifs hiérarchiques (hierarchical heavy hitters) dans un flux de données pour identifier en temps réel dans un réseau IP les préfixes dont la contribution au trafic dépasse une certaine proportion de ce trafic pendant un intervalle de temps donné.
**** *annee_2006  *numText_841
Ce papier présente une approche qui s'appuie sur une ontologie pour reconnaître automatiquement des concepts spécifiques à un domaine dans un corpus en langue naturelle. La solution proposée est non-supervisée et peut s'appliquer à tout domaine pour lequel une ontologie a été déjà construite. Un corpus du domaine est utilisé dans lequel les concepts seront reconnus. Dans une première phase, des connaissances sont extraites de ce corpus en faisant appel à des fouilles de textes. Une ontologie du domaine est utilisée pour étiqueter ces connaissance. Le papier donne un aperçu des techniques de fouilles employées et décrit le processus d 'étiquetage. Les résultats d'une première expérimentation dans le domaine de l'accidentologie sont aussi présentés
**** *annee_2006  *numText_842
Ce papier présente la version adaptative d'un algorithme d'extraction d'épisodes temporels développé précédemment. Les trois paramètres de réglages de l'algorithme ne sont plus fixes. Ils sont modifiés en ligne en fonction de la variance estimée du signal que l'on veut décomposer en épisodes temporels. La version adaptative de l'algorithme a été utilisée pour reconnaître automatiquement des aspirations trachéales à partir de plusieurs variables physiologiques enregistrés sur des patients hospitalisés en réanimation.Des résultats préliminaires sont présentés dans ce papier.
**** *annee_2006  *numText_843
La communauté de fouille de données a développé un grand nombre d'indices permettant de mesurer la qualité des règles d'association (RA) selon diverses sémantiques (Guillet,2004). Cependant ces sémantiques, qui permettent d'interpréter les règles simples, s'avèrent d'utilisation trop complexe pour un expert dans le cas de règles à prémisse composée. Notre objectif est donc de sélectionner les règles à prémisse composée de type AB→C qui apportent une information supplémentaire à celle des règles simples A→C et B→C. Pour cela nous définissons un indice de gain d'une règle composée par rapport aux règles simples.Dans l'application présentée, nous extrayons des RA de résultats de classifications pour en faciliter l'analyse . Le gain a permis de filtrer des règles d'interprétation simple
**** *annee_2006  *numText_847
La classification suivant le plus proche voisin est une règle simple et performante. Sa mise en oeuvre pratique nécessite, tant pour des raisons de coût de calcul que de robustesse, de sélectionner les instances à conserver. La partition de Voronoi induite par les prototypes constitue la structure sous-jacente à cette règle. Dans cet article, on introduit un critère descriptif d'évaluation d'une telle partition, quantifiant le compromis entre nombre de cellules et discrimination de la variable cible entre les cellules. Une heuristique d'optimisation est proposée, tirant partie des propriétés des partitions de Voronoi et du critère. La méthode obtenue est comparée avec les standards sur une vingtaine de jeux de données de l'UCI. Notre technique ne souffre d'aucun défaut de performance prédictive, tout en sélectionnant un minimum d'instances. De plus, elle ne sur-apprend pas.
**** *annee_2006  *numText_848
Nous présentons un nouvel algorithme de SVM (Support Vector Machine ou Séparateur à Vaste Marge) linéaire et non-linéaire, parallèle et distribué permettant le traitement de grands ensembles de données dans un temps restreint sur du matériel standard. A partir de l'algorithme de Newton-GSVM proposé par Mangasarian, nous avons construit un algorithme incrémental, parallèle et distribué permettant d'améliorer les performances en temps d'exécution et mémoire en s'exécutant sur un groupe d'ordinateurs. Ce nouvel algorithme a la capacité de classifier un million d'individus en 20 dimensions et deux classes en quelques secondes sur un ensemble de dix PC
**** *annee_2006  *numText_850
Dans cet article, nous montrons comment les techniques de fouilles de données peuvent résoudre efficacement le problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. A partir d'une formalisation du problème de la réécriture dans le cadre de la logique de description ALN(Ov), nous montrons comment ce problème se rattache à un cadre de découverte de connaissances dans les bases de données. L'exploitation de ce cadre nous permet de bénéficier de solutions algorithmiques existantes pour la résolution du problème de réécriture. Nous proposons une implémentation de cette approche, puis nous l'expérimentons. Les premiers résultats démontrent l'intérêt d'une telle approche en termes de capacité à traiter un grand nombre de sources de données.
**** *annee_2006  *numText_851
Le logiciel Teximus Expertise est un outil évolué de gestion dynamique de connaissances basé sur les notions de référentiel sémantique. Cette suite intégrée facilite le partage de connaissances et d'informations dans les entreprises.
**** *annee_2006  *numText_852
L'analyse statistique implicative traite des tableaux sujets x variables afin d'extraire règles et méta-règles statistiques entre les variables.L'article interroge les structures obtenues représentées par graphe et hiérarchie orientés afin de dégager la responsabilité des sujets ou des groupes de sujets(variables supplémentaires) dans la constitution des chemins du graphe ou des classes de la hiérarchie. On distingue les concepts de typicalité pour signifier la proximité des sujets avec le comportement moyen de la population envers les règles statistiques extraites, puis de contribution pour quantifier le rôle qu'auraient les sujets par rapport aux règles strictes associées. Un exemple de données réelles, traité à l'aide du logiciel CHIC, illustre et montre l'intérêt de ces deux concepts.
**** *annee_2006  *numText_853
On ne peut s'intéresser aux textes sans s'intéresser à leur nature. La nature des textes permet de distinguer les textes d'un point de vue primaire. Elle est utilisée pour identifier les textes artificiels, pour la reconnaissance de la langue, afin d'identifier les SPAMS... En ce sens, la méthode la plus connue reste encore la méthode de Zipf. Cet article propose une nouvelle méthode basée sur un automate. L'automate construit un signal pour chaque texte. L'automate est présenté en détail et des expérimentations montrent son utilité dans les domaines aussi divers que ceux cités précédemment/
**** *annee_2006  *numText_855
Ce travail s'intègre dans la problématique générale de la recherche d'information ; et plus particulièrement dans la personnalisation et la qualité d'information. Dans cet article nous proposons un modèle multidimensionnel de la qualité de l'information décrivant les différents facteurs de qualité influant sur la personnalisation de l'information. Ce modèle permet de structurer les différents facteurs de qualité de l'information dans une hiérarchie afin d'assister l'utilisateur dans la construction de son propre profil selon ses besoins et ses exigences en termes de qualité.
**** *annee_2006  *numText_857
Les systèmes d'informations géographiques (SIG) sont utilisés pour améliorer l'efficacité des entreprises et des services publics, en associant méthodes d'optimisation et prise en compte de la dimension géographique.Cependant, les bases de données géographiques (BDG) stockées dans les SIG sont restreintes à l'application pour laquelle elles ont été conçues. Souvent, les utilisateurs demeurent contraints de l'existant et se trouvent dans le besoin de données complémentaires pour une prise de décision adéquate. D'où, l'idée de l'enrichissement de l'aspect descriptif des BDG existantes. Pour atteindre cet objectif, nous proposons une approche qui consiste à intégrer un module de fouille de données textuelles au SIG lui même. Il s'agit de proposer une méthode distribuée de résumé de documents multiples à partir de corpus en ligne.L'idée est de faire coopérer un ensemble d'agents s'entraidant afin d'aboutir à un résumé optimal.
**** *annee_2006  *numText_858
Ce papier est consacré à la simulation ou à la réalisation automatique de schémas tactiques par un groupe d´agents footballeurs autonomes. Son objectif est de montrer ce que peuvent apporter des techniques d'apprentissage par renforcement à des agents réactifs conçus pour cette tâche. Dans un premier temps, nous proposons une plate-forme et une architecture d'agents capable d'effectuer des schémas tactiques dans des cas relativement simples. Ensuite, nous mettons en œuvre un algorithme d'apprentissage par renforcement pour permettre aux agents de faire face à des situations plus complexes. Enfin,une série d'expérimentations montrent le gain apporté aux agents réactifs par l'utilisation d'algorithmes d'apprentissage.
**** *annee_2006  *numText_859
Le transit des flux d'information dans le réseau Internet à l'échelle mondiale est régi par des accords commerciaux entre systèmes autonomes, accords qui sont mis en œuvre via le protocole de routage BGP. La négociation de ces accords commerciaux repose implicitement sur une hiérarchie des systèmes autonomes et la position relative de deux systèmes débouche sur un accord de type client/fournisseur (un des systèmes, le client, est nettement mieux classé que l'autre, le fournisseur, et le client paye le fournisseur pour le transit des flux d'information) ou sur un accord de type peering (transit gratuit du trafic entre les deux systèmes). En dépit de son importance, il n'existe pas de hiérarchie officielle de l'Internet (les clauses commerciales des accords entre systèmes autonomes ne sont pas nécessairement publiques) ni de consensus sur la façon d'établir une telle hiérarchie. Nous proposons une heuristique simple inspirée de la notion de centralité spectrale issue de l'analyse des réseaux sociaux pour analyser la position relative des systèmes autonomes de l'Internet à partir des informations des seules informations de connectivité entre systèmes autonomes.
**** *annee_2006  *numText_860
Cet article propose une comparaison graphique de certains indices de pertinence pour évaluer l'intérêt des règles d'association. Nous nous sommes appuyés sur une étude existante pour sélectionner quelques indices auxquels nous avons ajouté l'indice de Jaccard et l'indice d'accords désaccords (IAD).Ces deux derniers nous semblent plus adaptés pour discriminer les règles intéressantes dans le cas où les items sont des événements peu fréquents. Une application est réalisée sur des données réelles issues du secteur automobile
**** *annee_2006  *numText_862
Les ontologies sont au cœur du processus de gestion des connaissances.Différentes mesures sémantiques ont été proposées dans la littérature pour évaluer quantitativement l'importance de la liaison sémantique entre paires de concepts. Cet article propose une synthèse analytique des principales mesures sémantiques basées sur une ontologie modélisée par un graphe et restreinte iciaux liens hiérarchiques is-a. Après avoir mis en évidence différentes limites des mesures actuelles, nous en proposons une nouvelle, la PSS (Proportion of SharedSpecificity), qui sans corpus externe, tient compte de la densité des liens dans le graphe reliant deux concepts
**** *annee_2006  *numText_863
Les résultats des méthodes de fouille de données sont difficilement interprétables par un utilisateur n'ayant pas l'expertise requise. Dans ce papier nous proposons un outil permettant aux utilisateurs d'interpréter les résultats issus des méthodes de classification non supervisée. Cet outil est basé sur des métadonnées utilisées pour formaliser le processus d'interprétation automatique. Ces métadonnées vont servir à l'utilisateur pour comprendre dans quelles circonstances les données originales ont été collectées et de quelle manière elles ont été agrégées puis classifiées. L'intérêt de ce travail porte sur la souplesse qu'auront les utilisateurs à pouvoir interpréter facilement les classes obtenues. Nous développons notre approche basée sur l'utilisation des métadonnées. Nous traduirons notre méthodologie par un exemple concret.
**** *annee_2006  *numText_864
Cet article aborde le problème de l'utilisation d'un modèle de connaissance dans un contexte de fouille de données. L'approche méthodologique proposée montre l'intérêt de la mise en œuvre de réseaux bayésiens couplée à l'extraction de règles d'association dites delta-fortes (membre gauche minimal, fréquence minimale et niveau de confiance contrôlé). La découverte de règles potentiellement utiles est alors facilitée par l'exploitation des connaissances décrites par l'expert et représentées dans le réseau bayésien. Cette approche est validée sur un cas d'application concernant la fouille de données d'interruptions opérationnelles dans l'industrie aéronautique.
**** *annee_2006  *numText_865
Un certain nombre de travaux en fouille de données se sont intéressés à l'extraction de motifs et à la génération de règles d'association à partir de ces motifs. Cependant, ces travaux se sont jusqu'à présent, centrés sur la notion de motifs fréquents. Le premier algorithme à avoir permis l'extraction de tous les motifs fréquents est Apriori mais d'autres ont été mis au point par la suite, certains n'extrayant que des sous-ensembles de ces motifs (motifs fermés fréquents, motifs fréquents maximaux, générateurs minimaux). Dans cet article, nous nous intéressons aux motifs rares qui peuvent également véhiculer des informations importantes. Les motifs rares correspondent au complémentaire des motifs fréquents. A notre connaissance, ces motifs n'ont pas encore été étudiés, malgré l'intérêt que certains domaines pourraient tirer de ce genre de modèle. C'est en particulier le cas de la médecine, où par exemple, il est important pour un praticien de repérer les symptômes non usuels ou les effets indésirables exceptionnels qui peuvent se déclarer chez un patient pour une pathologie ou un traitement donné.
**** *annee_2006  *numText_866
Les systèmes de gestion des connaissances servent de support pour la création et la diffusion de mémoires d'entreprises qui permettent de capitaliser,conserver et enrichir les connaissances des experts. Dans ces systèmes, l'interaction avec les experts est effectuée avec des outils adaptés dans lesquels une formalisation graphique des connaissances est utilisée. Cette formalisation est souvent basée au niveau théorique sur des modèles de graphes mais de façon pratique, les représentations visuelles sont souvent des arbres et des limitations apparaissent par rapport aux représentations basées sur des graphes. Dans cet article nous présentons le modèle utilisé par le serveur de connaissances Atanor qui utilise des arbres pour visualiser les connaissances, et nous développons une nouvelle approche qui permet de représenter les mêmes connaissances sous la forme de graphes en niveaux. Une analyse comparative des deux méthodes dans un contexte industriel de maintenance permet de mettre en valeur l'apport des graphes dans le processus de visualisation graphique des connaissances.
**** *annee_2006  *numText_867
Nous présentons dans cet article une méthode de visualisation interactive de données numériques ou symboliques permettant à un utilisateur expert du domaine d'obtenir des informations et des connaissances pertinentes. Nous proposons une approche nouvelle en adaptant l'utilisation des points d'intérêts dans un contexte de fouille visuelle de données. A partir d'un ensemble de points d'intérêt disposés sur un cercle, les données sont visualisées à l'intérieur de ce cercle en fonction de leur similarité à ces points d'intérêt. Des opérations interactives sont alors définies : sélectionner, zoomer, changer dynamiquement les points d'intérêts. Nous évaluons les propriétés d'une telle visualisation sur des données aux caractéristiques connues. Nous décrivons une application réelle encours dans le domaine de l'exploration de données issues d'enquêtes de satisfaction.
**** *annee_2006  *numText_868
Cet article décrit le projet MEAT (Mémoire d'Expériences pour l'Analyse du Transcriptome) dont le but est d'assister les biologistes travaillant dans le domaine des puces à ADN, pour l'interprétation et la validation de leurs résultats. Nous proposons une aide méthodologique et logicielle pour construire une mémoire d'expériences pour ce domaine. Notre approche, basée sur les technologies du web sémantique, repose sur l'utilisation des ontologies et des annotations sémantiques sur des articles scientifiques et d'autres sources de connaissances du domaine. Notre approche peut être généralisée à d'autres domaines requérant des expérimentations et traitant un grand flux de données(protéomique, chimie,etc.).
**** *annee_2006  *numText_869
Les techniques de Web Usage Mining existantes sont actuellement basées sur un découpage des données arbitraire (e.g. un log par mois) ou guidé par des résultats supposés (e.g. quels sont les comportements des clients pour la période des achats de Noël ? ). Ces approches souffrent des deux problèmes suivants. D'une part, elles dépendent de cette organisation arbitraire des données au cours du temps. D'autre part elles ne peuvent pas extraire automatiquement des pics saisonniers dans les données stockées. Nous proposons d'exploiter les données pour découvrir de manière automatique des périodes denses de comportements. Une période sera considérée comme dense si elle contient au moins un motif séquentiel fréquent pour l'ensemble des utilisateurs qui étaient connectés sur le site à cette période.
**** *annee_2005  *numText_870
Les services Web sont des technologies émergentes permettant une interopérabilité entre les différents acteurs (fournisseurs et demandeurs de services) du fait de leur architecture reposant sur des technologies standard.Cependant à ce jour, aucun des standards des services Web ne prend cependant réellement en charge le concept d'adaptation. Ceci est d'autant plus problématique que les utilisateurs de services Web attendent d'eux non seulement qu'ils répondant à leur besoin mais aussi qu'ils soient adaptés à leur profil (caractéristique personnelles, et celles de leur environnement). Nous proposons une extension du standard de description des services Web(WSDL), appelée AWSDL, afin de supporter l'adaptation des services Web.Un module a été développé pour mettre en correspondance les descriptions AWSDL des Services Web Adaptés (SWA) et les demandes des utilisateurs.
**** *annee_2005  *numText_871
Cet article décrit une méthode de prétraitement destinée à faciliter la découverte de motifs fréquents dans un log d'alarmes. Au cours d'une première étape les types d'alarmes qui présentent un comportement temporel similaire sont regroupés à l'aide d'une carte auto-organisatrice. Puis on recherche les parties du log qui sont riches en alarmes pour les différents groupes. Des sous-logs sont construits à partir des alarmes des zones sélectionnées. La méthode a été validée sur un log provenant d'un réseau ATM.
**** *annee_2005  *numText_872
Dans cet article nous présentons une application des courbes de Peano pour la caractérisation de régions par leur texture et l'établissement d'inter-relations spatiales à des fins de CBIR. Les résultats obtenus sont comparables à ceux d'un humain sur une base de 330 images aériennes.
**** *annee_2005  *numText_873
Pour reconnaître les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement à partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets à partir de ces graphes en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples à ce modèle.
**** *annee_2005  *numText_874
Nous décrivons comment apprendre automatiquement une hiérarchie de concepts à partir d'une collection de documents. Les concepts, identifiés par des ensembles de mots-clés, sont organisés en une hiérarchie de type spécialisation/généralisation. Cette hiérarchie peut être utilisée pour construire un modèle de domaine pour des collections de documents hypermédias. Nous proposons des idées sur la façon de construire des modèles d'utilisateurs à partir de tels modèles de domaine. Les modèles d'utilisateurs et de domaine peuvent être visualisés à l'aide d'outils efficaces comme les Treemaps.
**** *annee_2005  *numText_875
Nous présentons deux expériences d'apprentissage relationnel de motifs temporels comportant des contraintes numériques - des chroniques à partir de séries temporelles. La première concerne l'apprentissage d'arythmies cardiaques à partir d'´électrocardiogrammes. La deuxième réalise l'apprentissage de règles prédisant la dégradation de la qualité de service dans un réseau de télécommunications à partir de données d'exploitation. L'influence de la méthode de discrétisation et de segmentation des données sur la qualité des résultats est discutée.
**** *annee_2005  *numText_876
Nous reconsidérons dans cet article le critère BIC pour arbres d'induction proposé dans Ritschard et Zighed (2003, 2004) et discutons deux aspects liés à sa portée. Le premier concerne les possibilités de le calculer. Nous montrons comment il s'obtient à partir des statistiques du rapport vraisemblance utilisées pour tester l'indépendance ligne-colonne de tables de contingence. Le second point porte sur son intérêt dans une optique de classification. Nous illustrons sur l'exemple du Titanic la relation entre le BIC et le taux d'erreur en généralisation lorsqu'on regarde leur évolution selon la complexité de l'arbre. Nous esquissons un plan d'expérimentation en vue de vérifier la conjecture selon laquelle le BIC minimum assurerait en moyenne le meilleur taux d'erreur en généralisation.
**** *annee_2005  *numText_877
Le choix de mesures d'intérêt pour la validation des règles d'association constitue un défi important dans le contexte de l'évaluation de la qualité en fouille de données. Mais, comme l'intérêt dépend à la fois de la structure des données et des buts de l'utilisateur (décideur, analyste), certaines mesures peuvent s'avérer pertinentes dans un contexte donné, et ne plus l'être dans un autre. Dans cet article, nous proposons un outil original ARQAT afin d'étudier le comportement spécifique de 34 mesures d'intérêt dans le contexte d'un jeu de règles, selon une approche résolument exploratoire mettant en avant l'interactivité et les représentations graphiques.
**** *annee_2005  *numText_878
Le modèle des cartes cognitives offre une représentation graphique d'un réseau d'influences entre différentes notions. Nous proposons un nouveau modèle de cartes cognitives qui intègre la partie représentation des connaissances et l'opération de projection du modèle des graphes conceptuels
**** *annee_2005  *numText_880
Nous présentons une approche probabiliste pour déterminer les valeurs manquantes des objets incomplets pendant leur classement dans les arbres de décision. Cette approche est dérivée de la méthode d'apprentissage supervisé appelée Arbres d'Attributs Ordonnés proposée par Lobo et Numao en 2000, qui construit un arbre de décision pour chacun des attributs, selon un ordre croissant en fonction de l'information mutuelle entre chaque attribut et la classe. Notre approche étend la méthode de Lobo et Numao d'une part en prenant en compte les dépendances entre les attributs pour la construction des arbres d'attributs, et d'autre part en fournissant un résultat de classement d'un objet incomplet sous la forme d'une distribution de probabilités (au lieu de la classe la plus probable).
**** *annee_2005  *numText_881
Les praticiens et les chercheurs dans le domaine d'extraction des connaissances de données (ECD) sont souvent confrontés à des difficultés qui sont relatives à la nature des données, à l'implication de l'opérateur humain et aux aspects algorithmiques. Aujourd'hui, s'il y a un consensus sur la complexité du processus d'ECD, ce n'est pas le cas pour la définition et la caractérisation de cette complexité. Définir la complexité de l'ECD, la caractériser et connaître ses sources sont des questions qui animent aujourd'hui la communauté de fouille de données. Dans cet article, pour répondre à ces questions, nous menons une réflexion sur la notion de complexité en ECD en utilisant l'approche systémique, une approche de modélisation de systèmes complexes.
**** *annee_2005  *numText_883
Le Web Usage Mining est un processus d'extraction de connaissance qui permet la détection d'un type de comportement usager sur un site internet. Cette tâche relève de l'extraction de connaissances à partir de données : plusieurs étapes sont nécessaires à la réalisation du processus complet. Les données brutes, utilisées et souvent incomplètes correspondent aux requêtes enregistrées par un serveur. Le prétraitement nécessaire de ses données brutes pour les rendre exploitables se situe en amont du processus et est donc très important. Nous voulons travailler sur des modèles structurés, issus de l'inférence grammaticale. Nous détaillons un ensemble de techniques de traitement des données brutes et l'évaluons sur des données artificielles. Nous proposons, enfin, des expérimentations mettant en évidence l'affectation des algorithmes classiques d'inférence grammaticale par la mauvaise qualité des logs bruts.
**** *annee_2005  *numText_885
Les algorithmes actuels de fouille de données ne supportent que de façon très limitée les mécanismes de guidage et d'engagement d'expert dans le processus de découverte. Dans cet article, nous présentons une nouvelle approche interactive de fouille des images IRMf, guidée par les données, permettant l'observation du fonctionnement cérébral. La discrimination des voxels d'image du cerveau qui présentent une réelle activité est en général très difficile à cause d'un faible rapport signal sur bruit et de la présence d'artefacts. L'exploration de donnée visuelle se focalise sur l'intégration de l'utilisateur dans le processus de découverte de connaissance en utilisant des techniques de visualisation efficaces, d'interaction et de transfert de connaissances. Dans ce travail, nous montrons sur les données réelles, que l'exploration visuelle permet d'accélérer le processus d'exploration d'images IRMf et aboutit à de meilleurs résultats dotées d'une confiance accrue.
**** *annee_2005  *numText_886
Les systèmes d'apprentissage qui utilisent les TIC peuvent enregistrer sous forme électronique de nombreuses données. Ces données peuvent être fouillées par des logiciels adéquats pour en retirer des informations pédagogiques. Cet article illustre cette approche en prenant pour exemple le Logic-ITA, un système d'apprentissage pour les preuves formelles en logique.
**** *annee_2005  *numText_887
L'arrivée des images de télédétection à très haute résolution spatiale impose de reconsidérer les méthodes de description des surfaces représentées dans les images satellites. Dans ce qui suit, nous proposons une approche de segmentation morphologique auto-adaptative d'images satellitaires à très haute résolution spatiale. La segmentation est associée à l'exploitation des données élicitées tout au long du processus, dans l'objectif de collecter, modéliser et homogénéiser ces données au sein de descripteurs. Exploiter l'information ainsi disponible sur les objets implique la prise en compte des relations spatiales, décrivant les relations entre les objets, et leurs caractéristiques. La modélisation et la génération de descripteurs proposées rendent une telle approche opérationnelle originale dans le contexte des méthodes de télédétection.
**** *annee_2005  *numText_888
Nous présentons une réalisation en cours sur l'extraction de motifs temporels à partir de séquences d'événements dans le cadre de la détection dynamique des conflits ethno-politiques. Notre contexte d'application présente plusieurs difficultés : le phénomène que l'on cherche à modéliser est fortement variable et les données sont bruitées. Mais nous disposons d'une connaissance a priori du domaine qui peut être exploitée pour guider l'apprentissage en contraignant l'espace de recherche des motifs. Nous proposons une méthode supervisée d'apprentissage de scénarios dont l'originalité est d'utiliser une mesure de pertinence qualitative par opposition aux mesures basées sur la fréquence.Cette méthode intègre des concepts de logique floue.
**** *annee_2005  *numText_889
Le projet Pépite a pour objectif la construction d'un diagnostic des compétences d'élèves en algèbre élémentaire permettant aux enseignants de gérer la diversité cognitive de leurs élèves. Dans cet article, nous présentons une étude pluridisciplinaire (linguistique, didactique et informatique),s'appuyant sur un corpus de productions d'élèves utilisant le logiciel Pépite.Le corpus est analysé selon les points de vue croisés de la linguistique et de la didactique. L'objectif de cette démarche est d'améliorer l'évaluation des réponses d'élèves aux questions ouvertes quand ces derniers répondent avec leurs propres mots. Après avoir situé notre étude, nous présentons la méthodologie retenue et les premiers résultats. Nous montrons ensuite la pertinence de ces résultats avec le point de vue de la recherche sur l'enseignement des mathématiques. Nous terminons par les perspectives ouvertes par ce travail en nous interrogeant sur les apports d'approches EGC à la problématique du diagnostic de compétences
**** *annee_2005  *numText_890
En relation avec des approches classiques de l'incertain, l'analyse statistique implicative (A.S.I.) peut apparaître innovante, particulièrement pour l'opérateur d'implication. L'article montre en effet que la notion de variables à valeurs intervalles et celle de variables-intervalles sont efficaces dans la détermination de leur distribution et dans la recherche de règles entre variables floues. De plus, elles apportent de riches informations sur la qualité de ces règles, tout en permettant d'étudier le rôle des variables supplémentaires dans l'existence de ces règles. Cette nouvelle perspective épistémologique de l'incertain ouvre d'intéressantes perspectives d'application.
**** *annee_2005  *numText_891
Dans cet article, nous considérons les séquences vidéo couleur comme des données complexes. Notre contribution porte sur deux méthodes adaptées à ce type de données et permettant d'extraire des indices spatiaux et temporels. Nous pensons que ces méthodes peuvent être intégrées avec succès dans un processus plus complexe de fouille de données multimédia, aspect qui ne sera pas abordé ici. Les méthodes présentées sont basées sur l'espace Teinte Saturation Luminance. L'extraction d'indices spatiaux est assimilée au problème de la séparation du fond et des objets, résolu par une approche multi résolution ne nécessitant qu'une seule image. L'extraction d'indices temporels correspond à la détection des changements de plans dans une séquence d'images, obtenue par l'utilisation de mesures de distances indépendantes du contexte. Les caractéristiques communes de nos deux méthodes sont l'utilisation de l'espace TSL, l'efficacité calculatoire, et la robustesse aux artefacts. Nous illustrons ces approches par des résultats obtenus sur des séquences vidéo sportives.
**** *annee_2005  *numText_892
Une méthode générique pour l'extraction non supervisée de motifs dans des séquences temporelles multidimensionnelles et hétérogènes est proposée, puis expérimentée pour l'identification des comportements récurrents d'une personne à domicile. L'objectif est de concevoir un système d'apprentissage des habitudes de vie, à partir des données de capteurs, pour la détection d'évolutions critiques à long terme.
**** *annee_2005  *numText_893
De nos jours, les entreprises, organismes ou individus se trouvent submergés par la quantité d'information et de documents disponibles. Les utilisateurs ne sont plus capables d'analyser ou d'appréhender ces informations dans leur globalité. Dans ce contexte, il devient indispensable de proposer de nouvelles méthodes pour extraire et caractériser de manière automatique les informations contenues dans les bases documentaires. Nous proposons dans cet article l'approche IC-Doc de caractérisation automatique et thématique du contenu de collections de documents textuels. IC-Doc est basée sur une méthode originale d'extraction et de classification de connaissances textuelles prenant en considération les co-occurrences contextuelles et le partage de contextes entre les différents termes représentatifs du contenu. IC-Doc permet ainsi une extraction automatique de KDMs (Knowledge Dynamic Maps) sur les contenus des bases documentaires. Ces KDMs permettent de guider et d'aider les utilisateurs dans leurs tâches de consultations documentaires. Ce papier présente également une expérimentation de notre approche sur des collections de documents textuels.
**** *annee_2005  *numText_894
Nous décrivons un processus de fouille de données en bioinformatique. Il se traduit par la spécification de modèles de Markov cachés du second-ordre, leur apprentissage et leur utilisation pour permettre une segmentation de grandes séquences d'ADN en différentes classes qui traduisent chacune un état organisationnel et structural des motifs d'ADN locaux sous-jacents. Nous ne supposons aucune connaissance a priori sur les séquences que nous étudions. Dans le domaine informatique, ce travail est dédié à la définition d'observations structurées (les k-d-k-mers)permettant la localisation en contexte d'irrégularités, ainsi qu'à la description d'une méthode de classfication utilisant plusieurs classifieurs.Dans le domaine biologique, cet article décrit une méthode pour prédire des ensembles de gènes co-régulés, donc susceptibles d'avoir des fonctions liées en réponse à des conditions environnementales spécifiques.
**** *annee_2005  *numText_895
Nous présentons dans ce papier des approches de fusion d'informations haut niveau applicables pour des données numériques ou des données symboliques.Nous étudions l'intérêt des telles approches particulièrement pour la fusion de classifieurs. Une étude comparative est présentée dans le cadre de la caractérisation des fonds marins à partir d'images sonar. Reconnaître le type de sédiments sur des images sonar est un problème difficile en soi en partie à cause de la complexité des données. Nous comparons les approches de fusion d'informations haut niveau et montrons le gain obtenu.
**** *annee_2005  *numText_899
Dans le cadre d'un projet pluridisciplinaire relatif à la gestion intégrée du littoral (projet Syscolag), nous proposons un système de mutualisation de ressources et de connaissances. Ce système repose sur un service de métadonnées, une base de données inventaire d'objets géographiques de référence et un vocabulaire thématique co-construit par l'ensemble des partenaires. L'accès aux ressources partagées est guidé par une interface adaptable au gré de l'usage et axée sur des critères de recherche thématique, spatiaux et temporels.
**** *annee_2005  *numText_900
La mesure de la qualité des connaissances est une étape clef d'un processus de découverte de règles d'association. Dans cet article, nous présentons IPEE, un indice de qualité de règle qui a la particularité unique d'associer les deux caractéristiques suivantes : d'une part, il est fondé sur un modèle probabiliste, et d'autre part, il mesure un écart à l'équilibre(incertitude maximum de la conclusion sachant la prémisse vraie).
**** *annee_2005  *numText_901
De nombreux travaux ont porté sur l'extraction de règles d'association. Cependant, cette tâche continue à intéresser les chercheurs en fouille de données car elle soulève encore plusieurs défis. En particulier, son utilisation en pratique reste difficile : d'une part, le nombre de règles apprises est souvent très grand, d'autre part, le traitement des valeurs numériques dans cette tâche est loin d'être maîtrisé. Nous nous intéressons dans cet article au rôle que peut jouer l'utilisateur pour pallier ces difficultés. Il s'agit d'impliquer l'utilisateur dans le processus de recherche de règles d'association qui est dans ce cas interactif et guidé par des schémas de règles qu'il aurait choisis. Nous illustrons notre propos avec QuantMiner qui est un outil convivial et interactif que nous avons développé. La présence de l'expert reste indispensable durant tout le processus d'extraction de règles.
**** *annee_2005  *numText_903
Le Risque d'Audit est un indice d'existence d'erreurs dans les états financiers d'une entreprise. Trois modèles mathématiques sont associés à ce concept du RA : un modèle « Bayesien », un modèle « évidentialiste », et un modèle « flou ». Ces trois modèles accusent des incohérences mathématiques et des difficultés d'application pratique, surtout au niveau de la composante « Risque Inhérent » du risque d'audit. Ils considèrent le processus cognitif d'estimation du RI en tant que « boîte noire ». Nous proposons un simple algorithme d'inférence flou interprétable pour capter le processus cognitif d'estimation du RI, algorithme basé sur l'induction d'arbre de décision flou. Notre objectif est d'identifier les éléments de cette structure et de démontrer que l'utilisation d'une telle structure d'inférence floue est proche de la décision réelle d'estimation du RI. Il s'agit d'une recherche exploratoire et expérimentale.
**** *annee_2005  *numText_904
Nous présentons dans ce travail un outil pour la mesure d'audience sur Internet, reposant sur l'extraction de profils de navigation représentatifs de l'activité des internautes sur les sites. Ces profils sont obtenus par l'application d'un algorithme de classification non supervisée - inspiré du système de reconnaissance chimique des fourmis - sur des sessions de navigations construites à partir des fichiers log du site étudié. Cet algorithme de classification a été associé à une représentation multimodale des sessions utilisateurs permettant d'employer l'ensemble des informations à disposition dans les fichiers log (impacts sur les pages, heure de connexion, durée,séquence des pages, ...), ainsi qu'à une mesure de similarité adaptée pour créerles profils de chacun des clusters obtenus. Il reste cependant d'autres modalités(basées sur le contenu des documents accédés) qui pourraient améliorer la capacité de l'outil à donner du sens aux profils découverts.
**** *annee_2005  *numText_905
Cet article présente nos travaux sur la mesure de l'intérêt des règles d'association. Une vingtaine de mesures ont été retenues, sur la base d'un critère d'éligibilité.Différentes propriétés sont d'abord proposées qui fondent une étude formelle des mesures. Cette étude formelle se double d'une étude de comportement, grâce à HERBS, une plate-forme développée pour expérimenter les mesures sur des bases de règles. Il est alors possible de confronter la typologie formelle des mesures et la typologie expérimentale. Une fois transformées en critères, ces propriétés fondent une méthode d'assistance au choix de l'utilisateur. Le problème de la validation est enfin abordé, où l'on présente une méthode de contrôle du risque multiple adaptée au problème.
**** *annee_2005  *numText_906
Nous présentons une méthode de classification et de recherche de sources biologiques. Elle consiste à construire un treillis de Galois à partir d'un ensemble de méta-données associées aux sources et converties en propriétés booléenne. Un concept construit à partir d'une requête utilisateur est ensuite inséré dans le treillis grâce à un algorithme de construction incrémentale. Le calcul du résultat se ramène à extraire l'ensemble des sources figurant dans les extensions des subsumants du concept requête dans le treillis de Galois résultant. L'ordre de pertinence des sources est déduit à partir de l'ordre de subsumption des concepts correspondants dans le treillis. Une amélioration de la méthode consiste à enrichir la requête à partir d'ontologies de domaine avant de l'insérer dans le treillis. Deux modes d'enrichissement sont possibles : l'enrichissement par généralisation et l'enrichissement par spécialisation.
**** *annee_2005  *numText_907
Ces dernières années, les progrès en informatique et en imagerie numérique ont fait émerger une nouvelle discipline, la chirurgie assistée par ordinateur. Les systèmes de chirurgie assistée par ordinateur contribuent à l'amélioration du déroulement des procédures chirurgicales. Un des objectif à long terme de nos travaux est de proposer des solutions d'amélioration de ces systèmes, basées sur les connaissances du chirurgien quant au déroulement de la procédure, par l'utilisation d'un modèle générique qui permet de capturer et de représenter ces connaissances. Cet article présente une méthodologie d'exploitation d'un ensemble de cas chirurgicaux décrits à l'aide de ce modèle générique, par des algorithmes issus de l'extraction de connaissance à partir de données, afin de mettre en évidence des invariants dans les descriptions structurées du déroulement des cas chirurgicaux. Il détaille en outre les difficultés rencontrées de par notamment le caractère complexe des données étudiées.
**** *annee_2005  *numText_909
Les recherches en psychologie ont permis d'établir une relation entre émotions et prise de décision. La prise en compte de caractéristiques humaines telles que les émotions et la personnalité dans les processus d'interaction entre agents est au centre de ce travail. Il s'inscrit dans le cadre du projet GRACE(Groupes Relationnels d'Agents Collaborateurs Emotionnels)/ RIAM (Réseau des Industries, de l'Audiovisuel et du Multimédia) .
**** *annee_2005  *numText_910
Notre approche « sémantique de l'utilisabilité », basée sur la catégorisation, correspond à un mode de représentation des connaissances,sous la forme d'un treillis de Galois qui permet de modéliser et simuler les procédures utilisateurs sur un dispositif technique. Cette approche, qui diffère de celles qu'on trouve avec SOAR ou ACT, associe les actions et les procédures aux catégories d'objets, comme propriétés de ces catégories(Poitrenaud, Richard & Tijus, sous presse). L'accès aux actions et procédures a lieu à partir des catégories d'objets. Dans le cadre de cette approche, les erreurs relèvent de méprises catégorielles et l'analogie relève des processus de reconnaissance qui ont lieu lors de la catégorisation. La modélisation et la simulation dans le cadre de cette approche se réalisent avec les formalismes développés par Poitrenaud (1995): ProcOpe et STONE.
**** *annee_2005  *numText_911
Cet article présente OCGL (Ontology Conceptual Graph Language), un langage de représentation d'ontologie basé sur le modèle des Graphes Conceptuels. Il décrit en détail la façon dont une ontologie est modélisée en OCGL, et présente l'implémentation de ce langage dans l'atelier d'ingénierie ontologique TooCoM.
**** *annee_2005  *numText_912
L'objectif de cet article est de présenter un travail en cours qui consiste à proposer, implanter et valider expérimentalement un modèle pour estimer le coût d'un processus de nettoyage de documents XML. Notre approche de calcul de coût est basée sur une méthode par calibration selon une analyse probabiliste. Pour cela, nous proposons de calculer des probabilités de pollution et au préalable de détection des différents types de pollutions. Pour valider notre modèle, nous avons choisi de polluer artificiellement une collection de données XML avec l'ensemble des types d'erreurs possibles(erreurs typographiques, ajout de doublons, de valeurs manquantes, tronquées,censurées, etc.) et d'estimer, grâce au modèle proposé, le nombre et le coût des opérations nécessaires au nettoyage des données afin de proposer des stratégies de réparation ciblées et économes. Les expérimentations en cours ne sont pas rapportées dans cet article.
**** *annee_2005  *numText_913
Nous présentons dans cet article deux méthodes d'élaboration des signatures, une méthode globale à l'aide d'histogrammes et une méthode de description des régions et de leur disposition dans l'image. Nous exposons ensuite une méthode dédiée à la requête partielle qui est basée sur la mise en correspondance de graphes de régions et une méthode interactive basée sur l'apprentissage statistique.
**** *annee_2005  *numText_914
A ce jour, le média image est omniprésent dans de nombreuses applications. Un volume de données considérable est produit ce qui conduit à la nécessité de développer des outils permettant de retrouver efficacement de l'information pertinente. Les systèmes de recherche actuels montrent aujourd'hui leurs limites en raison de l'absence de sémantique. Une voie qui semble intéressante à explorer afin de combler le fossé existant entre les propriétés extraites et le contenu sémantique, est la fouille de données. C'est un domaine de recherche encore immature mais très prometteur. Cet article présente des travaux préliminaires sur la manière de définir de nouveaux descripteurs intégrant la sémantique. Le clustering et la caractérisation des classes obtenues sont utilisés pour réduire l'espace de recherche et produire une vue résumée de la base. La navigation basée sur une ontologie visuelle estun moyen puissant et convivial pour retrouver de l'information pertinente.
**** *annee_2005  *numText_915
Nous présentons dans ce papier un système de fouille coopérative de données d'usage de moteurs de recherche sur le Web dont l'objectif est d'améliorer le tri des résultats rendus par un moteur de recherche. Le système est construit selon une architecture multi-agents où chaque utilisateur est assisté par un agent personnel. Les agents coopèrent entre-eux et utilisent la méthodologie du raisonnement à partir de cas pour re-trier les résultats rendus par un moteur de recherche. Nous nous servons de ce système pour 1)présenter notre analyse des choix de conception d'un système d'exploration coopérative de données d'usage du Web et 2) montrer les problèmes qui restent à résoudre et l'apport attendu des techniques de fouille de données d'usage pour les résoudre.
**** *annee_2005  *numText_916
Le contexte intervient dans toute étude du comportement humain.Nous présentons les graphes contextuels qui sont utilisés dans de nombreux domaines comme l'intelligence artificielle, la psychologie, la sécurité informatique, la gestion d'incidents, le diagnostic médical, ... L'idée centrale de ce formalisme est la représentation au même niveau des éléments de compréhension d'un utilisateur et des éléments contextuels dans lesquels les éléments de compréhension prennent un sens et ont une validité. Nous donnons un exemple dans le domaine de la recherche d'information. Cette modélisation de l'utilisateur au travers de ses actions offre un intérêt pour redéfinir les tâches prescrites dans le cadre du travail collaboratif.
**** *annee_2005  *numText_918
Étant donné une famille de variables aléatoires (Xi)i∈I , munie de la structure de réseau bayésien et un sous-ensemble S de I, nous considérons le problème de calcul de la loi de la sous-famille (Xa)a∈S (resp. la loi de (Xb)b∈ ¯ S, où ¯ S = I − S, conditionnellement à (Xa)a∈S).Nous mettons en évidence la possibilité de décomposer cette tâche en plusieurs calculs parallèles dont chacun est associé à une partie de S (resp. de ¯ S) ; ces résultats partiels sont ensuite regroupés dans un produit. Dans le cas du calcul de (Xa)a∈S, ceci revient à la mise en place sur S d'une structure de réseau bayésien de niveau deux.
**** *annee_2005  *numText_921
Nous proposons un système fédérateur de caches pour les grilles que les applications de la grille utilisent comme un service de cache uniforme. Le système est fondé sur le concept de l'activité de données où les applications partagent et réutilisent l'information sémantique liée à l'activité des données sous la forme de métadonnées. Ces métadonnées représentent la connaissance sur les données et sur leur gestion. Elles permettent d'optimiser, suivant le contenu et l'utilisation de ces données, leur placement, leur recherche, leur durée de vie et leur pertinence vis-à-vis de leur exploitation.
**** *annee_2005  *numText_922
Il existe un besoin d'outils avancés d'apprentissage sur le Web. Le développement des nouvelles technologies comme le Web sémantique, le calcul sur grille et les services web ouvrent de nouvelles perspectives et défis pour la conception d'une nouvelle génération de systèmes d'apprentissage.Cette nouvelle génération peut être conçue comme des services distribués,autonomes, contextualisés, services web ou grille. Le papier présente le rôle et les perspectives des nouvelles technologies pour le développement d'une nouvelle génération de services d'apprentissage en s'appuyant sur le modèle utilisateur et sur la modélisation des utilisateurs à base d'ontologies.
**** *annee_2005  *numText_923
Le web peut être considéré comme une grande base de connaissances. La recherche des informations pertinentes sur la toile est rendue de plus en plus difficile, voire impossible avec l'accroissement de la volumétrie des pages disponibles. Le problème réside dans le fait que les outils existants ne peuvent pas s'appuyer actuellement sur une description du contenu des documents. Le web sémantique utilise différents langages pour mieux exploiter et traiter les contenus des ressources web. Dans le but de passer de UML vers OWL, il est intéressant d'étudier la possibilité de transformer chacun des concepts du diagramme de classe UML en OWL.
**** *annee_2005  *numText_924
Lorsqu'ils sont fortement prescriptifs, les Environnement Interactifs pour l'Apprentissage Humain (EIAH) cloisonnent les utilisateurs entre eux.Pour mettre en valeur le travail de chacun, nous voulons faire prendre conscience aux utilisateurs qu'ils font partie d'un groupe qui a des intérêts convergents. Nous pensons qu'un système adaptatif est le support approprié pour faire émerger la prise de conscience d'un groupe et ainsi développer des pratiques coopératives. Nous proposons de formaliser une architecture qui définit des services autonomes opérant en tâche de fond, pour extraire du système les données pertinentes déclenchant les mécanismes de signalisation adéquat. Notre support d'expérimentation est une plate-forme pour la formation des professeurs stagiaires de l'enseignement agricole public
**** *annee_2005  *numText_925
Les artistes en arts visuels et médiatiques explorent l'utilisation de nouvelles technologies pour créer des œuvres multimédia et qui sont diffusées en ligne, lors d'expositions ou d'installations. L'utilisation des technologies faites par ces créateurs pose de nombreux défis en termes de mécanismes à mettre en œuvre pour concevoir, créer, expérimenter et diffuser ces œuvres. Dans cet article, nous nous intéressons aux mécanismes d'adaptation pour la création d’œuvres médiatiques adaptatives et interactives. A travers un cas concret, nous proposons un modèle d'adaptation intégrant la gestion de différents types de métadonnées pour réaliser aussi bien l'adaptation sémantique que l'adaptation physique et qui peut être spécialisé selon les besoins spécifiques.
**** *annee_2005  *numText_926
Nous présentons dans cet article un EIAH conçu et développé selon des méthodes et techniques du Web sémantique et de l'ingénierie des connaissances. L'environnement d'apprentissage est conçu comme une mémoire de formation et le système que nous avons développé constitue un Web sémantique de formation (par extension de la notion de Web sémantique d'entreprise). Une approche d'acquisition et de gestion des connaissances a été adoptée pour expliciter la stratégie pédagogique d'un enseignant, acquérir des ressources pédagogiques à partir d'un document de cours initial et organiser ces ressources. La visualisation des ressources et la navigation de l'apprenant dans la mémoire ou le Web de formation est basée sur l'utilisation d'un moteur de recherche sémantique.
**** *annee_2005  *numText_927
UML est le langage graphique de référence dans l'industrie pour la modélisation objet. Cependant UML reste un langage, et ne fournit aucun moyen de vérification ou d'interrogation de ses schémas. Il existe aujourd'hui des outils de vérification, mais ils se comportent comme des boîtes noires où l'utilisateur ne peut accéder. Nous proposons une méthode graphique de vérification et d'interrogation de diagrammes de classes UML. L'aspect intuitif et dessinable de notre méthode offre à l'utilisateur la possibilité d'interroger le contenu de diagrammes de classes, ainsi que de définir et d'adapter ses propres critères de vérification. Le modèle calculatoire de notre approche est celui des graphes conceptuels
**** *annee_2005  *numText_928
Dans le contexte d'applications distribuées et d'environnements multi plates-formes, la personnalisation inclut le choix de la plate-forme d'interaction (PC,PDA ...), le mode d'interaction (textuel, vocal, ...), la configuration logicielle, les données fournies etc. Plusieurs méthodes, basées généralement sur le profil de l'utilisateur, permettent de prendre en compte ces différents aspects. Nous proposons une plate forme, à base d'agents logiciels, pouvant servir de support pour la conception de système de personnalisation (SP). Le SP facilite la gestion et la transmission des résultats ou recommandations dans un système distribué et évolutif. Le SP est appliqué pour la personnalisation de l'information transport. Il s'agit d'ici d'aider les usagers des transports collectifs dans leur choix d'itinéraires, de les informer des perturbations éventuelles et de les guider tout au long de leur déplacement.
**** *annee_2005  *numText_929
La recherche de structures fréquentes au sein de données arborescentes est une problématique actuellement très active qui trouve de nombreux intérêts dans le contexte de la fouille de données comme,par exemple, la construction automatique d'un schéma médiateur à partir de schémas XML. Dans ce contexte, de nombreuses propositions ont été réalisées mais les méthodes de représentation des arborescences sont très souvent trop coûteuses. Dans cet article, nous proposons donc une méthode originale de représentation de ces données. Les propriétés de cette représentation peuvent être avantageusement utilisées par les algorithmes de recherche de structures fréquentes (sous-arbres fréquents). La représentation proposée et les algorithmes associés ont été évalués sur des jeux de données synthétiques montrant ainsi l'interêt de l'approche proposée.
**** *annee_2005  *numText_930
Dans le cadre d'une validation d'expertise textuelle contenue dans un test de compétences comportementales informatisé, nous proposons un méthode visant à extraire des sous ensembles de termes caractéristiques utilisés pour décrire des caractères psychologiques. Notre approche consiste, après l'extraction de termes, à évaluer les associations possibles entre termes et caractères psychologiques qui structurent le corpus en s'appuyant sur la théorie de l'implication statistique.
**** *annee_2005  *numText_931
Le monitorage des paramètres physiologiques en milieu de Réanimation génère un flux abondant et continu de données. L'extraction et la synthèse des informations sont une étape obligatoire pour tout système d'aide à la décision. L'Analyse de la Tendance Linéaire par Morceaux (A.T.L.M.) est une méthode originale d'analyse de la dynamique qui utilise deux niveaux d'interprétation. Le premier niveau, mono paramétrique, exprime la variation de chaque paramètre en quatre classes (constant, diminue, augmente,transitoire) selon la méthode décrite dans (Calvelo et al. 2001). Le second niveau, multiparamétrique, définit le comportement du système par une variable continue ; l'introduction de la connaissance du domaine est alors nécessaire pour différencier des états. La combinaison des deux niveaux d'informations permet l'élaboration de scénarios. Le propos de ce document est d'exposer la méthode sur des signaux simulés présentant des modifications pouvant s'observer en clinique et d'introduire les perspectives offertes par son implémentation en ligne.
**** *annee_2005  *numText_932
Cet article présente une approche (ACKA an Approach for Cooperative Knowledge Acquisition) participative et coopérative d'acquisition de connaissances nécessaires pour la construction d'un modèle de simulation basé sur des agents. Elle est basée sur le principe de jeu de rôles dans une réunion d'entreprise. Nous proposons de construire un modèle multi-acteurs, représentant un modèle initial du système multi-agents. Dans cette étude, Nous appliquons ACKA pour construire un modèle multi-acteurs pour la compréhension des processus de décision dans les ?rmes de la ?liere avicole. En particulier, nous cherchons à comprendre les impacts des comportements individuels sur la gestion de l'utilisation des matières premières agricoles.
**** *annee_2005  *numText_933
Nous présentons une méthodologie d'extraction, de gestion et d'exploitation de connaissances dans un contexte multi-experts. Elle repose sur trois étapes : extraction des connaissances de chaque expert, gestion des connaissances individuelles afin de constituer une base de connaissances commune et exploitation de cette base afin de fournir une aide à la décision aux experts. La méthodologie proposée a été mise en œuvre au Cameroun avec cinq experts en micro-finance. Elle a donné des résultats en adéquation avec les pratiques des experts. Au-delà, on envisage de mettre en oeuvre un système de capitalisation des connaissances. Il doit permettre d'analyser rapidement un plus grand nombre de situations, les experts restant en nombre limité, et contribuer à un transfert de compétences pour former les décideurs locaux. En effet, les experts sont en général membres d'ONG et restent rarement plus de deux ans sur place.
**** *annee_2005  *numText_934
Devant la prolifération des données complexes qui ne cessent de croître, et la diversité des structures qui se multiplient, la conception des schémas de base de données en général et des schémas objet-relationnels en particulier, est devenue une activité difficile et complexe, qui fait appel à des connaissances variées. Lors de la conception d'un schéma, l'utilisateur (non averti) doit connaître la théorie sous-jacente au modèle de données, de façon à énoncer son modèle, syntaxiquement correct lui permettant de construire un schéma de base de données objet-relationnel répondant à ses besoins. Plusieurs outils spécialisés dans la conception de schémas de base de données provenant aussi bien de la communauté académique que du monde industriel, tels Super, Totem, Rational/Rose, etc. ont été développés dans des contextes et avec des buts souvent très différents. Affin de répondre à ce besoin pressant, nous avons proposé une solution consistant en l'élaboration d'environnements intégrés facilitant la cohabitation de plusieurs modèles et techniques utilisés lors de la conception d'un schéma de base de données. Il s'agit d'offrir une plate-forme logicielle appelée AID (Aided Interface for Database design) offrant des mécanismes opératoires uniformes représentant un soutien graphique et interactif pour une conception incrémentale basée sur des manipulations directes et systémiques des graphes au travers d'une palette graphique d'opérateurs. L'innovation d'AID est son approche systémique qui facilite l'expression des besoins par le concepteur averti ou non, en lui automatisant sa tâche.
**** *annee_2005  *numText_936
Notre travail s'appuie sur l'analyse d'un corpus bibliographique dans le domaine de la géotechnique à l'aide de cartes réalisées avec la plate-forme Stanalyst®. Celui-ci intègre un algorithme de classification automatique non hiérarchique (les K-means axiales) donnant des résultats dépendant du nombre de classes demandé. Cette instabilité rend difficile toute comparaison entre classifications, et laisse un doute quant au choix du nombre de classes nécessaire pour représenter correctement un domaine. Nous comparons les résultats de classifications selon 3 protocoles : (1) analyse des intitulés des classes ; (2) relations entre les classes à partir des membres communs ; (3) règles d'association floues. Les graphes obtenus présentant des similitudes remarquables, nous privilégions les règles d'association floues : elles sont extraites automatiquement et se basent sur la description des classes et non des membres. Ceci nous permet donc d'analyser des classifications issues de corpus différents.
**** *annee_2005  *numText_937
Dans cet article nous appliquons l'analyse de données symboliques au graphe de connaissances d'un agent. Nous présentons une mesure de similarité entre des données symboliques adaptée à nos graphes de connaissances. Nous utilisons les pyramides symboliques pour extraire un nouvel objet symbolique. Le nouvel objet est ensuite réinséré dans le graphe où il peut être utilisé par l'agent, faisant ainsi évoluer sa sémantique. Il peut alors servir d'individu lors des analyses ultérieures, permettant de découvrir de nouveaux concepts prenant en compte l'évolution de la sémantique.
**** *annee_2005  *numText_939
Cet article concerne la découverte de signatures (ou modèles de chroniques) à partir d'une séquence d'événements discrets (alarmes) générée par un agent cognitif de surveillance (Monitoring Cognitive Agent ou MCA).Considérant un couple (Processus, MCA) comme un générateur stochastique d'événements discrets, deux représentations complémentaires permettent de caractériser les propriétés stochastiques et temporelles d'un tel générateur : une chaîne de Markov à temps continu et une superposition de processus de Poisson. L'étude de ces deux représentations duales permet de découvrir des signatures décrivant les relations stochastiques et temporelles entre événements dans une séquence. Ces signatures peuvent alors être utilisées pour reconnaître des comportements spécifiques, comme le montre l'application de l'approche à un outil de production industriel piloté par un système Sachem, le MCA développé et utilisé par le groupe Arcelor pour aider au pilotage de ses outils de production.
**** *annee_2005  *numText_940
Nous présentons la méthode INSYSE (INterface SYntaxe SEmantique) pour l'annotation de documents textuels. Notre objectif est de construire des annotations sémantiques de ces résumés pour interroger le corpus sur la fonction des gènes et leurs relations de causalité avec certaines maladies. Notre approche est semi-automatique, centrée sur (1) l'extraction d'informations lexico-syntaxiques à partir de certaines phrases du corpus comportant des lexèmes de causation, et (2) l'élaboration de règles basées sur des grammaires d'unification permettant d'acquérir à partir de ces informations des schémas conceptuels instanciés. Ceux-ci sont traduits en annotations RDF(S) sur la base desquelles le corpus de textes peut être interrogé avec le moteur de recherche sémantique Corese.
**** *annee_2005  *numText_941
Pour reconnaitre les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement a partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et sont transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets a partir de ces graphes, en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples a ce modèle. Nous sommes parvenus a obtenir des modèles de ponts et de ronds-points qui sont compatibles avec les modèles construits manuellement.
**** *annee_2005  *numText_943
L'inférence de signatures de facteurs de transcription à partir des données puces à ADN a déjà été étudié dans la communauté bioinformatique. La principale difficulté à résoudre est de trouver un ensemble d'heuristiques pertinentes, afin de contrôler la complexité de résolution de ce problème NP-difficile. Nous proposons dans cet article une solution heuristique alternative à celles utilisées dans les approches bayésiennes, fondée sur la recherche de motifs fréquents maximaux dans une matrice discrétisée issue des données numériques de puces ADN. Notre méthode est appliquée sur des données de cancer de vessie de l'Institut Curie et de l'Hôpital Henri Mondor de Créteil.
**** *annee_2005  *numText_944
Le formalisme des modèles graphiques connait actuellement un essor dans les domaines du machine learning. En particulier, les réseaux bayésiens sont capables d'effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables d'utiliser les bases d'exemples incomplètes pour leur apprentissage. En s'inspirant du principe de AMS-EM proposé par (Friedman, 1997) et des travaux de(Chow & Liu, 1968), nous proposons une méthode permettant de faire l'apprentissage de réseaux bayésiens particuliers, de structure arborescente, à partir de données incomplètes. Une étude expérimentale expose ensuite des résultats préliminaires qu'il est possible d'attendre d'une telle méthode, puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation d'une méthode de recherche gloutonne comme AMS-EM.
**** *annee_2005  *numText_945
L'utilisation d'un algorithme d'apprentissage non supervisé de type k-Means sur un jeu de séries temporelles amène à se poser deux questions : Celle du choix d'une mesure de similarité et celle du choix d'une méthode effectuant l'agrégation de plusieurs séries afin d'en estimer le centre (i.e. calculer les k moyennes). Afin de répondre à la première question, nous présentons dans cet article les principales mesures de similarité existantes puis nous expliquons pourquoi l'une d'entre elles (appelée Dynamic Time Warping) nous paraît la plus adaptée à l'apprentissage non supervisé. La deuxième question pose alors problème car nous avons besoin d'une méthode d'agrégation respectant les caractéristiques bien particulières du Dynamic Time Warping. Nous pensons que l'association de cette mesure de similarité avec l'agrégation Euclidienne peut générer une perte d'informations importante dans le cadre d'un apprentissage sur la forme des séries. Nous proposons donc une méthode originale d'agrégation de séries temporelles, compatible avec le Dynamic Time Warping, qui améliore ainsi les résultats obtenus à l'aide de l'algorithme des k-Means.
**** *annee_2005  *numText_946
Un problème important de la production automatique de règles de classification concerne la durée de génération de ces règles ; en effet, les algorithmes mis en œuvre produisent souvent des règles pendant un certain temps assez long. Nous proposons une nouvelle méthode de classification à partir d'une base de données images. Cette méthode se situe à la jonction de deux techniques : l'algèbre de P-tree et l'arbre de décision en vue d'accélérer le processus de classification et de recherche dans de grandes bases d'images. La modélisation que nous proposons se base, d'une part, sur les descripteurs visuels tels que la couleur, la forme et la texture dans le but d'indexer les images et, d'autre part, sur la génération automatique des règles de classification à l'aide d'un nouvel algorithme C4.5(P-tree). Pour valider notre méthode, nous avons développé un système baptisé C.I.A.D.P-tree qui a été implémenté et confronté à une application réelle dans le domaine du traitement d'images. Les résultats expérimentaux montrent que cette méthode réduit efficacement le temps de classification.
**** *annee_2005  *numText_947
Le critère de découpage binaire de Kolmogorov-Smirnov nécessite un ordre total des valeurs prises par les variables explicatives. Nous pouvons ordonner des intervalles fermés bornés de nombres réels de différentes façons. Notre contribution dans cet article consiste à évaluer et à comparer des arbres de décision obtenus sur des données de type intervalle à l'aide du critère de découpage binaire de Kolmogorov-Smirnov étendu à ce type de données (Mballo et al. 2004). Pour ce faire, nous axons notre attention sur le taux d'erreur mesuré sur l'échantillon de test. Pour estimer ce paramètre, nous divisons aléatoirement chaque base de données en deux parties égales en terme d'effectif (à un objet près) pour construire deux arbres. Ces deux arbres sont d'abord testés par un même échantillon puis par deux échantillons différents.
**** *annee_2005  *numText_948
Une image est un support d'information qui a montré son efficacité. Néanmoins une image comporte souvent plusieurs zones, l'arrière plan et une zone d'intérêt privilégiée. La vision humaine permet la segmentation de manière naturelle et intégrant toute la connaissance que le sujet peut avoir de l'objectif visé par l'image. Nous proposons ici une méthode de détermination des régions d'intérêt d'une image numérique comme zones saillantes. Les lois de Zipf et Zipf inverse sont adaptées au traitement des images et permettent d'évaluer la complexité structurelle d'une image. Une comparaison des modèles locaux évalués sur des imagettes permet de mettre en évidence une région de l'image. Deux méthodes de classification ont été utilisées pour la détermination de la région d'intérêt : la partition d'un nuage de points représentant les caractéristiques associées aux imagettes, et les réseaux de neurones. Cette méthode de détection permet d'obtenir des zones d'intérêt conformes à la perception humaine. On opère une hiérarchisation sur les zones en fonction de la structuration de l'information élémentaire, les pixels.
**** *annee_2005  *numText_949
Cet article a pour but de montrer les possibilités offertes par le logiciel CHIC (Classification Hiérarchique Implicative et Cohésitive) pour effectuer certaines analyses de données. Il est basé sur la théorie de l'Analyse Statistique Implicative ou A.S.I. développée par Régis Gras et ses collaborateurs. Le principe premier de l'A.S.I. repose sur la problématique d'une mesure des règles d'association du type : «si a alors b» dans une population instanciant les variables a et b. CHIC enrichit sa réponse, établie sur des bases statistiques, en évaluant la responsabilité des sujets dans l'élection de la règle. L'article présent explique la démarche à suivre pour utiliser le logiciel ainsi que les possibilités offertes par celui-ci.
**** *annee_2005  *numText_951
Les modèles de mélange, qui supposent que l'échantillon est formé de sous-populations caractérisées par une distribution de probabilité, constitue un support théorique intéressant pour étudier la classification automatique. On peut ainsi montrer que l'algorithme des k-means peut être vu comme une version classifiante de l'algorithme d'estimation EM dans un cas particulièrement simple de mélange de lois normales. Lorsque l'on cherche à classifier les lignes (ou les colonnes) d'un tableau de contingence, il est possible d'utiliser une variante de l'algorithme des k-means, appelé Mndki2, en s'appuyant sur la notion de profil et sur la distance du khi-2. On obtient ainsi une méthode simple et efficace pouvant s'utiliser conjointement à l'analyse factorielle des correspondances qui s'appuie sur la même représentation des données. Malheureusement et contrairement à l'algorithme des k-means classique, les liens qui existent entre les modèles de mélange et la classification ne s'appliquent pas directement à cette situation. Dans ce travail, nous montrons que l'algorithme Mndki2 peut être associé, à une approximation près, à un modèle de mélange de lois multinomiales.
**** *annee_2005  *numText_952
Le nombre de documents issus d'une requête sur le Web devient de plus en plus important. Cela nous amène à chercher des solutions pour aider l'utilisateur qui est confronté à cette masse de données. Une alternative possible à un affichage linéaire non triée selon un critère, consiste à effectuer une classification des résultats. C'est dans ce but que l'on s'intéresse aux cartes auto-organisatrices de Kohonen qui sont issues d'un d'algorithme de classification non supervisée. Cependant, il faut ajouter des contraintes à cet algorithme afin qu'il soit adapté à la classification des résultats d'une requête. Par exemple, il doit être déterministe. De plus la classification obtenue dépend fortement de la distance utilisée pour comparer deux documents. On évalue alors l'impact de différentes distances ou dissimilarités, afin de trouver la plus adaptée à notre problème. Un compromis doit également être trouvé entre le temps d'exécution de l'algorithme et la qualité de la classification obtenue. Pour cela, l'utilisation d'un échantillonnage est envisagée. Enfin, ces travaux sont intégrés dans un prototype qui permet de visualiser les résultats en trois dimensions et d'interagir avec eux.
**** *annee_2005  *numText_953
Ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web. C'est ainsi que les systèmes d'intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme. Pour faciliter l'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.
**** *annee_2005  *numText_954
Nous proposons une méthode d'apprentissage automatique pour la sélection de passages susceptibles de contenir la réponse à une question dans les systèmes de Question-Réponse (QR). Les systèmes de RI ad hoc ne sont pas adaptés à cette tâche car les passages recherchés ne doivent pas uniquement traiter du même sujet que la question mais en plus contenir sa réponse. Pour traiter ce problème les systèmes actuels ré-ordonnent les passages renvoyés par un moteur de recherche en considérant des critères sous forme d'une somme pondérée de fonctions de scores. Nous proposons d'apprendre automatiquement les poids de cette combinaison, grâce à un algorithme de réordonnancement défini dans le cadre du Boosting, qui sont habituellement déterminés manuellement. En plus du cadre d'apprentissage proposé, l'originalité de notre approche réside dans la définition des fonctions allouant des scores de pertinence aux passages. Nous validons notre travail sur la base de questions et de réponses de l'évaluation TREC-11 des systèmes de QR. Les résultats obtenus montrent une amélioration significative des performances en terme de rappel et de précision par rapport à un moteur de recherche standard et à une méthode d'apprentissage issue du cadre de la classification.
**** *annee_2005  *numText_956
Le but de ce travail est de faciliter l'interprétation d'une classification pyramidale construite sur un tableau de données symboliques. Alors que dans une hiérarchie binaire le nombre de paliers est égal à n-1, si n est le nombre d'individus à classer, dans le cas d'une pyramide ce dernier peut atteindre n(n-1)/2. Afin de réduire ce nombre, on élague la pyramide et on utilise un critère de sélection de paliers basé sur la hauteur. De plus on décrit tous les paliers retenus par des variables que l'on sélectionne également en utilisant le degré de généralité ainsi que des mesures de dissimilarités de type symbolique-numérique. L'aide à l'interprétation se sert d'outils graphiques et interactifs grâce à la bibliothèque OpenGL. Enfin une simulation montre comment évoluent ces sélections quand le nombre de classes et de variables croit.
**** *annee_2005  *numText_957
Ce travail a pour objectif la construction automatique d'un entrepôt thématique de données, à partir de documents de format divers provenant du Web. L'exploitation de cet entrepôt est assurée par un moteur d'interrogation fondé sur une ontologie. Notre attention porte plus précisément sur les tableaux extraits de ces documents et convertis au format XML, aux tags exclusivement syntaxiques. Cet article présente la transformation de ces tableaux, sous forme XML, en un formalisme enrichi sémantiquement dont la plupart des tags et des valeurs sont des termes construits à partir de l'ontologie.
**** *annee_2005  *numText_959
Très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange, l'algorithme EM présente l'inconvénient majeur de converger parfois lentement. Son application sur des tableaux de grande taille devient ainsi irréalisable. Afin de remédier à ce problème, plusieurs méthodes ont été proposées. Nous présentons ici le comportement d'une méthode connue, LEM, et d'une variante que nous avons proposée récemment eLEM. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification, et nous illustrons le bon comportement de notre variante sur des données continues simulées et réelles.
**** *annee_2005  *numText_960
Cet article présente différentes expériences de classification de documents XML de structure homogène, en vue d'expliquer et de valider une présentation organisationnelle pré-existante. Le problème concerne le choix des éléments et mots utilisés pour la classification et son impact sur la typologie induite. Pour cela nous combinons une sélection structurelle basée sur la nature des éléments XML et une sélection linguistique basée sur un typage syntaxique des mots. Nous illustrons ces principes sur la collection des rapports d'activité 2003 des équipes de recherche de l'Inria en cherchant des groupements d'équipes (Thèmes) à partir du contenu de différentes parties de ces rapports. Nous comparons nos premiers résultats avec les thèmes de recherche officiels de l'Inria.
**** *annee_2005  *numText_961
La fonction de correspondance, qui permet de sélectionner et de classer les documents par rapport à une requête est un composant essentiel dans tout système de recherche d'information. Nous proposons de modéliser une fonction de correspondance prenant en compte à la fois le contenu et les liens hypertextes des pages Web. Nous avons expérimenté notre système sur la collection de test TREC-9, et nous concluons que pour certains types de requêtes, inclure le texte ancre associé aux liens hypertextes des pages dans la fonction de similarité s'avère plus efficace.
**** *annee_2005  *numText_962
Nous traitons l'extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles. La méthode proposée nous permet de découvrir des règles d'association au niveau des concepts. Cette extension implique notamment de nouvelles définitions pour le support et la confiance afin d'exploiter la structure symbolique des données. Au fil de l'article l'exemple classique du panier de la ménagère est développé. Ainsi, plutôt que d'extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique, nous extrayons des règles d'association au niveau des clients afin d'étudier leurs comportements d'achat.
**** *annee_2005  *numText_963
Les bases de données inductives intègrent le processus de fouille de données dans une base de données qui contient à la fois les données et les connaissances induites. Nous nous proposons d'étendre les données traitées afin de permettre l'extraction de motifs temporels fréquents et non fréquents à partir d'un ensemble de séquences d’événements. Les motifs temporels visés sont des chroniques qui permettent d'exprimer des contraintes numériques sur les délais entre les occurrences d’événements.
**** *annee_2005  *numText_964
Cet article décrit la génération automatique et l'utilisation d'une base de patterns pour le go 19x19. La représentation utilisée est celle des K plus proches voisins. Les patterns sont engendrés en parcourant des parties de professionnels. Les probabilités d'appariement et de jeu des patterns sont également estimées à ce moment là. La base créée est intégrée dans un programme existant, Indigo. Soit elle est utilisée comme un livre d'ouvertures en début de partie, soit comme une extension des bases pré-existantes du générateur de coups du programme. En terme de niveau de jeu, le gain résultant est estimé à 15 points en moyenne.
**** *annee_2005  *numText_965
Le Catalogue et Index des Sites Médicaux Francophones (CISMeF) recense les principales ressources institutionnelles de santé en français. La description de ces ressources, puis leur accès par les utilisateurs, se fait grâce à la terminologie CISMeF, fondée sur le thésaurus américain Medical Subject Headings (MeSH). La version française du MeSH comprend tous les descripteurs MeSH, mais de nombreux synonymes américains restent à traduire. Afin d'enrichir la terminologie, nous proposons ici une méthode de traduction automatique de ces synonymes. Pour ce faire, nous avons constitué deux corpus parallèles anglais/français du domaine médical. Après alignement semi-automatique des corpus paragraphe à paragraphe, nous avons procédé automatiquement à l'appariement bilingue des termes. Pour cela, le lexique constitué des descripteurs MeSH américains et de leur traduction en français a fourni les couples amorces qui ont servi de point de départ à la propagation syntaxique des liens d'appariement. 217 synonymes ont pu être traduits, avec une précision de 70%.
**** *annee_2005  *numText_966
Trouver et classer les documents pertinents par rapport à une requête est fondamental dans le domaine de la recherche d'information. Notre étude repose sur la localisation des termes dans les documents. Nous posons l'hypothèse que plus les occurrences des termes d'une requête se retrouvent proches dans un document alors plus ce dernier doit être positionné en tête de la liste de réponses. Nous présentons deux variantes de notre modèle à zone d'influence, la première est basée sur une notion de proximité floue et la seconde sur une notion de pertinence locale.
**** *annee_2005  *numText_967
L'extraction de règles d'association est devenue aujourd'hui une tâche populaire en fouille de données. Cependant, l'algorithme Apriori et ses variantes restent dédiés aux bases de données renfermant des informations catégoriques.Nous proposons dans cet article QuantMiner, qui est un outil que nous avons développé dans le but d'extraire des règles d'association gérant variables catégoriques et numériques. L'outil que nous proposons repose sur un algorithme génétique permettant de découvrir de façon dynamique les intervalles des variables numériques apparaissant dans les règles.Nous présentons également une application réelle de notre outil sur des données médicales relatives à la maladie de l'athérosclérose et donnons des résultats de notre expérience pour la description et la caractérisation de cette maladie.
**** *annee_2005  *numText_968
Nous développons un logiciel, Exit, capable d'aider un expert à extraire des termes qu'il trouve pertinents dans des textes de spécialité. Tout est mis en place pour faciliter le travail de l'expert afin qu'il puisse consacrer son temps à la seule reconnaissance des termes pertinents. Pour cela, différentes mesures statistiques et de nombreuses options d'extraction sont disponibles dans Exit. Afin d'utiliser au mieux les connaissances de l'expert, notre approche est semi-automatique. De plus, l'expert construit des termes pouvant inclure des termes précédemment extraits ce qui rend itératif et constructif notre processus de formation des termes. Enfin, l'ergonomie du logiciel a profité des enseignements tirés lors de son utilisation pour une compétition internationale d'extraction de connaissances.
**** *annee_2005  *numText_971
Pour nous attaquer au problème du forage de très grandes bases de données distribuées, nous proposons d'étudier deux approches. La première est de télécharger seulement un échantillon de chaque base de données puis d'y effectuer le forage. La deuxième approche est de miner à distance chaque base de données indépendamment, puis de télécharger les modèles résultants, sous forme de règles de classification, dans un site central où l'agrégation de ces derniers est réalisée. Dans cet article, nous présentons une vue d'ensemble des techniques d'échantillonnage les plus communes. Nous présentons ensuite cette nouvelle technique de forage distribué des données où la mécanique d'agrégation est basée sur un coefficient de confiance attribué à chaque règle et sur de très petits échantillons de chaque base de données. Le coefficient de confiance d'une règle est calculé par des moyens statistiques en utilisant le théorème limite centrale. En conclusion, nous présentons une comparaison entre les meilleures techniques d'échantillonnage que nous avons trouvées dans la littérature, et notre approche de forage distribué des données (FDD) basée sur l'agrégation de modèles.
**** *annee_2005  *numText_973
Cet article présente une méthode permettant la découverte non supervisée de motifs fréquents représentatifs de symboles sur des images de documents. Les symboles sont considérés comme des entités graphiques porteurs d'information et les images de document sont représentées par des graphes relationnels attribués. Dans un premier temps, la méthode réalise la découverte de sous-graphes disjoints fréquents et fait correspondre pour chacun d'eux un symbole différent. Une recherche des règles d'association entre ces symboles permet alors d'accéder à une partie des connaissances du domaine décrit par ces symboles. L'objectif à terme est d'utiliser les symboles découverts pour la classification ou la recherche d'images dans un flux hétérogène de document là ou une approche supervisée n'est pas envisageable.
**** *annee_2005  *numText_974
La finalité de ce papier est d'analyser l'apport de techniques de fouille de données textuelles à une méthodologie de construction d'ontologie à partir de textes. Le domaine d'application de cette expérimentation est celui de l'accidentologie routière. Dans ce contexte, les résultats des techniques de fouille de données textuelles sont utilisés pour orienter la construction d'une ressource terminologique à partir de procès-verbaux d'accidents. La méthode TERMINAE et l'outil du même nom offrent le cadre général pour la modélisation de la ressource. Le papier présente les techniques de fouille employées et l'intégration des résultats des fouilles dans les différentes étapes du processus de construction de la ressource.
**** *annee_2005  *numText_975
L'extraction de règles d'association est souvent exploitée comme méthode de fouille de données. Cependant, une des limites de cette approche vient du très grand nombre de règles extraites et de la difficulté pour l'analyste à appréhender la totalité de ces règles. Nous proposons donc de pallier ce problème en structurant l'ensemble des règles d'association en hiérarchies. La structuration des règles se fait à deux niveaux. Un niveau global qui a pour objectif de construire une hiérarchie structurant les règles extraites des données. Nous définissons donc un premier type de subsomption entre règles issue de la subsomption dans les treillis de Galois. Le second niveau correspond à une analyse locale des règles et génère pour une règle donnée une hiérarchie de généralisation de cette règle qui repose sur des connaissances complémentaires exprimées dans un modèle terminologique. Ce niveau fait appel à un second type de subsomption inspiré de la subsomption en programmation logique inductive. Nous définissons ces deux types de subsomptions, développons un exemple montrant l'intérêt de l'approche pour l'analyste et étudions les propriétés formelles des hiérarchies ainsi proposées.
**** *annee_2005  *numText_976
Nous présentons dans cet article une nouvelle approche de fouille qui permet d'appliquer des algorithmes de construction d'arbres de décision en répondant à deux objectifs : (1) traiter des bases volumineuses, (2) en des temps de traitement acceptables. Le premier objectif est atteint en intégrant ces algorithmes au cœur des SGBD, en utilisant uniquement les outils fournis par ces derniers. Toutefois, les temps de traitement demeurent longs, en raison des nombreuses lectures de la base. Nous montrons que, grâce aux index bitmap, nous réduisons à la fois la taille de la base d'apprentissage et les temps de traitements. Pour valider notre approche, nous avons implémenté la méthode ID3 sous forme d'une procédure stockée dans le SGBD Oracle.
**** *annee_2005  *numText_977
Il n'est plus à rappeler que le corpus textuel, est tel qu'il est actuellement, intraitable à l'échelle que sa croissance nous confirme l'obligation d'utiliser des outils automatique de traitement. Cet article s'intéresse plus particulièrement à la caractérisation de textes et par là même à celle d'auteurs. A l'heure actuelle, toutes les méthodes existant travaillent sur le document fini, sans admettre qu'un cheminement existe entre le début du document et sa fin. Nous proposons une méthode tentant d'apporter cette notion d'évolution textuelle en traitant le texte par un automate et l'évaluation choisie. Puis nous présenterons des résultats validés par des experts, obtenus sur un corpus d'entretiens sociologiques.
**** *annee_2005  *numText_978
La gestion des ressources humaines repose d'une part sur la connaissance des individus et de leurs compétences et d'autre part sur la connaissance de l'organisation et de ses métiers. C'est par la mise en correspondance de ces connaissances qu'il est possible d'améliorer l'emploi, de valoriser les connaissances et les compétences individuelles et de mieux gérer l'organisation. Cette mise en correspondance nécessite une représentation explicite des connaissances, ce qui permet de répondre à de nouveaux besoins : annuaire de compétences, gestion des projets et des retours d'expériences, identification des connaissances à risques, etc.Nous verrons dans le cadre de cet article l'intérêt de l'approche ontologique tant d'un point de vue méthodologique pour la clarification des notions mises en jeu dans le cadre de la GPECC (Gestion Prévisionnelle des Emplois des Compétences et des Connaissances) que pour la construction, la représentation et la maintenance des référentiels des compétences, des connaissances et des métiers. Elle permet en particulier une gestion de l'information par la terminologie et le sens métier propre à l'organisation.
**** *annee_2005  *numText_981
La compréhension de textes de spécialité nécessite un étiquetage morpho-syntaxique de bonne qualité. Or, lorsque les textes étudiés sont issus de domaines spécifiques et peu usités, il est rare de disposer de dictionnaires et autres ressources lexicales fiables. Le logiciel que nous proposons permet d'utiliser un étiquetage réalisé par un étiqueteur généraliste, puis d'améliorer cet étiquetage en intégrant des connaissances d'experts du domaine étudié. Grâce au logiciel développé, il est relativement aisé pour un expert du domaine de détecter des erreurs d'étiquetage et de mettre en place des règles de ré-étiquetage. Ces règles peuvent être obtenues de deux manières différentes : (1) soit en utilisant un langage de programmation permettant d'exprimer des règles complexes de ré-étiquetage, (2) soit par apprentissage automatique des règles à partir d'exemples corrigés au moyen d'une interface dédiée. Cet apprentissage propose de nouvelles règles à l'expert, acquises automatiquement.
**** *annee_2005  *numText_982
Cet article définit une algèbre permettant de manipuler des tables dimensionnelles extraites d'une base de données multidimensionnelles. L'algèbre intègre un noyau minimum d'opérateurs unaires permettant d'effectuer les analyses décisionnelles par combinaison d'opérateurs. Cette algèbre intègre un opérateur binaire permettant la fusion de tables dimensionnelles facilitant les corrélations des sujets analysés.
**** *annee_2005  *numText_983
L'approche présentée dans cet article a pour objectif la construction d'une ontologie à partir du treillis de l'iceberg de Galois. Nous entendons par ontologie un ensemble de termes structurés entre eux par un ensemble de liens de divers types. Dans notre cas d'étude, cette ontologie constitue un support de connaissances documentaires. En effet, elle peut être utilisée dans diverses applications en Recherche d'Information (RI), telles que l'indexation automatique et l'expansion de requêtes ainsi qu'en text-mining. La méthode de construction que nous proposons est fondée sur l'analyse formelle de concepts (AFC) et plus précisément, la structure du treillis de l'iceberg de Galois. En utilisant cette structure hiérarchique partiellement ordonnée, nous présentons une translation directe des relations laticielles vers celles ontologiques. Nous proposons ainsi d'enrichir l'ontologie dérivée par des règles associatives génériques entre termes, découvertes dans le cadre d'un processus de textmining.
**** *annee_2005  *numText_985
L'extraction de toutes les requêtes fréquentes dans une base de données relationnelle est un problème difficile, même si l'on ne considère que des requêtes conjonctives. Nous montrons que ce problème devient possible dans le cas suivant : le schéma de la base est un schéma en étoile, et les données satisfont un ensemble de dépendances fonctionnelles et de contraintes référentielles. De plus, les schémas en étoile sont appropriés pour les entrepôts de données et que les dépendances fonctionnelles et les contraintes référentielles sont les contraintes les plus usuelles dans les bases de données. En considérant le modèle des instances faibles, nous montrons que les requêtes fréquentes exprimées par sélection-projection peuvent être extraites par des algorithmes de type Apriori.
**** *annee_2005  *numText_986
Travaillant sur l'élaboration d'une méthodologie de développement de systèmes de médiation intégrés dans des systèmes coopératifs, nous avons proposé une architecture à 3 composants : le premier concerne la coopération, le second l'assistance et le troisième est relatif aux connaissances nécessaires aux 2 précédents. Dans cet article nous présentons plus particulièrement le point de vue des connaissances. Ces connaissances sont de 2 natures : des connaissances statiques, sur le domaine par exemple, et des connaissances acquises pendant l'utilisation coopérative du système, notamment la mémoire des activités et les descriptions des actes de résolutions de problèmes. Pour illustrer cette modélisation de connaissances, nous nous intéresserons aux activités coopératives de suivi, de gestion et d'évaluation de projets d'étudiants, assistées par l'outil iPédagogique.
**** *annee_2005  *numText_988
L'objectif de ce papier est de présenter une contribution à la modélisation des individus et de leurs relations pour permettre l'aide à l'intégration des acteurs dans une organisation. Nous étudions en particulier le cas du remplacement d'un acteur (« turn-over »). Dans ce cadre, nous proposons un modèle regroupant un ensemble de données relatives à un individu, aux relations que celui-ci entretient avec les autres acteurs et à son espace informationnel. L'étude porte sur la mise en œuvre de mécanismes d'aide fournissant à un acteur les moyens de son intégration : la mise à disposition d'une image des espaces informationnels et relationnels de son prédécesseur ainsi que la mise en relation de l'acteur avec les autres acteurs de l'organisation. Cette étude est menée en partenariat avec des experts en GRH.
**** *annee_2005  *numText_989
Pour faciliter l'étude de certains phénomènes, des outils de simulation ont été créés dans de nombreux domaines. L'étude du comportement humain à jusque là échappé à cette tendance. Aujourd'hui, les systèmes multi-agents couplés aux avancées des sciences humaines fournissent les bases nécessaires à l'élaboration de ce type d'outil. Cet article s'inscrit ainsi dans cette dynamique avec l'objectif de développer un outil de simulation du comportement d'individus traumatisés crâniens sur une chaîne de production. Cet outil doit permettre la collecte de la connaissance relative au système étudié et fournir une aide à la décision pour les responsables de l'entreprise. Cet article propose une modélisation des interactions entre individus dans le formalisme AgentUML. Une implémentation du modèle au sein d'un outil de simulation fonctionnel et les résultats obtenus seront également présentés. A terme, le but est la production de données de simulation exploitables par des techniques d'ECD.
**** *annee_2005  *numText_990
La gestion d'objets mobiles a connu un regain d'intérêt ces dernières années, particulièrement dans le but de gérer et de prédire la localisation d'objets mobiles. Cependant, il y a peu de recherches sur l'exploitation d'historiques de bases d'objets mobiles. La première étape dans ce processus est la mise en oeuvre d'un entrepôt d'objets mobiles. Seulement, les modèles d'entrepôts existants ne permettent pas de traiter directement ce type de données complexes. Cet article présente une approche originale pour pallier ce problème. Cette approche offre la puissance de l'algèbre OLAP sur toute combinaison de données classiques, spatiales et/ou temporelles et mobiles. Elle a été validée par un prototype et appliquée à l'analyse de la mobilité urbaine. Les résultats de l'expérimentation montrent la validité de l'approche et les tests de performances son efficacité.
**** *annee_2005  *numText_991
Pouvoir extraire de la connaissance à partir d'une plate-forme de simulation est aujourd'hui envisageable en conjuguant les avancées obtenues en Intelligence Artificielle autour des systèmes multi-agents et les méthodes de formalisation et d'extraction des connaissances. C'est donc dans un cadre général de gestion des connaissances que nous proposons de modéliser un agent artificiel doté de connaissances et d'émotions. Pour cela, une expertise psychologique a été recueillie et formalisée de manière à être stockée dans une base de connaissances sous forme de règles et de classes en UML et RDF. L'implémentation du modèle permet d'entrevoir les perspectives d'une telle simulation : enrichissement par des données issues de simulations, découverte de nouvelles connaissances par l'application de processus d'ECD.
**** *annee_2005  *numText_992
La plupart des bases de données issues du monde réel sont constituées de données numériques et historiées (données de capteurs, données scientifiques, données démographiques). Dans ce cadre les algorithmes d'extraction de motifs séquentiels, s'ils sont adaptés au caractère temporel des données ne permettent pas le traitement de données numériques. es données sont alors pré-traitées pour les transformer en données binaire, ce qui entraîne une perte d'information. Des algorithmes ont donc été proposés pour traiter les données numériques sous forme d'intervalles et d'intervalles flous notamment. En ce qui concerne la recherche de motifs séquentiels fondée sur des intervalles flous, les deux méthodes de la littérature ne sont pas satisfaisantes car incomplètes soit dans le traitement des séquences soit dans le calcul du support. Dans cet article, nous proposons donc trois méthodes d'extraction de motifs séquentiels flous {SPEEDYFUZZY, MINIFUZZY et TOTALLYFUZZY) et en détaillons les algorithmes sous-jacents en soulignant les différents niveaux de fuzzification. Ces algorithmes sont implémentés et évalués à travers différentes expérimentations menées sur des jeux de tests synthétiques.
**** *annee_2005  *numText_993
La notion de règles entre attributs est très générale, allant des règles d'association en fouille de données aux dépendances fonctionnelles (DF) en bases de données. Malgré cette diversité, la syntaxe des règles est toujours la même, seule leur sémantique diffère. Pour une sémantique donnée, en fonction des propriétés induites, des techniques algorithmiques sont mises en oeuvre pour découvrir les règles à partir des données. A partir d'un ensemble de règles, il est aussi utile en pratique de raisonner sur ces règles, comme cela est le cas par exemple avec les axiomes d'Armstrong pour les dépendances fonctionnelles. Dans cet article, nous proposons un cadre qui permet de s'assurer qu'une sémantique donnée pour les règles est bien-formée, i.e. les axiomes d'Armstrong sont justes et complets pour cette sémantique. Les propositions faites dans ce papier proviennent du contexte applicatif de l'analyse de données de biopuces. A partir de plusieurs sémantiques pour les données d'expression de gènes, nous montrons comment ces sémantiques s'intègrent dans le cadre présenté.
**** *annee_2005  *numText_994
Nous avons conçu un outil de classification de données original que nous détaillons dans le présent article. Cet outil comporte un module de création de résumés et un module d'affichage. Le module de visualisation permet une lecture aisée des résumés grâce à une interface graphique évoluée permettant la présentation et l'exploration des résumés sous forme d'une hiérarchie de profils ou d'un tableau de profils. Chaque profil donne de manière claire les informations relatives au résumé de données correspondant. La lecture de la hiérarchie et du tableau est aussi grandement facilitée par les choix d'un ordre optimal pour la présentation des variables et des résumés.
**** *annee_2005  *numText_995
Dans cet article on propose une nouvelle approche qui rend explicite la notion de point de vue dans une analyse multi-vues issue d'un processus d'Extraction de Connaissances à partir de Données (ECD). Par point de vue, nous entendons la vision particulière d'un analyste lors de son processus ECD, vision référant à un corps de connaissances qui lui est spécifique. On cherche, d'une part, à faciliter la réutilisabilité et l'adaptabilité du processus, et d'autre part à garder une trace des points de vues sous-jacents aux analyses faites. Le processus d'ECD sera vu comme un processus de génération et de transformation de vues qui seront annotées par des métadonnées pour garder la sémantique de la connaissance extraite. Un positionnement de notre approche vis-à-vis des travaux méthodologiques du processus d'ECD sera donné. Des éléments de modélisation du processus ECD basé sur les points de vue seront décrits au niveau ontologique. Enfin, on illustrera notre approche sur l'analyse des usages d'un site web à partir des fichiers log, selon le point de vue fiabilité.
**** *annee_2005  *numText_996
Le développement des réseaux à haut-débit et de l'Internet fournit un nouveau support à l'enseignement à distance. Aujourd'hui, de nombreux acteurs dans le domaine de l'enseignement ont mis en place des dispositifs de formation en ligne. Ceux-ci se composent généralement d'une sélection de matériaux organisés et présentés de manière à suivre un programme pédagogique particulier, de mécanismes de communication entre apprenants et enseignants, et d'outils de suivi des apprenants. Les plates-formes d'enseignement à distance devenant de plus en plus génériques, des nouveaux modèles ont été définis, standardisés ou normalisés, permettant la formalisation de méta-données pédagogiques ou tentant d'évaluer les connaissances acquises par les apprenants. En nous appuyant sur ces modèles, nous proposons de construire une base de connaissances, associant notamment les termes des domaines enseignés en relations à sémantique pédagogique. L'exploitation de cette base de connaissances fournit un premier niveau d'aide à l'ingénierie pédagogique, en particulier lorsque le volume de contenus en ligne est important. Des inférences mettant en jeu ces connaissances permettent alors un meilleur suivi du dispositif d'enseignement.
**** *annee_2005  *numText_998
Nous nous intéressons au raisonnement sur les compétences des ressources humaines pour simplifier leur gestion. Dans cet article, nous proposons une méthode de raisonnement pour l'aide à l'identification des compétences d'un individu. Un processus de knowledge-mining défini par analogie avec l'extraction de règles d'association en datamining est proposé afin d'induire une base de règles à partir d'une base de connaissances sur le domaine. De plus, un prototype a été développé pour expérimenter notre approche sur un exemple académique.
**** *annee_2005  *numText_1000
Nous proposons dans cet article une stratégie de réécriture de requêtes sur des données multimédias décrites moyennant le standard MPEG-7. Ce standard se base sur XML schéma qui permet de décrire la structure des données. Cependant, aucune sémantique n'est assignée à cette structure. Nous proposons d'étendre ce standard d'une ontologie permettant d'exprimer les connaissances du domaine. Ainsi, l'ontologie sera utilisée durant l'indexation des données multimédias et la réécriture de requêtes. Le but de la réécriture de requêtes est de transformer une requête initiale en une ou plusieurs requêtes équivalentes ou sémantiquement proches compte tenu des connaissances représentées dans l'ontologie.
**** *annee_2005  *numText_1001
L'annotation se distingue de l'indexation automatique par l'utilisation d'une ou plusieurs ontologies qui définissent un domaine global de référence permettant de cadrer et de normaliser les annotations effectuées, par ailleurs une ressource annotée doit l'être non pas par une liste de mots clefs, mais bien par une ou plusieurs ontologies. Malheureusement, il est peu réaliste de penser que les centaines de millions de ressources mises à disposition sur le Web puissent être annotées par leurs auteurs. Pour résoudre ce problème, notre démarche consiste à indexer les documents en se basant sur l'ontologie globale et ensuite propager les annotations en utilisant des documents déjà annotés pour annoter d'autres documents référencés par ceux-ci. La propagation des annotations suit des règles que nous proposons dans cet article. L'illustration est effectuée sur un corpus de livres dont le thème relève de l'informatique.
**** *annee_2005  *numText_1002
Le développement du Web Sémantique a conduit à l'élaboration de standards pour la représentation des connaissances sur le Web. RDF, comme un de ces standards, est devenu une recommandation du W3C. Même s'il a été conçu pour être interprétable par l'homme et la machine (encodage XML, triplets, graphes étiquetés), RDF n'a pas été fourni avec des services d'interrogation et de raisonnement. La plupart des travaux concernant l'interrogation de documents RDF se sont concentrés sur l'usage de techniques issues de la programmation logique et sur des extensions de SQL. Nous portons un nouveau regard sur les techniques d'interrogation et de raisonnement sur les documents RDF et nous montrons que la sémantique des termes OSF (Order Sorted Features) est compatible avec la représentation isomorphique (triplets) des propositions RDF. Cette transformation permet l'ordonnancement des ressources en ontologies et, à travers ceci, des meilleurs mécanismes de réponses (par approximation et recouvrement) aux interrogations de documents RDF.
**** *annee_2005  *numText_1003
L'interrogation de grandes bases de documents semi-structurés (type XML) est un problème ouvert important. En effet, pour interroger un document dont le schéma est nouveau, un système doit pouvoir soit adapter la requête posée au document, soit adapter le document pour pouvoir lui appliquer la requête. Nous nous positionnons ici dans le cadre de la restructuration de documents qui consiste à transformer des documents semi-structurés issus de diverses sources dans un schéma de médiation connu. Nous proposons un cadre statistique général à la problématique de la restructuration de documents et détaillons une instance d'un modèle stochastique de documents structurés appliquée à cette problématique. Nous détaillons enfin un ensemble d'expériences effectuées sur les documents du corpus INEX afin de mesurer la capacité de notre modèle.
**** *annee_2005  *numText_1004
Ce travail concerne le développement de méthodes de classification discriminantes pour des données séquentielles. Quelques techniques ont été proposées pour étendre aux séquences les méthodes discriminantes, comme les machines à vecteurs supports, par nature plus adaptées aux données en dimension fixe. Elles permettent de classifier des séquences complètes mais pas de réaliser la segmentation, qui consiste à reconnaître la séquence d'unités, phonèmes ou lettres par exemple, correspondant à un signal. En utilisant une correspondance donnée / modèle nous transformons le problème de l'apprentissage des modèles à partir de données par un problème de sélection de modèles, qui peut être attaqué via des méthodes du type machines à vecteurs supports. Nous proposons et évaluons divers noyaux pour cela et fournissons des résultats expérimentaux pour deux problèmes de classification.
**** *annee_2005  *numText_1005
Le clustering semi-supervisé combine l'apprentissage supervisé et non-supervisé pour produire meilleurs clusterings. Dans la phase initiale supervisée de l'algorithme, un échantillon d'apprentissage est produit par sélection aléatoire. On suppose que les exemples de l'échantillon d'apprentissage sont étiquetés par un attribut de classe. Puis, un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur (tels que les exemple de chaque cluster ont la même étiquette), qui servent de seeding clusters pour la deuxième phase non-supervisée de l'algorithme. Dans cette phase, l'algorithme incrémentiel est appliqué aux données non étiquetées. La qualité du clustering est évaluée par l'index de Gini moyen des clusters. Les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons d'apprentissage.
**** *annee_2005  *numText_1006
Dans cet article, nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes (Prediction suffix trees - PST). Ce type d'arbre permet de représenter une chaîne de Markov d'ordre variable. Dans différentes application, il s'est avéré plus efficace qu'une chaîne de Markov d'ordre fixe avec un coût calculatoire moindre. Pour ces raisons, il nous a paru intéressant d'exploiter le caractère arborescent de ce mode de représentation non seulement d'un point de vue algorithmique, mais aussi d'un point de vue visuel.
**** *annee_2005  *numText_1007
Cet article se place dans le cadre du subspace clustering, dont la problématique est double : identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini, et caractériser chaque cluster par un nombre minimal de dimensions, permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application. Les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique. L'objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels. Nous présentons une méthode basée sur l'algorithme classique EM mais opérant sur un modelé simplifié des données et suivi d'une technique originale de sélection d'attributs pour ne garder que les dimensions pertinentes de chaque cluster. Les expérimentations présentées ensuite, menées sur des bases de données aussi bien artificielles que réelles, montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus.
**** *annee_2005  *numText_1008
Nous présentons un algorithme de SVM et des méthodes graphiques pour le traitement de grands ensembles de données. Pour pouvoir traiter de tels ensembles de données, nous utilisons une représentation des données de plus haut niveau (sous forme symbolique). L'algorithme de séparateur à vaste marge (SVM) est adapté pour pouvoir traiter ce nouveau type de données. Nous construisons un nouveau noyau RBF (Radial Basis Function) que l'algorithme utilise à la fois pour la classification, la régression et la détection d'individus atypiques dans des données de type intervalle. Nous utilisons ensuite des méthodes de visualisation interactive (elles aussi adaptées au cas des variables de type intervalle) pour expliquer les résultats obtenus par les SVM. La méthode est évaluée sur des ensembles de données symboliques existant ou créés artificiellement.
**** *annee_2005  *numText_1010
TANAGRA est un logiciel « open source » librement accessible sur le web, il tente de concilier deux types d'utilisation. D'une part, en proposant une interface suffisamment conviviale, il est accessible aux utilisateurs non spécialistes qui veulent effectuer des études sur des données réelles. D'autre part, en définissant une architecture simplifiée à l'extrême, les efforts de développement portent sur l'essentiel, à savoir la mise au point et l'intégration d'algorithmes de fouille de données, les chercheurs peuvent ainsi mener des expérimentations sur les méthodes. Dans cet article, nous présentons les principales fonctionnalités du logiciel en essayant de le positionner sur l'échiquier des (très) nombreux logiciels diffusés actuellement.
**** *annee_2005  *numText_1011
L'étude de l'expression des gènes est depuis quelques années révolutionnée par les puces à ADN. Les méthodes habituellement mises en oeuvre pour analyser ces données s'appuient sur des algorithmes de partitionnement, comme les clustering hiérarchiques, et sur une hypothèse communément admise qui associe à un ensemble de profils d'expression similaires, une fonction identique. Cette analyse étudie l'ensemble des gènes sans distinction. L'approche que nous proposons deux catégories de gènes : connus ou putatifs. Pour chaque gène n'ayant pas d'information rattachée, nous étudions son voisinage afin d'y trouver des motifs fréquents (itemsets). Ensuite, l'Analyse est guidée par l'interprétation biologique afin de faire émerger des propriétés intéressantes. Un premier jeu de test sur Plasmodium Falciparum (agent de la Malaria) nous a permis de mettre en évidence, en nous intéressant aux items relatifs à la glycolyse, un transporteur de nucléosides qui intervient au niveau énergétique dans la phase ring (précoce) du parasite.
**** *annee_2005  *numText_1012
Cet article se situe dans le domaine de l'analyse formelle de concepts et du treillis de concepts (treillis de Galois) lequel est un cadre théorique intéressant pour le regroupement conceptuel des données et la génération des règles d'association. Puisque la prospection de données (data mining) est utilisée comme support à la prise de décision par des analystes rarement intéressés par la liste exhaustive (souvent très longue) des concepts et des règles, l'élaboration d'une solution approximative sera dans la plupart des cas un compromis satisfaisant et relativement moins coûteux qu'une solution exhaustive. Dans cet article, on propose une approche appelée CIGA (Closed Itemset Generation using an Automata) de génération partielle ou complète de concepts par la construction et le parcours d'un automate à états finis. La génération des concepts permet l'identification des itemsets fermés fréquents, étape cruciale pour l'extraction des règles d'association.
**** *annee_2005  *numText_1013
Cet article aborde le problème de la sélection de variables dans le cadre de la classification supervisée. Les méthodes de sélection reposent sur un algorithme de recherche et un critère d'évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables. Nous présentons un nouveau critère d'évaluation fondé sur une mesure d’ambiguïté. Cette mesure est fondée sur une combinaison d'étiquettes représentant le degré de spécificité ou d'appartenance aux classes en présence. Les tests menés sur de nombreux jeux de données réels et artificiels montrent que notre méthode est capable de sélectionner les variables pertinentes et d'augmenter dans la plupart des cas les taux de bon classement.
**** *annee_2005  *numText_1014
Avec le développement d'Internet et d'applications hypermédias, la construction et l'exploitation de profils ou modèles des utilisateurs deviennent capitaux dans de nombreux domaines. Pouvoir cibler un utilisateur d'un hypermédia ou d'un site web afin de lui proposer ce qu'il attend devient essentiel, par exemple lorsque l'on veut lui présenter les produits qu'il est le plus susceptible d'acheter, ou bien plus généralement à chaque fois que l'on veut éviter de noyer l'utilisateur dans un flot d'informations. Nous présentons un système d'aide à la navigation, intégrant un système de modélisation du comportement de navigation et un stratège qui met en oeuvre, en fonction du comportement détecté, une aide visant à recommander des liens particuliers.
**** *annee_2005  *numText_1015
La Sélection de Variable (SV) constitue une technique efficace pour réduire la dimension des espaces d'apprentissage et s'avère être une méthode essentielle pour le pré-traitement de données afin de supprimer les variables bruitées et/ou inutiles. Peu de méthodes de SV ont été proposées dans le cadre de l'apprentissage non supervisé, et, la plupart d'entre elles, sont des méthodes dites enveloppes nécessitant l'utilisation d'un algorithme d'apprentissage pour évaluer les sous ensembles de variables. Or, l'approche enveloppe est largement mal adaptée à une utilisation lors de cas réels. En effet, d'une part ces méthodes ne sont pas indépendantes vis à vis des algorithmes d'apprentissage non supervisé qui nécessitent le plus souvent de fixer un certain nombre de paramètres ; mais surtout, il n'existe pas de critères bien adaptés à l'évaluation de la qualité d'apprentissage non supervisé dans des sous espaces différents. Nous proposons et évaluons dans ce papier une méthode filtre et donc indépendante des algorithmes d'apprentissage non supervisé. Cette méthode s'appuie sur deux indices permettant d'évaluer l'adéquation entre deux ensembles de variables (entre deux sous espaces).
**** *annee_2005  *numText_1016
Dans cet article nous présentons une méthode d'évaluation de la pertinence des pages Web retournées par un moteur de recherche.
**** *annee_2005  *numText_1017
Cet article présente une application en grandeur réelle des arbres de classification dans un contexte non classificatoire. Les arbres générés visent à mettre en lumière les différences régionales dans la façon dont les femmes décident de leur participation au marché du travail. L'accent est donc mis sur la capacité descriptive plutôt que prédictive des arbres. L'application porte sur des données relatives à la participation féminine au marché du travail issues du Recensement Suisse de la Population de l'an 2000. Ce vaste ensemble de données a été analysé en deux phases. Un premier arbre exploratoire a mis en évidence la nécessité de procéder à des études séparées pour les non mères, les mères mariées ou veuves, et les mères célibataires ou divorcées. Nous nous limitons ici aux résultats de ce dernier groupe, pour lequel nous avons généré un arbre séparé pour chacune des trois régions linguistiques principales. Les arbres obtenus font apparaître des différences culturelles fondamentales entre régions. Du point de vue méthodologique, la principale difficulté de cet usage non classificatoire des arbres concerne leur validation, puisque le taux d'erreur de classification généralement retenu perd tout son sens dans ce contexte. Nous commentons cet aspect et illustrons l'usage d'alternatives plus pertinentes et facilement calculables.
**** *annee_2005  *numText_1018
Notre travail de recherche consiste à représenter l'ontologie des modèles e-business e-BMO par le langage BM²L spécifié sur la base d'un méta-modèle XML. BM²L est comparé à d'autres langages de définition d'ontologie à savoir, RDF(S), DAML + OIL et OWL et ce, selon un framework établis sur les spécificités de cette ontologie. Aussi, introduisons nous une application Web e-BMH pour la conception et l'exploitation des modèles e-business conformément à l'ontologie.
**** *annee_2005  *numText_1019
En apprentissage supervisé, la prédiction de la classe est le but ultime. Plus largement, on attend d'une bonne méthodologie d'apprentissage qu'elle permette une représentation des données susceptible de faciliter la navigation de l'utilisateur dans la base d'exemples et d'aider au choix des exemples et des variables pertinents tout en assurant une pré-diction de qualité dont on comprenne les ressorts. Différents travaux ont montré l'aptitude des graphes de voisinage issus des prédicteurs à fonder une telle méthodologie, ainsi le graphe des voisins relatifs de Toussaint. Cependant, la complexité de leur construction, en O(n3), reste élevée. Dans le cas de données volumineuses, nous proposons de substituer aux graphes de voisinage les cartes de Kohonen construites sur les prédicteurs. Après un bref rappel du principe des cartes de Kohonen en apprentissage non supervisé, nous montrons comment celles-ci peuvent fonder une stratégie d'apprentissage optimisée. Nous proposons ensuite d'évaluer la qualité de cette stratégie par une statistique originale qui est étroitement corrélée au taux d'erreur en généralisation. Différentes expérimentations montrent la faisabilité de cette approche. On dispose alors d'un critère fiable pour sélectionner les individus et les attributs pertinents.
**** *annee_2005  *numText_1020
Nous proposons dans cet article une méthode de visualisation de l'activité des utilisateurs d'un site web qui permet d'évaluer qualitativement l'adéquation entre son architecture logique et la perception de celle-ci par les internautes. Nous travaillons sur les parcours des internautes sur le site étudié, après reconstruction de ceux-ci grâce aux fichiers logs des serveurs concernés. Nous utilisons la structure logique des sites étudiés pour simplifier la représentation des parcours, en ne tenant pas compte de l'ordre de visite des catégories sémantiques du site. Les parcours simplifiés sont utilisés pour calculer une dissimilarité entre les catégories sémantiques qui sont ensuite représentées dans un plan par Multi Dimensional Scaling. Nous complétons cette visualisation d'ensemble par une représentation de l'arbre couvrant minimal des catégories sémantiques qui permet de mieux appréhender certaines interactions. Nous illustrons l'intérêt de la méthode en l'appliquant au site de l'INRIA.
**** *annee_2005  *numText_1021
Suite à la survenue récente de scandales financiers, la synthèse des idées mobilisables en gouvernance d'entreprise semble désormais essentielle si l'on veut sécuriser les investisseurs. Dans cette perspective, le présent projet de recherche consiste à mettre en oeuvre un panel d'outils d'analyse de données textuelles (Alceste, Syntex, Tropes-Zoom/Decision Explorer, Wordmapper, Weblex) afin d'évaluer les moyens dont peut disposer un analyste désireux d'extraire des connaissances contenues dans un ensemble d'articles académiques. La qualité de représentation du corpus dans sa globalité est tout d'abord testée. L'étude est ensuite centrée sur le concept même de connaissance, mobilisé dans la théorie de la gouvernance des entreprises. La convergence et la complémentarité des approches méthodologiques sont alors explicitées. Il en est de même pour ce qui concerne la capacité d'extraction d'une connaissance pertinente à partir des textes étudiés.
**** *annee_2004  *numText_1022
L'augmentation vertigineuse de la taille des données (textuelles ou transactionnelles) est un défi constant pour la scalabilité des techniques d'extraction des connaissances. Dans ce papier, on présente une approche pour la dérivation des bases génériques de règles associatives. Les principales caractéristiques de cette approches sont les suivantes. D'une part, l'introduction d'une structure de données appelée Trie-itemset pour le stockage de la relation en entrée. D'autre part, on utilise une méthode Diviser pour régner pour réduire le coût de construction de structures partiellement ordonnées, à partir desquelles les bases génériques de règles sont directement extraites.
**** *annee_2004  *numText_1023
Nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de l'espace métrique des partitions d'un ensemble fini. Nous discutons deux nouvelles idées fondamentales : une généralisation des techniques de discrétisation de Fayyad-Irani basée sur une distance sur des partitions, dérivée de l'entropie généralisée de Daroczy, et un nouveau critère géométrique pour arrêter l'algorithme de discrétisation. Les arbres de décision résultants sont plus petits, ont moins de feuilles, et montrent des niveaux plus élevés d'exactitude établis par la validation croisée stratifiée.
**** *annee_2004  *numText_1024
Dans le domaine de l'apprentissage supervisé, les méthodes de groupage des modalités d'un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de l'attribut initial et diminuant le nombre de modalités. Nous proposons ici une généralisation de l'algorithme de discrétisation Khiops pour le problème du groupage des modalités. L'algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et d'améliorer significativement la robustesse des groupages produits. Cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du Khi2 lors de regroupements de lignes d'un tableau de contingence et en modélisant le comportement statistique de l'algorithme Khiops. Des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage Khiops aboutit à des groupages performants, à la fois en terme de qualité prédictive et de faible nombre de groupes.
**** *annee_2004  *numText_1025
L'algorithme EM est très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange. L'inconvénient majeur de cet algorithme est la lenteur de sa convergence. Son application sur des tableaux de grande taille pourrait ainsi prendre énormément de temps. Afin de remédier à ce problème, nous étudions ici le comportement de plusieurs variantes connus de EM, ainsi qu'une nouvelle méthode. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification. Nous réalisons une étude comparative entre les différentes variantes sur des données simulées et réelles et proposons une stratégie d'utilisation de notre méthode qui s'avère très efficace.
**** *annee_2004  *numText_1026
Qu'y a t'il de commun aujourd'hui entre l'acquisition de données 3D, la gestion d'informations patrimoniales, ou encore la modélisation tridimensionnelle en temps réel ? Bien peu, force est de le constater, si ce n'est que l'édifice patrimonial sert là souvent de terrain d'expérimentation. Pourtant, il ne saurait être réduit à ce seul statut : il est objet de connaissances dont l'étude doit bénéficier de différents jeux de technologies. Notre proposition, expérimentée sur des vestiges du théâtre antique d'Arles, place cet édifice au centre d'un dispositif visant à intégrer, au sein d'un système d'informations architecturales 3D en devenir, les résultats de différentes phases de son étude. Un jeu de connaissances formalisé sur l'édifice sert de dénominateur commun depuis l'acquisition de données 3D jusqu'à la représentation dans une maquette temps réel pour la toile. Cette maquette devient outil de navigation dans le jeu d'informations et de savoirs qui caractérise l'édifice.
**** *annee_2004  *numText_1027
La découverte de connaissances à partir d'importantes masses de données hétérogènes débouche le plus souvent sur l'analyse relationnelle. La recherche d'informations stratégiques s'appuie en effet sur les liens fonctionnels et sémantiques entre documents, acteurs, terminologie et concepts d'un domaine sans oublier le paramètre temps. De nombreuses méthodes sont proposées pour identifier, analyser et visualiser les mécanismes mis à jour : analyse relationnelle, classifications supervisées et non supervisées, analyse factorielle, analyse sémantique, cartes, dendogrammes, ... Mais ces approches demandent souvent une expertise non négligeable pour être comprises et ne s'adressent donc pas aux non initiés. Par contre, la vue d'un graphe mettant en relation une ou deux classes d'éléments interdépendants est directement assimilable par tout le monde. Nous proposons donc un ensemble de visualisations interactives de graphes dont la manipulation doit permettre une découverte de connaissances intuitive et basée sur un langage graphique naturel. Nous illustrons notre propos de nombreux exemples tirés de cas réels d'analyses stratégiques qui ont permis d'évaluer cette approche sur un panel très large de données.
**** *annee_2004  *numText_1028
Nous proposons dans cet article un mécanisme automatique d'annotation de documents. Ce mécanisme s'appuie sur une opération de composition permettant de créer de nouveaux documents à partir de documents existants et sur un algorithme permettant d'inférer l'annotation d'un document composé à partir d'annotation de ses parties. Notre modèle est illustré par une étude de cas consacrée à la mise en commun de documents pédagogiques au format XML, dans un environnement coopératif d'enseignement à distance. Nous décrivons un prototype permettant d'annoter ces documents, et d'engendrer une description RDF contenant les annotations.
**** *annee_2004  *numText_1031
Cet article présente une méthode d'apprentissage des profils dans les systèmes de filtrage d'information. Le processus d'apprentissage est effectué d'une manière incrémentale au fur et à mesure que les informations sont filtrées et jugées par l'utilisateur. Des expérimentations effectuées sur une collection de test de référence TREC, montrent que la méthode permet effectivement l'amélioration des profils.
**** *annee_2004  *numText_1032
Dans ce papier, nous proposons une nouvelle méthode d'extraction des règles d'association dans des bases de données relationnelles basée sur la technologie des arbres de Peano (Ptree). La structure de données utilisée pour représenter la base de données est un ensemble de Ptrees de base représentant chacun un vecteur binaire et tous ces Ptrees sont stockés dans des fichiers binaires. Nous montrons que la structure Ptree combinée avec la technique de réduction appelée élagage par support minimum produisent des règles d'association fortes et réduisent considérablement le temps de construction de l'association. En effet, notre approche présente l'avantage de ne pas effectuer des parcours coûteux de la base de données. Cette approche a été testée à travers un prototype que nous avons implémenté. Les résultats expérimentaux montrent que les règles d'association fortes sont générées dans un temps minimum comparativement à d'autres travaux.
**** *annee_2004  *numText_1033
Il existe de nombreuses techniques qui permettent de classifier les documents textuels en fonction de l'intérêt d'un utilisateur (kNN, SVM, ...). Malheureusement, l'intégration de ces méthodes dans les plates-formes de textmining est souvent très statique au cours du temps. Le but de cet article est de présenter une plate-forme de webmining dans laquelle les données hétérogènes sont représentées uniformément selon un formalisme XML/TEI et où l'utilisateur peut interagir sur les processus de récupération et d'analyse de ces données. Pour cela, les modules de traitements sont représentés par des agents fonctionnant sur la plate-forme MadKit et l'apprentissage se fait par une méthode dérivée de VSM et TFIDF utilisant un principe de listes noires pondérées permettant la reconnaissance de documents indésirables. La dynamique de la plate-forme repose principalement sur la possibilité d'ajouter à la volée des agents de traitement et de pouvoir modifier l'ordre et les paramètres d'analyse des documents.
**** *annee_2004  *numText_1034
Un projet ciblé sur l'étude du domaine des maladies à prions à permis de formaliser une méthodologie commune, sociologique et informatique, de compréhension de sa dynamique par l'analyse thématique. Nous avons créé une plate forme d'indexation de notices bibliographiques dont le but est d'extraire des associations évoluant à travers des intervalles de temps. Beluga propose une chaîne de traitement basée sur l'indexation des documents en unités de base : références, auteurs, termes simples et composés, organismes. L'outil est fondé sur une double approche d'apprentissage et de visualisation qui automatise les processus d'extraction de groupes d'auteurs et de termes, et permet à l'utilisateur de revenir aux données documentaires sources. L'analyse diachronique de corpus de documents électroniques nous permet d'analyser comment la terminologie est structurée en thématiques émergentes.
**** *annee_2004  *numText_1035
Nous nous intéressons à la représentation et au chargement de bases de transactions en mémoire. Pour cela, nous proposons d'utiliser un format condensé fondé sur les diagrammes de décision binaires et nous présentons un algorithme que nous avons implanté en un système baptisé BooLoader, pour charger des bases de transactions. Nous donnons également des résultats expérimentaux de notre système sur des bases éparses et denses.
**** *annee_2004  *numText_1036
L'identification de signatures de protéines est un problème majeur pour la découverte de nouveaux membres dans des familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles distantes sont trop hétérogènes pour qu'on puisse identifier les régions conservées à partir des algorithmes classiques de la bioinformatique. Nous proposons une approche génétique pour la découverte de motifs hiérarchiques; l'algorithme suit une démarche descendante en s'appuyant dans une première phase sur les classes physico-chimiques des acides aminés. Les signatures sont ensuite définies par des séquences des motifs ainsi obtenus. Elles sont extraites au moyen d'un algorithme de découverte d'itemsets séquentiels où les motifs jouent le rôle d'items. Une dernière étape consiste à fouiller dans cette base d'itemsets pour n'en retenir qu'un ensemble réduit de signatures. Plusieurs stratégies sont proposés pour déterminer un ensemble optimal de signatures qui respecte des contraintes de complétude, de cardinalité et de spécificité. Nous appliquons notre démarche sur la famille des cytokines. L'analyse de la base de protéines SCOP a montré que les groupes de signatures que nous avons extrait cible spécifiquement cette famille d'intérêt.
**** *annee_2004  *numText_1037
La caractérisation globale de l'exécution de jobs passe par l'exploitation de mesures recueillies sur les machines en production. Afin de répondre à la problématique, il est nécessaire de tenir compte des différents types de données, ainsi que de la dualité de la caractérisation : statique et dynamique. Une solution technique répondant aux contraintes est proposée. Elle repose sur l'utilisation de SVM afin de détecter des phases, et à un niveau supérieur, à un réseau bayésien afin d'automatiser l'analyse de modèles de Markov enrichis. Ceux-ci sont introduits comme la base formelle et synthétique de description du comportement du job, aussi bien sur un système batch, que parallèle. Enfin, les résultats obtenus à l'aide d'un prototype sont discutés.
**** *annee_2004  *numText_1039
L'analyse du comportement des utilisateurs d'un site Web est un domaine riche et complexe. Le grand nombre de méthodes d'extraction de connaissances appliquées aux logs Web, ainsi que la diversité du type de ces méthodes en est une preuve. Cependant, compte tenu de cette complexité, nous posons dans cet article la question suivante : Est-il possible de combiner des méthodes existantes pour proposer une analyse qui tire profit des résultats de plusieurs spécialités et extraire par exemple des comportements fréquents minoritaires ?Notre étude à donc porté sur une nouvelle approche hybride (issue de la classification neuronale et de la recherche de motifs séquentiels) visant à classer les navigations des utilisateurs d'un site (à l'aide de leurs résumés sémantiques) puis, pour chaque classe de navigations, d'en extraire les comportements fréquents. Notre objectif est 1) de pallier les limites de l'extraction de motifs fréquents par rapport à la quantité de données à traiter et aussi par rapport à la qualité des résultats et 2) de pallier les limites d'une première méthode d'analyse du comportement appelée Diviser pour Découvrir, que nous avons proposé en 2003. Nous avons mené des expérimentations sur les logs HTTP des sites INRIA. Les résultats obtenus confirment le bien fondé de notre approche vis à vis de l'état de l'art.
**** *annee_2004  *numText_1042
Les algorithmes d'extraction de règles d'association parcourent efficacement le treillis des itemsets pour constituer une base de règles admissibles à des seuils de support et de confiance, mais donnent une multitude de règles peu exploitables. Nous suggérons d'épurer de telles bases en éliminant les règles non statistiquement significatives. La multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélectionnées à tort. après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque, mais le nombre de fausses découvertes, nous proposons BS_DF, un algorithme original fondé sur le bootstrat qui sélectionne les règles significatives en contrôlant le nombre de fausses découvertes. Des expérimentations montrer l'efficacité de ces procédures.
**** *annee_2004  *numText_1043
Cet article présente l'utilisation d'une technique de fouille de données pour aider à la spécification de vues sur des sources XML. Notre langage de vues permet d'intégrer des données XML provenant de sources hétérogènes. Cependant, la définition de motifs sur les sources permettant de spécifier les données à extraire est souvent difficile, car la structure des données n'est pas toujours connue. Nous proposons d'extraire les structures fréquentes dans les données des sources pour spécifier des motifs pertinents à utiliser dans la spécification des vues.
**** *annee_2004  *numText_1046
Nous proposons une méthode automatique de comparaison de textes reposant sur une technique de transformation d'un texte en une image de taille donnée et l'analyse à l'aide des outils de la géométrie fractale. Nous présentons une application à l'étude d'un corpus de 90 textes longs.
**** *annee_2004  *numText_1047
La validation des connaissances extraites d'un processus d'ECD par un expert métier nécessite de filtrer ces connaissances. Pour ce faire, de nombreuses mesures ont été proposées, chacune répondant à des besoins spécifiques. Ces mesures présentent des caractéristiques variées et parfois contradictoires qu'il convient alors d'examiner. Arguant du fait que la sélection des bonnes connaissances passe aussi par l'utilisation d'un ensemble de mesures adaptées au contexte, nous présentons dans cet article une étude expérimentale de différentes mesures. Cette étude est mise en regard d'une étude formelle synthétisant les qualités des mesures.
**** *annee_2004  *numText_1050
Après l'ère du décodage des génomes, les biologistes sont de plus en plus confrontés à l'intégration de myriades de connaissances parcellaires, stockées majoritairement sous forme textuelle. Nous montrons, à travers un exemple concret, que la conjonction de deux chaînes de traitement faisant appel de façon modérée à l'expertise humaine offre au biologiste une aide utile pour parcourir cette littérature, à partir d'une structuration sans a priori de son corpus ; il s'agit ici de résumés Medline indexés par les gènes et protéines qu'ils citent, et que l'algorithme structure (sans superviseur) en principales voies métaboliques et de régulation présentes dans le corpus choisi. 1) Une chaîne d'indexation par les noms de gènes et protéines inclut un expert pour valider, 2) Un environnement interactif de clustering thématique attribue des valeurs graduées de centralité dans chaque thème aux résumés comme aux noms, comme à toute autre variable illustrative (autres termes bio., MeSH, ...).
**** *annee_2004  *numText_1052
Cet article présente le fonctionnement d'un SIAD : alimentation, traitement des données qui l'alimentent, production des résultats, outils de consultation mis à la disposition des utilisateurs, exploitation éditoriale.
**** *annee_2004  *numText_1053
Les entrepôts de données stockent des quantités de données de plus en plus massives, en particulier du fait de la constitution d'historiques. Nous proposons ici une solution pour éviter la saturation des entrepôts de données. Nous définissons un langage de spécifications de fonctions d'oubli des données les plus anciennes, permettant de déterminer ce qui doit être présent dans l'entrepôt de données à chaque instant. Ces spécifications de fonctions d'oubli se traduisent par des opérations de résumé par agrégation, et par des opérations de suppression des données anciennes réalisées de façon mécanique à chaque pas de mise à jour. La communication présente tout d'abord une description syntaxique du langage de spécifications des fonctions d'oubli. Les contraintes à vérifier pour assurer la cohérence du langage sont ensuite décrites. Enfin, nous proposons des structures de données adaptées au stockage des données nécessaires à la gestion des fonctions d'oubli.
**** *annee_2004  *numText_1054
La prolifération des documents XML appelle des techniques appropriées pour extraire et exploiter l'information contenue dans ces documents. On distingue deux approches de fouille : XML Content Mining portant sur le contenu et XML Structure Lining qui a trait à la structure des documents. Combiner ces deux approches est très intéressant. Les informations contenues dans la structure orientent la fouille sur le contenu. Nous présentons la première étape de cette démarche : une nouvelle méthode d'extraction des règles d'association à partir de la structure des documents XML qui permet de gérer les aspects hiérarchiques de ces documents tout en améliorant les mécanismes d'extraction grâce à la création d'une structure spéciale représentant la hiérarchie des balises rencontrées.
**** *annee_2004  *numText_1055
Les SVM (support vector machines) ont montré leur efficacité dans plusieurs domaines d'application. L'apprentissage des SVM se ramène à résoudre un programme quadratique, dont la mise en œuvre est en général coûteuse en temps. Une reformulation plus récente des SVM (proximal SVM), proposée par Fung et Mangasarian, ne nécessite que la résolution d'un système linéaire, cet algorithme de PSVM est plus efficace et permet de traiter des données dont le nombre d'individus est très important (109) et le nombre d'attributs plus restreint (104). Nous proposons d'utiliser la formule de Sherman-Morrison-Woodbury pour adapter le PSVM à la fouille d'ensembles de données dont le nombre d'attributs est très important et le nombre d'individus plus restreint sur un matériel standard. Puis nous présentons un algorithme de boosting de PSVM pour classifier des données de très grandes tailles en nombre d'individus et d'attributs. Nous évaluons les performances du nouvel algorithme sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et Ndc.
**** *annee_2004  *numText_1059
Contrairement aux méthodes usuelles de classification ne cherchant généralement qu'une seule partition, soit des instances, soit des attributs, les méthodes de classification croisée et de classification directe fournissent des blocs de données liant des instances à des attributs. Les premières consistent à chercher simultanément une partition en lignes et une partition en colonnes. Les secondes, elles, s'appliquent directement sur les données, et permettent d'obtenir des blocs de données homogènes de toutes tailles, ainsi que des hiérarchies de classes en lignes et en colonnes. Combinant les avantages des deux méthodes, nous présentons ici une méthodologie permettant de travailler sur de grandes bases de données.
**** *annee_2004  *numText_1060
Lorsque des outils inductifs sont inclus dans un système d'acquisition des connaissances, on dit que l'on construit un système apprenti. C'est dans le but de soulager la charge de travail de l'expert du domaine que cette forme d'apprentissage comporte des outils inductifs. La difficulté tient en ce que l'énumération des connaissances expertes produit des données peu bruitées mais très incomplètes que les itérations successives d'induction vont compléter, toutefois en y ajoutant de grandes quantités de bruit. Il en résulte qu'on doit utiliser des procédures inductives spéciales, adaptées à l'apprentissage par croissance de noyaux de connaissance supervisée. En particulier, pour résoudre le problème difficile de la reconnaissance de concepts dans les textes, nous avons défini une forme d'apprentissage qui intègre l'apprentissage à partir d'instances et les systèmes apprentis, que nous nommons Induction Extensionnelle, un oxymoron qui souligne que malgré l'absence de création d'un modèle explicite, une induction prend effectivement place.
**** *annee_2004  *numText_1061
Cet article présente une nouvelle approche permettant d'appliquer des algorithmes de fouille, en particulier d'apprentissage supervisé, à de grandes bases de données et en des temps de traitement acceptables. Cet objectif est atteint en intégrant ces algorithmes dans un SGBD. Ainsi, nous ne sommes limités que par la taille du disque et plus par celle de la mémoire. Cependant, les entrées-sorties nécessaires pour accéder à la base engendrent des temps de traitement longs. Nous proposons donc dans cet article une méthode originale pour réduire la taille de la base d'apprentissage en construisant sa table de contingence. Les algorithmes d'apprentissage sont alors adaptés pour s'appliquer à la table de contingence. Afin de valider notre approche, nous avons implémenté la méthode de construction d'arbre de décision ID3 et montré que l'utilisation de la table de contingence permet d'obtenir des temps de traitements équivalents à ceux des logiciels classiques.
**** *annee_2004  *numText_1062
Nous nous plaçons dans le cadre d'un projet de constitution d'une plate-forme intégrative de données biomédicales pour l'étude génomique des cancers. La plate-forme comporte, entre autres, un certain nombre de scénarios d'analyse qui sont proposés à l'utilisateur. A chaque étape d'un scénario qu'il a choisi de réaliser pour les besoins de son étude, l'utilisateur peut être amené à poser une requête nécessitant d'accéder à différentes sources et il doit alors choisir les sources pertinentes. Nous proposons un guide à l'utilisateur sous forme d'un algorithme de sélection de sources adapté à sa requête et à ses préférences. Pour cela, nous explorons quelques spécificités des banques de données biomédicales et définissons différents critères de préférence utiles pour les biologistes. Nous illustrons notre démarche avec un exemple de requête biomédicale.
**** *annee_2004  *numText_1065
L'acquisition des connaissances terminologiques de l'entreprise se fait souvent à partir des textes qu'elle utilise. Dans le cadre de ce travail, la base de connaissances terminologiques repose sur la modélisation des concepts-métier sous la forme d'une ontologie. Le problème de la maintenance de cette base et de cette ontologie doit alors être traité.Dans cet article, après avoir donné une définition d'une base de connaissances terminologiques (BCT) et des problèmes de diachronie, nous présentons notre modèle et notre méthode d'acquisition des connaissances terminologiques de l'entreprise. Nous exposons alors notre proposition pour maintenir au cours du temps la base de connaissances terminologiques ainsi construite.Nous illustrons ce travail sur une base de connaissance terminologique sur le cinéma d'animation en décrivant le problème de la maintenance dans une reconstitution historique de différents états de cette base lors de l'apparition des techniques numériques d'animation.
**** *annee_2004  *numText_1067
Cet article développe une extension d'une architecture de médiation pour intégrer le Web sémantique. Plus précisément, XLive est un médiateur tout XML développé à PRiSM. Il permet d'exécuter des XQuery sur des sources de données hétérogènes. Après une rapide présentation de XLive et du Web sémantique, une architecture à trois niveaux d'ontologies et de schémas est introduite pour connecter des adaptateurs pour le Web sémantique. Cette architecture vise à intégrer des sources de type Web service d'information conformément à une ontologie globale de référence. Elle conduit à étendre XLive avec le support de vues, un outil de conception de vues et de mappings, et des adaptateurs pour les Web services.
**** *annee_2004  *numText_1068
La validation des connaissances est l'une des étapes les plus problématiques d'un processus de découverte de règles d'association. Pour que le décideur (expert des données) puisse trouver des connaissances intéressantes dans les grandes quantités de règles produites par les algorithmes de fouille de données, il est nécessaire de mesurer la qualité des règles. Nous insérant dans le cadre de l'analyse statistique implicative, nous proposons dans cet article d'évaluer les règles en considérant leur contenu informationnel à travers un nouvel indice de qualité fondé sur l'entropie de Shannon : TIC (Taux Informationnel modulé par la Contraposée). Cet indice a l'avantage d'être bien adapté à la sémantique des règles, puisque d'une part il respecte leur caractère asymétrique et d'autre part il tire profit de leurs contraposées. Par ailleurs, c'est à notre connaissance la seule mesure de qualité de règles qui intègre à la fois indépendance et déséquilibre, c'est-à-dire qui permette de rejeter simultanément les règles entre variables corrélées négativement et les règles qui possèdent plus de contre-exemples que d'exemples. Des comparaisons de TIC avec la J-mesure, l'information mutuelle, l'indice de Gini, et la confiance sont réalisées sur des simulations numériques.
**** *annee_2004  *numText_1069
Nous rendons compte d'une démarche mise en place pour construire une représentation fine des usages d'internet et de leur évolution, en procédant à du traitement secondaire de données de trafic, provenant de panels représentatifs d'internautes. Après avoir présenté les caractéristiques des cohortes étudiées et les différents modes d'enrichissement des données de trafic mis en place, nous présentons quelques résultats construits à partir de ces données enrichies, et en particulier une segmentation des internautes construite sur la base de l'entrelacement des pratiques de communication et de navigation.
**** *annee_2004  *numText_1070
La fouille de données spatiales nécessite l'analyse des interactions dans l'espace. Ces interactions peuvent être matérialisées dans des tables de distances, ramenant ainsi la fouille de données spatiales à l'analyse multitables. Or, les méthodes de fouilles de données traditionnelles considèrent une seule table en entrée où chaque tuple est une observation à analyser. De simples jointures entre ces tables ne résout pas le problème et fausse les résultats en raison du comptage multiple des observations. Nous proposons trois alternatives de fouille de données multi-tables dans le cadre de la fouille des données spatiales. La première consiste à interroger à la volée les différentes tables et modifie en dur les algorithmes existants. La seconde est une optimisation de la première qui pré -calcule les jointures et adapte les algorithmes existants. La troisième réorganise les données dans une table unique en complétant - et non en joignant- la table d'analyse par les données présentes dans les autres tables, ensuite applique un algorithme standard sans modification. Cet article présente ces trois alternatives. Il décrit leur implémentation pour la classification supervisée et compare leur performance.
**** *annee_2004  *numText_1071
La compétence et la connaissance sont deux concepts qui nous semblent fortement conjoints, cependant, ils sont rarement étudiés et gérés ensemble. Nous cherchons donc à identifier les liens et frontières qui peuvent exister entre eux. Ceci a pour objectif de développer un modèle de représentation et de gestion, intégré aux connaissances et aux compétences. Dans cet article, est tout d'abord présentée, une synthèse sur les concepts de compétence et de connaissance. Ensuite, les modèles et outils de gestion de ces concepts sont exposés. Puis, le modèle CKIM (Competency and Knowledge Integrated Model) développé, est défini. Les utilités de ce modèle et son exploitation sont discutées en quatrième partie. La dernière partie représente un prototype d'implantation du modèle CKIM réalisé sur le serveur de connaissances ATHANOR.
**** *annee_2004  *numText_1072
Nous proposons dans cet article un modèle topologique de représentation de bases d'images. Chaque image est représentée à l'aide d'un vecteur de caractéristiques dans R^p et figure comme nœud dans un graphe de voisinage. L'exploration du graphe correspond à la navigation dans la base de données, les voisins d'un nœud représentent des images similaires. Afin de pouvoir traiter des requêtes, nous définissons un modèle topologique. L'image requête est représentée par un vecteur de caractéristiques dans R^p et insérée dans le graphe en mettant à jour localement les relations de voisinage. Ce travail se positionne dans le domaine de la fouille de données complexes.
**** *annee_2004  *numText_1073
L'apprentissage efficace du profil utilisateur est un challenge car il évolue sans cesse. Dans cet article nous proposons une nouvelle approche pour l'apprentissage du profil long-terme de l'utilisateur pour le filtrage de documents textuels. Dans ce cadre, les documents consultés sont classés de manière dynamique et nous analysons la répartition dans le temps des classes de documents afin de déterminer le mieux possible les classes d'intérêts de l'utilisateur. L'étude empirique confirme la pertinence de notre approche pour une meilleure personnalisation de documents.
**** *annee_2004  *numText_1074
Nous présentons dans cet article une nouvelle approche de modélisation de l'expérience d'utilisation d'un système informatique, avec pour objectif de réutiliser cette expérience en contexte pour assister l'utilisateur à effectuer sa tâche. Quatre scénarios illustrent cette approche.
**** *annee_2004  *numText_1075
L'analyse en ligne OLAP (On-Line Analysis Processing) et la fouille de données (Data Mining) sont deux champs de recherche qui ont connu, depuis quelques années, des évolutions parallèles et indépendantes. De récentes études ont montré l'importance et l'intérêt de l'association entre ces deux domaines scientifiques. A l'heure actuelle, on assiste à l'accroissement du besoin d'une analyse en ligne plus élaborée. Nous pensons que le couplage entre OLAP et la fouille de données pourra apporter des réponses à ce besoin. Dans cet article, nous proposons d'adopter ce couplage en vue de créer un nouvel opérateur, baptisé OpAC (Opérateur d'Agrégation par Classification), d'analyse en ligne des données multidimensionnelles. OpAC consiste particulièrement en l'agrégation sémantique des modalités d'une dimension d'un cube de données en se basant sur la technique de la classification ascendante hiérarchique.
**** *annee_2004  *numText_1077
Cet article présente un système de visualisation permettant l'observation des comportements collectifs implicites. Il s'agit de reconnaître et de représenter des communautés à partir des connexions Internet des utilisateurs : les utilisateurs sont répartis en communautés en fonction des similarités entre des listes de termes établies sur l'analyse des documents consultés par chacun d'eux. L'étude est rendue dynamique par la comparaison des communautés reconnues sur des périodes de temps connexes. L'outil décrit ci après offre deux représentations différentes de ces communautés : une vision des liaisons thématiques entre les utilisateurs sur chaque période étudiée et une vue comparative des communautés reconnues sur toute la durée de l'étude.
**** *annee_2004  *numText_1078
Nous décrivons l'algorithme PoBOC (Pole-Based Overlapping Clustering) qui génère un ensemble de clusters non-disjoints (ou softclusters) présentés sous forme d'une hiérarchie de concepts à partir de la seule matrice de similarités sur les données considérées. Nous évaluons l'approche sur deux situations d'apprentissage : la classification par apprentissage de règles et l'organisation de données plus complexes et peu structurées telles que les données textuelles.La validation des méthodes de clustering est une étape difficile résolue le plus souvent par une évaluation d'experts. Les deux applications proposées permettent de valider la méthode d'organisation selon deux points de vue : d'une part quantitativement en évaluant l'influence de la méthode pour la classification, d'autre part en permettant une analyse humaine du résultat dans le cas des données textuelles. Nous mettons en évidence l'intérêt de PoBOC comparativement à d'autres approches d'apprentissage non-supervisé.
**** *annee_2004  *numText_1081
Les langages de requêtes mots-clés pour le web manquent souvent de précision lorsqu'il s'agit de rechercher des documents particuliers difficilement caractérisables par de simples mots-clés (exemple : des cours java ou des photos de formule 1). Nous proposons un langage multi-critères de type attribut-valeur pour augmenter la précision de la recherche de documents sur le web.Nous avons expérimentalement montré le gain de précision de la recherche de documents basé sur ce langage.
**** *annee_2004  *numText_1082
Une base d'images fixes peut être décrite de plusieurs façons, notamment par des descripteurs visuels globaux de couleur, de texture, ou de forme. Les requêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types de descripteurs : par exemple, retrouver toutes les images ayant une couleur et une texture semblables à celles d'une image requête donnée. Pour retrouver plus efficacement et plus rapidement une image dans une grande base, nous exploitons des combinaisons appropriées de descripteurs et étudions l'intérêt des règles d'association entre clusters de descripteurs pour accélérer le temps de réponse à des requêtes sur de grandes bases d'images fixes.
**** *annee_2004  *numText_1083
L'Extraction de Connaissances dans la Bases de Données est devenue, pour les banques, une alternative au problème lié à la quantité de données qui sont stockées et qui ne cessent d'augmenter. Ceci aboutit à un paradoxe puisqu'il faut mieux cibler la clientèle susceptible d'être intéressée par une offre en utilisant des méthodes qui ne permettent plus de traiter le nombre croissant d'enregistrements des bases de données. Nos travaux se situent dans la continuité d'une étude que nous avons réalisée sur la recherche de règles d'association appliquée au marketing bancaire. En effet, des premiers résultats encourageants nous ont conduit à approfondir nos travaux vers une recherche de règles d'association hiérarchiques utilisant non plus une approche automatique mais une approche anthropocentrée. Il s'agit d'une approche dans laquelle l'expert fait partie intégrante du processus en jouant le rôle d'heuristique évolutive. Cet article présente les résultats de notre démarche de recherche.
**** *annee_2004  *numText_1085
De nombreuses tâches en Fouille de Données visent à extraire des connaissances exprimées sous la forme d'un ensemble de règles. Les algorithmes dédiés à ces tâches engendrent des règles dont l'adéquation aux données doit être évaluée. On se place dans le cadre où cette évaluation est réalisée directement en lançant des requêtes de dénombrement sur la base de données, et où cette base est relationnelle. Les requêtes comptent les données qui s'apparient avec la règle, calcul qui peut être extrêmement coûteux. Dans cet article, nous étudions l'impact d'une approche d'échantillonnage visant à réduire le coût de l'évaluation des règles relationnelles en tenant compte des spécificités structurelles des requêtes induites.
**** *annee_2004  *numText_1086
Dans l'étude du patrimoine bâti, la gestion d'informations pose aujourd'hui des problèmes d'interfaçage non triviaux, notamment par la masse, la diversité, la complexité et le caractère hétérogène des contenus. La représentation tridimensionnelle du tissu urbain à différentes échelles (de la ville au corpus architectural), parce qu'elle localise spatialement l'information à délivrer et l'attache à la morphologie de l'édifice, apparaît comme une des réponses possibles. Cette réponse semble par ailleurs bien adaptée aux problématiques spécifiques de l'analyse architecturale du patrimoine que sont par exemple la restitution d'édifices disparus (et les notions d'incertitude qui s'y attachent) ou le réemploi d'éléments de corpus. Pourtant, la représentation tridimensionnelle dans notre champ d'application est aujourd'hui loin de remplir ce rôle. Notre contribution vise à discuter quelques uns des pré-requis qui nous semblent s'imposer à la lumière de nos expériences pour faire de la maquette 3D un outil d'investigation des connaissances sur l'édifice.
**** *annee_2004  *numText_1087
Le présent papier concerne l'extension des méthodes classiques de régression linéaire aux cas des données symboliques et fait suite à de précédents travaux de Billard et Diday sur la régression linéaire avec variables intervalles et histogrammes. Dans ce papier, nous présentons des méthodes de régression avec variables taxonomiques. Les variables taxonomiques sont des variables organisées en arbre exprimant plusieurs niveaux de généralité (les villes sont regroupées en régions qui sont elles-mêmes regroupées en pays). La méthode proposée sera testée sur données simulées. Finalement, nous observerons que ces méthodes nous permettent d'utiliser la régression linéaire pour étudier des concepts et pour réduire le nombre de données afin d'améliorer les résultats obtenus par rapport à une régression classique.
**** *annee_2004  *numText_1088
Des relations entre gènes et protéines impliqués dans les cancers de la thyroïde ont été mises en évidence par l'analyse d'un important corpus de résumés de la base de données bibliographique Medline. Une approche pluridisciplinaire (biologistes, cliniciens, linguistes et chercheurs en sciences de l'information) a permis l'indexation automatique et l'analyse de ce corpus. L'indexation contrôlée, structurée en classes sémantiques, à partir de vastes ressources hétérogènes (les bases biomédicales et génétiques UMLS et LocusLink), prend en compte la spécificité des termes : nomenclatures biochimiques, acronymes de gènes, aberrations chromosomiques ou encore variantes linguistiques de termes. Les deux méthodes de classification complémentaires appliquées révèlent un réseau lexical dense de gènes concurrents autour de trois principales pathologies de la thyroïde : les cancers médullaires, papillaires et des dysfonctionnements du système immunitaire. Les développements apportés aux outils de visualisation interactifs du serveur VISA de l'INIST facilitent lecture et navigation au sein des documents.
**** *annee_2004  *numText_1089
Les motifs émergents sont des associations de caractéristiques fortement présentes dans une classe et rares dans les autres. Ils font ressortir les distinctions entre classes et se révèlent particulièrement efficaces pour construire des classifieurs et apporter une aide au diagnostic. À cause de la forte combinatoire du problème, la recherche et la représentation des motifs émergents restent des tâches complexes pour de grandes bases de données. Nous proposons ici une représentation condensée exacte des motifs émergents (i.e., les motifs et leurs taux de croissance sont directement obtenus depuis la représentation condensée). L'idée principale est de s'appuyer sur les récents résultats relatifs aux représentations condensées de motifs fermés fréquents. À partir de cette représentation, nous donnons aussi une méthode aisée à mettre en œuvre pour obtenir les motifs émergents ayant les meilleurs taux de croissance. Ces motifs, appelés motifs émergents forts, ont été exploités avec succès dans une collaboration avec la société Philips.
**** *annee_2004  *numText_1090
L'extraction d'information de grands graphes repose le plus souvent sur leur représentation dans des espaces de dimension réduite et on utilise généralement des méthodes factorielles appliquées à des mesures de dissimilarités calculées à partir des matrices associée du graphe ou l'analyse spectrale de leur Laplacien discret. Efficaces pour dégager les structures globales, ces représentations sont parfois peu exploitables dès lors que l'on s'intéresse à une perspective du graphe à partir de certains sommets privilégiés. Or l'information recherchée a souvent un caractère local. Pour représenter le graphe du point de vue d'un ou plusieurs sommets sélectionnés, nous proposons une méthode d'Analyse en Composantes Principales Granulaire consistant à appliquer une A.C.P. filtrée à un tableau de proximités. La visualisation d'un graphe de dictionnaire dont la mesure de proximité est obtenue à partir d'un algorithme original illustre notre propos.
**** *annee_2004  *numText_1091
Dans le contexte des entrepôts de données, et des magasins de données multidimensionnelles, les outils OLAP fournissent des moyens aux utilisateurs de naviguer dans leur données afin d'y découvrir des informations pertinentes. cependant, les données à traiter sons souvent très volumineuses et ne permettent pas une exploration systématique et exhaustive. Il s'agit donc de développer des traitements automatisés facilitant la visualisation et la navigation dans les données. Dans cet article, nous étudions une méthode originale permettant de construire et d'identifier de manière automatique et efficace des blocs de données similaires présents dans les cubes de données pouvant être exprimés sous la forme de règles. Cette méthode est fondée sur l'utilisation combinée d'un algorithme par niveaux (de type Apriori) et de la théorie des sous-ensembles flous. Cette théorie nous permet en effet de pallier les problèmes posés par le fait que les blocs de données calculés par notre algorithme peuvent se recouvrir.
**** *annee_2004  *numText_1093
La sélection de variables (SdV) permet de réduire l'espace de représentation des données. Ce processus est de plus en plus critique en raison de l'augmentation de la taille des bases de données. Traditionnellement, les méthodes de SdV nécessitent plusieurs accès au jeu de données, ce qui peut représenter une part relativement importante du temps d'exécution de ces algorithmes. Nous proposons une nouvelle méthode efficiente et rapide (ne nécessitant qu'un unique accès aux données). Cette méthode utilise les algorithmes génétiques ainsi que des mesures de validité de classification non supervisée (cns).
**** *annee_2004  *numText_1094
Les sous-ensembles flous peuvent être utilisés pour représenter des valeurs imprécises, comme un intervalle aux limites mal définies. Ils peuvent également servir à l'expression de préférences dans les critères de sélection de requêtes en bases de données. En représentation des connaissances, l'utilisation de hiérarchies de types est largement répandue afin de modéliser les relations existant entre les types d'objets d'un domaine donné. Nous nous intéressons aux sous-ensembles flous dont le domaine de définition est une hiérarchie d'éléments partiellement ordonnés par la relation sorte de, que nous appelons ontologie. Nous introduisons la notion de sous-ensemble flou défini sur une partie de l'ontologie, puis sa forme développée définie sur l'ensemble de l'ontologie, que nous appelons extension du sous-ensemble flou. Des classes d'équivalence de sous-ensembles flous définis sur une ontologie peuvent être caractérisées par un représentant unique que nous appelons sous-ensemble flou minimal. Nous concluons par un exemple d'application dans un système d'information relatif à la prévention du risque micro-biologique en sécurité alimentaire.
**** *annee_2004  *numText_1096
Le traitement de grand volume de données est un problème pour l'extraction de connaissances. La fouille de données nécessite des méthodes de résolution efficaces. Le treillis de concepts (treillis de Galois) est un outil utile pour l'analyse de données. Des travaux en classification et sur les règles d'association ont permis d'accroître son intérêt. Plusieurs algorithmes de génération on été proposés, parmi lesquels NextClosure est l'un des meilleurs pour traiter des données de grande taille.Mais la complexité de NextClosure reste malgré tout très élevé. Aussi nous proposons un nouvel algorithme efficace nommé ScalingNextClosure, et basé sur une méthode de partitionnement de données pour générer de manière indépendante les itemsets fermés de chaque partition. Les résultats expérimentaux montrer que cette technique de partitionnement améliore efficacement NextClosure.
**** *annee_2004  *numText_1099
La gestion explicite des savoirs et savoir-faire occupe une place de plus en plus importante dans les organisations. La construction de mémoires d'entreprise dans un but de préservation et de partage est devenu une pratique assez courante. Cependant, on oublie trop suivent que l'efficacité de ces activités est étroitement liée aux capacités d'appropriation et d'apprentissage des acteurs de l'organisation.Dans cet article, nous proposons des démarches générales d'accompagnement permettant de faciliter le processus d'appropriation des mémoires d'entreprise construits avec la méthode MASK, en exploitant des techniques d'ingénierie pédagogique.
**** *annee_2004  *numText_1100
La classification suivant les plus proches voisins est une règle simple et attractive, basée sur une définition paramétrique du voisinage. Les graphes des proximité, quand à eux, induisent des notions plus souples de voisinage. Il s'agit ici d'effectuer la substitution.Les variantes obtenues, peu testées dans la bibliographie, ont été soumises à une expérimentation intensive, sur bases de données de l'UCI et de France Télécom. On a ainsi considéré divers types de prétraitement des données et plusieurs catégories de graphes. De plus, on a caractérisé les effets du piège de la dimension sur le comportement théorique de tous les graphes présentés, une quantification empirique du phénomène ayant été réalisée.Il ressort de notre étude que l'utilisation du voisinage de Gabriel provoque une amélioration en moyenne et que le prétraitement basé sur la statistique de rang est le plus adéquate. Quoiqu'il arrive, des précautions doivent être prises en grande dimension.
**** *annee_2004  *numText_1101
Les travaux menés en validation des connaissances visent à améliorer la qualité des bases de connaissances. Le modèle des graphes conceptuels est un modèle de représentation des connaissances de la famille des réseaux sémantiques, fondé sur la théorie des graphes et sur la logique du premier ordre. Nous proposons une solution pour valider sémantiquement une base de connaissances composée de graphes conceptuels. La validation sémantique d'une base de connaissance consiste à confronter ses connaissances à des contraintes certifiées fiables. Nous proposons d'utiliser des contraintes descriptives, exprimées sous forme de graphes conceptuels, qui permettent de poser des conditions sur la représentation de certaines connaissance dans la base. Ces contraintes introduisent une notion de cardinalités, et sont soit minimales, soit maximales. Elles permettent respectivement d'exprimer si A, alors au moins ou au plus n fois B. La satisfaction de ces contraintes par une base de connaissances repose sur l'utilisation de l'opération de base du modèle des graphes conceptuels : la projection.
**** *annee_2004  *numText_1102
Le domaine de la veille technologique vise à récolter, traiter, et analyser des informations scientifiques et techniques utiles aux acteurs économiques. Dans cet article nous proposons d'utiliser des techniques de fouille de textes pour automatiser le processus de traitement des données issues de bases de textes scientifiques. Toutefois, la veille introduit une difficulté inhabituelle par rapport aux domaines d'application classiques des techniques de fouille de textes, puisqu'au lieu de rechercher de la connaissances fréquente cachée dans les données, il faut rechercher de la connaissance inattendue. Les mesures usuelles d'extraction de la connaissance à partir de textes doivent de ce fait être revues. Pour ce faire, nous avons développé le système UnexpectedMiner dans lequel de nouvelles mesures permettent d'estimer le caractère inattendu d'un document. Notre système est évalué sur une base d'articles dans le domaine de l'apprentissage automatique.
**** *annee_2004  *numText_1103
Les entrepôts de données sont l'un des plus importants développements dans le domaine des systèmes d'informations. Ils permettent d'intégrer des données de plusieurs sources, souvent très volumineux, distribuées et hétérogènes. Dans cet article, nous examinons la possibilité d'utiliser la technique d'entrepôt de données dans la gestion des risques naturels. Nous présentons un modèle conceptuel pour l'entrepôt proposé, avec la présence de formats et types variés de données tel que des données géographiques et multimédia. Nous proposons également des opérations OLAP pour la navigation des informations stockées dans le cube de données.